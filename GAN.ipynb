{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmRgmqei0mqp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ea51d97c-65b2-437c-cb6d-4e21a6899b81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 0s 2ms/step\n",
            "0 [D loss: 0.736483, acc.: 38.87%] [G loss: 0.675323]\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1 [D loss: 0.400397, acc.: 71.58%] [G loss: 0.703365]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2 [D loss: 0.367696, acc.: 79.49%] [G loss: 0.726067]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "3 [D loss: 0.347752, acc.: 83.50%] [G loss: 0.773325]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "4 [D loss: 0.336764, acc.: 83.79%] [G loss: 0.828650]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "5 [D loss: 0.324841, acc.: 85.55%] [G loss: 0.878346]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "6 [D loss: 0.307789, acc.: 87.55%] [G loss: 0.938587]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "7 [D loss: 0.288979, acc.: 90.77%] [G loss: 0.999844]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "8 [D loss: 0.266836, acc.: 92.92%] [G loss: 1.064653]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "9 [D loss: 0.255667, acc.: 93.70%] [G loss: 1.120394]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "10 [D loss: 0.247869, acc.: 93.99%] [G loss: 1.200233]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "11 [D loss: 0.228230, acc.: 95.70%] [G loss: 1.267418]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "12 [D loss: 0.208921, acc.: 97.71%] [G loss: 1.326482]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "13 [D loss: 0.198932, acc.: 97.31%] [G loss: 1.387075]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "14 [D loss: 0.186121, acc.: 98.78%] [G loss: 1.459564]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "15 [D loss: 0.179151, acc.: 98.78%] [G loss: 1.505692]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "16 [D loss: 0.166771, acc.: 98.83%] [G loss: 1.559887]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "17 [D loss: 0.155945, acc.: 99.61%] [G loss: 1.618925]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "18 [D loss: 0.147938, acc.: 99.56%] [G loss: 1.669435]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "19 [D loss: 0.138918, acc.: 99.80%] [G loss: 1.740884]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "20 [D loss: 0.130989, acc.: 99.80%] [G loss: 1.781761]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "21 [D loss: 0.124585, acc.: 99.76%] [G loss: 1.818630]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "22 [D loss: 0.120062, acc.: 99.85%] [G loss: 1.867968]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "23 [D loss: 0.112357, acc.: 100.00%] [G loss: 1.912442]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "24 [D loss: 0.109775, acc.: 100.00%] [G loss: 1.954776]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "25 [D loss: 0.103394, acc.: 99.90%] [G loss: 1.979671]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "26 [D loss: 0.102484, acc.: 100.00%] [G loss: 2.022834]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "27 [D loss: 0.095809, acc.: 99.95%] [G loss: 2.069827]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "28 [D loss: 0.092280, acc.: 100.00%] [G loss: 2.100597]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "29 [D loss: 0.088736, acc.: 100.00%] [G loss: 2.143015]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "30 [D loss: 0.086201, acc.: 100.00%] [G loss: 2.181733]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "31 [D loss: 0.082595, acc.: 100.00%] [G loss: 2.206480]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "32 [D loss: 0.080924, acc.: 100.00%] [G loss: 2.235561]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "33 [D loss: 0.081085, acc.: 100.00%] [G loss: 2.265559]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "34 [D loss: 0.077972, acc.: 100.00%] [G loss: 2.294732]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "35 [D loss: 0.073231, acc.: 100.00%] [G loss: 2.320718]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "36 [D loss: 0.072212, acc.: 100.00%] [G loss: 2.342681]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "37 [D loss: 0.069733, acc.: 100.00%] [G loss: 2.363412]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "38 [D loss: 0.069251, acc.: 100.00%] [G loss: 2.401883]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "39 [D loss: 0.066551, acc.: 100.00%] [G loss: 2.434739]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "40 [D loss: 0.065312, acc.: 100.00%] [G loss: 2.446146]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "41 [D loss: 0.064470, acc.: 100.00%] [G loss: 2.481730]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "42 [D loss: 0.060270, acc.: 100.00%] [G loss: 2.490122]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "43 [D loss: 0.059966, acc.: 100.00%] [G loss: 2.501135]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "44 [D loss: 0.060184, acc.: 100.00%] [G loss: 2.529938]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "45 [D loss: 0.056565, acc.: 100.00%] [G loss: 2.561177]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "46 [D loss: 0.055180, acc.: 100.00%] [G loss: 2.562432]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "47 [D loss: 0.055228, acc.: 100.00%] [G loss: 2.584019]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "48 [D loss: 0.054873, acc.: 100.00%] [G loss: 2.600601]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "49 [D loss: 0.054655, acc.: 100.00%] [G loss: 2.626444]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "50 [D loss: 0.052652, acc.: 100.00%] [G loss: 2.646004]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "51 [D loss: 0.052118, acc.: 100.00%] [G loss: 2.642761]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "52 [D loss: 0.050456, acc.: 100.00%] [G loss: 2.671058]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "53 [D loss: 0.051295, acc.: 100.00%] [G loss: 2.692586]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "54 [D loss: 0.049072, acc.: 100.00%] [G loss: 2.693396]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "55 [D loss: 0.048122, acc.: 100.00%] [G loss: 2.709777]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "56 [D loss: 0.048600, acc.: 100.00%] [G loss: 2.733467]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "57 [D loss: 0.048795, acc.: 100.00%] [G loss: 2.763513]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "58 [D loss: 0.046826, acc.: 100.00%] [G loss: 2.754563]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "59 [D loss: 0.045647, acc.: 100.00%] [G loss: 2.780497]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "60 [D loss: 0.044826, acc.: 100.00%] [G loss: 2.784594]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "61 [D loss: 0.045260, acc.: 100.00%] [G loss: 2.792130]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "62 [D loss: 0.045251, acc.: 100.00%] [G loss: 2.807818]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "63 [D loss: 0.045065, acc.: 100.00%] [G loss: 2.816248]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "64 [D loss: 0.042317, acc.: 100.00%] [G loss: 2.834814]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "65 [D loss: 0.042579, acc.: 100.00%] [G loss: 2.833136]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "66 [D loss: 0.042282, acc.: 100.00%] [G loss: 2.844431]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "67 [D loss: 0.040499, acc.: 100.00%] [G loss: 2.875620]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "68 [D loss: 0.040244, acc.: 100.00%] [G loss: 2.882458]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "69 [D loss: 0.041409, acc.: 100.00%] [G loss: 2.892513]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "70 [D loss: 0.039697, acc.: 100.00%] [G loss: 2.903579]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "71 [D loss: 0.039191, acc.: 100.00%] [G loss: 2.903384]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "72 [D loss: 0.040550, acc.: 100.00%] [G loss: 2.898309]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "73 [D loss: 0.039547, acc.: 100.00%] [G loss: 2.935477]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "74 [D loss: 0.037682, acc.: 100.00%] [G loss: 2.949073]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "75 [D loss: 0.037746, acc.: 100.00%] [G loss: 2.941930]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "76 [D loss: 0.038702, acc.: 100.00%] [G loss: 2.947276]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "77 [D loss: 0.037203, acc.: 100.00%] [G loss: 2.977141]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "78 [D loss: 0.037258, acc.: 100.00%] [G loss: 2.965548]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "79 [D loss: 0.037646, acc.: 100.00%] [G loss: 2.975147]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "80 [D loss: 0.036299, acc.: 100.00%] [G loss: 2.979179]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "81 [D loss: 0.036667, acc.: 100.00%] [G loss: 2.984015]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "82 [D loss: 0.037715, acc.: 100.00%] [G loss: 2.996006]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "83 [D loss: 0.037832, acc.: 100.00%] [G loss: 2.983367]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "84 [D loss: 0.037328, acc.: 100.00%] [G loss: 3.001736]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "85 [D loss: 0.036409, acc.: 100.00%] [G loss: 3.033497]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "86 [D loss: 0.036132, acc.: 100.00%] [G loss: 3.026344]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "87 [D loss: 0.035102, acc.: 100.00%] [G loss: 3.024507]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "88 [D loss: 0.035137, acc.: 100.00%] [G loss: 3.034691]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "89 [D loss: 0.035145, acc.: 100.00%] [G loss: 3.038244]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "90 [D loss: 0.035749, acc.: 100.00%] [G loss: 3.032813]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "91 [D loss: 0.034624, acc.: 100.00%] [G loss: 3.058232]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "92 [D loss: 0.034195, acc.: 100.00%] [G loss: 3.061825]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "93 [D loss: 0.032893, acc.: 100.00%] [G loss: 3.067368]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "94 [D loss: 0.034554, acc.: 100.00%] [G loss: 3.071360]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "95 [D loss: 0.034017, acc.: 100.00%] [G loss: 3.073914]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "96 [D loss: 0.034311, acc.: 100.00%] [G loss: 3.073131]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "97 [D loss: 0.034263, acc.: 100.00%] [G loss: 3.104020]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "98 [D loss: 0.034063, acc.: 100.00%] [G loss: 3.098107]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "99 [D loss: 0.032970, acc.: 100.00%] [G loss: 3.080723]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "100 [D loss: 0.033345, acc.: 100.00%] [G loss: 3.093319]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "101 [D loss: 0.033698, acc.: 100.00%] [G loss: 3.102042]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "102 [D loss: 0.033829, acc.: 100.00%] [G loss: 3.097485]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "103 [D loss: 0.033214, acc.: 100.00%] [G loss: 3.104706]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "104 [D loss: 0.032940, acc.: 100.00%] [G loss: 3.099776]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "105 [D loss: 0.032255, acc.: 100.00%] [G loss: 3.115836]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "106 [D loss: 0.032309, acc.: 100.00%] [G loss: 3.115227]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "107 [D loss: 0.032349, acc.: 100.00%] [G loss: 3.119310]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "108 [D loss: 0.032138, acc.: 100.00%] [G loss: 3.134948]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "109 [D loss: 0.032144, acc.: 100.00%] [G loss: 3.140178]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "110 [D loss: 0.032903, acc.: 100.00%] [G loss: 3.129282]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "111 [D loss: 0.031587, acc.: 100.00%] [G loss: 3.123914]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "112 [D loss: 0.031699, acc.: 100.00%] [G loss: 3.134625]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "113 [D loss: 0.031653, acc.: 100.00%] [G loss: 3.115449]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "114 [D loss: 0.031695, acc.: 100.00%] [G loss: 3.136627]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "115 [D loss: 0.032288, acc.: 100.00%] [G loss: 3.144590]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "116 [D loss: 0.031767, acc.: 100.00%] [G loss: 3.144646]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "117 [D loss: 0.031620, acc.: 100.00%] [G loss: 3.140074]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "118 [D loss: 0.032023, acc.: 100.00%] [G loss: 3.155138]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "119 [D loss: 0.031552, acc.: 100.00%] [G loss: 3.151105]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "120 [D loss: 0.031352, acc.: 100.00%] [G loss: 3.168933]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "121 [D loss: 0.031135, acc.: 100.00%] [G loss: 3.178745]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "122 [D loss: 0.031790, acc.: 100.00%] [G loss: 3.160391]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "123 [D loss: 0.030717, acc.: 100.00%] [G loss: 3.159917]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "124 [D loss: 0.031885, acc.: 100.00%] [G loss: 3.165682]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "125 [D loss: 0.032204, acc.: 100.00%] [G loss: 3.165872]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "126 [D loss: 0.032433, acc.: 100.00%] [G loss: 3.174314]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "127 [D loss: 0.032336, acc.: 99.95%] [G loss: 3.173257]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "128 [D loss: 0.030782, acc.: 100.00%] [G loss: 3.158873]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "129 [D loss: 0.031118, acc.: 100.00%] [G loss: 3.185410]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "130 [D loss: 0.030728, acc.: 100.00%] [G loss: 3.163320]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "131 [D loss: 0.031829, acc.: 100.00%] [G loss: 3.184618]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "132 [D loss: 0.032234, acc.: 100.00%] [G loss: 3.189480]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "133 [D loss: 0.033493, acc.: 100.00%] [G loss: 3.186533]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "134 [D loss: 0.033467, acc.: 100.00%] [G loss: 3.194421]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "135 [D loss: 0.031496, acc.: 100.00%] [G loss: 3.187440]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "136 [D loss: 0.030561, acc.: 100.00%] [G loss: 3.182732]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "137 [D loss: 0.032410, acc.: 100.00%] [G loss: 3.199008]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "138 [D loss: 0.031911, acc.: 100.00%] [G loss: 3.176640]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "139 [D loss: 0.030721, acc.: 100.00%] [G loss: 3.192292]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "140 [D loss: 0.031295, acc.: 100.00%] [G loss: 3.185725]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "141 [D loss: 0.031728, acc.: 100.00%] [G loss: 3.174473]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "142 [D loss: 0.031360, acc.: 100.00%] [G loss: 3.173764]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "143 [D loss: 0.031075, acc.: 100.00%] [G loss: 3.178932]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "144 [D loss: 0.031595, acc.: 100.00%] [G loss: 3.192639]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "145 [D loss: 0.031766, acc.: 100.00%] [G loss: 3.182685]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "146 [D loss: 0.032095, acc.: 100.00%] [G loss: 3.196716]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "147 [D loss: 0.031649, acc.: 100.00%] [G loss: 3.179706]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "148 [D loss: 0.032282, acc.: 100.00%] [G loss: 3.178653]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "149 [D loss: 0.031303, acc.: 100.00%] [G loss: 3.184272]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "150 [D loss: 0.032270, acc.: 100.00%] [G loss: 3.195833]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "151 [D loss: 0.031889, acc.: 100.00%] [G loss: 3.198118]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "152 [D loss: 0.031008, acc.: 100.00%] [G loss: 3.188448]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "153 [D loss: 0.031381, acc.: 100.00%] [G loss: 3.205538]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "154 [D loss: 0.032871, acc.: 100.00%] [G loss: 3.183668]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "155 [D loss: 0.032016, acc.: 100.00%] [G loss: 3.197899]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "156 [D loss: 0.031038, acc.: 100.00%] [G loss: 3.199667]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "157 [D loss: 0.031813, acc.: 100.00%] [G loss: 3.201065]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "158 [D loss: 0.032028, acc.: 100.00%] [G loss: 3.201489]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "159 [D loss: 0.031648, acc.: 100.00%] [G loss: 3.190668]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "160 [D loss: 0.030040, acc.: 100.00%] [G loss: 3.187094]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "161 [D loss: 0.033148, acc.: 100.00%] [G loss: 3.180726]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "162 [D loss: 0.031084, acc.: 100.00%] [G loss: 3.199860]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "163 [D loss: 0.032772, acc.: 100.00%] [G loss: 3.190806]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "164 [D loss: 0.032553, acc.: 100.00%] [G loss: 3.185017]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "165 [D loss: 0.032458, acc.: 100.00%] [G loss: 3.190112]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "166 [D loss: 0.032727, acc.: 100.00%] [G loss: 3.186018]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "167 [D loss: 0.033319, acc.: 100.00%] [G loss: 3.178082]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "168 [D loss: 0.032282, acc.: 100.00%] [G loss: 3.181259]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "169 [D loss: 0.033512, acc.: 100.00%] [G loss: 3.168201]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "170 [D loss: 0.032469, acc.: 100.00%] [G loss: 3.173801]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "171 [D loss: 0.032091, acc.: 100.00%] [G loss: 3.179343]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "172 [D loss: 0.033733, acc.: 100.00%] [G loss: 3.161062]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "173 [D loss: 0.032182, acc.: 100.00%] [G loss: 3.161118]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "174 [D loss: 0.032921, acc.: 100.00%] [G loss: 3.172368]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "175 [D loss: 0.033789, acc.: 100.00%] [G loss: 3.195834]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "176 [D loss: 0.034169, acc.: 100.00%] [G loss: 3.170516]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "177 [D loss: 0.032044, acc.: 100.00%] [G loss: 3.168964]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "178 [D loss: 0.033784, acc.: 100.00%] [G loss: 3.170413]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "179 [D loss: 0.034708, acc.: 100.00%] [G loss: 3.172831]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "180 [D loss: 0.034409, acc.: 100.00%] [G loss: 3.157261]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "181 [D loss: 0.034362, acc.: 100.00%] [G loss: 3.177931]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "182 [D loss: 0.033153, acc.: 99.95%] [G loss: 3.171648]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "183 [D loss: 0.034156, acc.: 100.00%] [G loss: 3.171886]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "184 [D loss: 0.034385, acc.: 100.00%] [G loss: 3.156615]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "185 [D loss: 0.034360, acc.: 100.00%] [G loss: 3.180000]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "186 [D loss: 0.034060, acc.: 100.00%] [G loss: 3.181568]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "187 [D loss: 0.036120, acc.: 100.00%] [G loss: 3.183040]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "188 [D loss: 0.033770, acc.: 100.00%] [G loss: 3.167642]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "189 [D loss: 0.033812, acc.: 100.00%] [G loss: 3.159802]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "190 [D loss: 0.034460, acc.: 100.00%] [G loss: 3.170655]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "191 [D loss: 0.035374, acc.: 100.00%] [G loss: 3.163129]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "192 [D loss: 0.034379, acc.: 100.00%] [G loss: 3.135712]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "193 [D loss: 0.033254, acc.: 100.00%] [G loss: 3.154846]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "194 [D loss: 0.035257, acc.: 100.00%] [G loss: 3.147087]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "195 [D loss: 0.034102, acc.: 100.00%] [G loss: 3.158005]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "196 [D loss: 0.035826, acc.: 100.00%] [G loss: 3.129054]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "197 [D loss: 0.034471, acc.: 100.00%] [G loss: 3.151878]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "198 [D loss: 0.034959, acc.: 100.00%] [G loss: 3.159284]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "199 [D loss: 0.034049, acc.: 100.00%] [G loss: 3.139119]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "200 [D loss: 0.035052, acc.: 100.00%] [G loss: 3.143574]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "201 [D loss: 0.035333, acc.: 100.00%] [G loss: 3.155055]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "202 [D loss: 0.034214, acc.: 99.95%] [G loss: 3.135727]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "203 [D loss: 0.035610, acc.: 100.00%] [G loss: 3.146012]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "204 [D loss: 0.035191, acc.: 100.00%] [G loss: 3.133518]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "205 [D loss: 0.035130, acc.: 100.00%] [G loss: 3.122798]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "206 [D loss: 0.033670, acc.: 100.00%] [G loss: 3.141984]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "207 [D loss: 0.036651, acc.: 100.00%] [G loss: 3.132453]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "208 [D loss: 0.034902, acc.: 100.00%] [G loss: 3.134875]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "209 [D loss: 0.034852, acc.: 100.00%] [G loss: 3.132378]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "210 [D loss: 0.037553, acc.: 100.00%] [G loss: 3.124212]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "211 [D loss: 0.035212, acc.: 100.00%] [G loss: 3.120540]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "212 [D loss: 0.037997, acc.: 100.00%] [G loss: 3.122381]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "213 [D loss: 0.036043, acc.: 100.00%] [G loss: 3.126144]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "214 [D loss: 0.036053, acc.: 100.00%] [G loss: 3.112884]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "215 [D loss: 0.036951, acc.: 100.00%] [G loss: 3.116678]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "216 [D loss: 0.036657, acc.: 100.00%] [G loss: 3.124219]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "217 [D loss: 0.037422, acc.: 100.00%] [G loss: 3.097598]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "218 [D loss: 0.038354, acc.: 100.00%] [G loss: 3.107254]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "219 [D loss: 0.036611, acc.: 100.00%] [G loss: 3.113780]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "220 [D loss: 0.037995, acc.: 100.00%] [G loss: 3.104085]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "221 [D loss: 0.039021, acc.: 100.00%] [G loss: 3.102680]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "222 [D loss: 0.036721, acc.: 100.00%] [G loss: 3.074673]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "223 [D loss: 0.038730, acc.: 100.00%] [G loss: 3.105444]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "224 [D loss: 0.038106, acc.: 100.00%] [G loss: 3.095024]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "225 [D loss: 0.037451, acc.: 100.00%] [G loss: 3.091433]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "226 [D loss: 0.040157, acc.: 100.00%] [G loss: 3.107133]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "227 [D loss: 0.037222, acc.: 100.00%] [G loss: 3.082855]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "228 [D loss: 0.038045, acc.: 99.95%] [G loss: 3.090400]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "229 [D loss: 0.037166, acc.: 100.00%] [G loss: 3.086831]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "230 [D loss: 0.036810, acc.: 100.00%] [G loss: 3.081840]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "231 [D loss: 0.038489, acc.: 100.00%] [G loss: 3.082187]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "232 [D loss: 0.039348, acc.: 100.00%] [G loss: 3.085575]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "233 [D loss: 0.039219, acc.: 100.00%] [G loss: 3.079041]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "234 [D loss: 0.038329, acc.: 100.00%] [G loss: 3.065519]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "235 [D loss: 0.037467, acc.: 100.00%] [G loss: 3.096252]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "236 [D loss: 0.039159, acc.: 100.00%] [G loss: 3.074005]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "237 [D loss: 0.039679, acc.: 100.00%] [G loss: 3.066803]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "238 [D loss: 0.037062, acc.: 100.00%] [G loss: 3.070294]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "239 [D loss: 0.038061, acc.: 100.00%] [G loss: 3.072885]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "240 [D loss: 0.038620, acc.: 100.00%] [G loss: 3.086739]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "241 [D loss: 0.041252, acc.: 99.95%] [G loss: 3.066904]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "242 [D loss: 0.040441, acc.: 99.95%] [G loss: 3.079470]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "243 [D loss: 0.039653, acc.: 100.00%] [G loss: 3.067398]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "244 [D loss: 0.039844, acc.: 100.00%] [G loss: 3.064756]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "245 [D loss: 0.041836, acc.: 100.00%] [G loss: 3.068318]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "246 [D loss: 0.041031, acc.: 99.95%] [G loss: 3.060719]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "247 [D loss: 0.040964, acc.: 100.00%] [G loss: 3.055254]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "248 [D loss: 0.040883, acc.: 100.00%] [G loss: 3.069715]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "249 [D loss: 0.041143, acc.: 100.00%] [G loss: 3.047112]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "250 [D loss: 0.039764, acc.: 100.00%] [G loss: 3.067658]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "251 [D loss: 0.042509, acc.: 100.00%] [G loss: 3.060023]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "252 [D loss: 0.042163, acc.: 100.00%] [G loss: 3.066730]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "253 [D loss: 0.040701, acc.: 100.00%] [G loss: 3.060948]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "254 [D loss: 0.039667, acc.: 100.00%] [G loss: 3.050876]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "255 [D loss: 0.040896, acc.: 100.00%] [G loss: 3.056021]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "256 [D loss: 0.041588, acc.: 100.00%] [G loss: 3.047952]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "257 [D loss: 0.041185, acc.: 100.00%] [G loss: 3.051059]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "258 [D loss: 0.042254, acc.: 100.00%] [G loss: 3.037936]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "259 [D loss: 0.040617, acc.: 100.00%] [G loss: 3.051350]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "260 [D loss: 0.041151, acc.: 100.00%] [G loss: 3.057651]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "261 [D loss: 0.040743, acc.: 100.00%] [G loss: 3.059740]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "262 [D loss: 0.040948, acc.: 100.00%] [G loss: 3.046277]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "263 [D loss: 0.043846, acc.: 100.00%] [G loss: 3.059923]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "264 [D loss: 0.041420, acc.: 100.00%] [G loss: 3.043625]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "265 [D loss: 0.042321, acc.: 100.00%] [G loss: 3.038908]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "266 [D loss: 0.040083, acc.: 100.00%] [G loss: 3.043911]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "267 [D loss: 0.040147, acc.: 100.00%] [G loss: 3.051798]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "268 [D loss: 0.040839, acc.: 99.95%] [G loss: 3.055452]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "269 [D loss: 0.041523, acc.: 100.00%] [G loss: 3.046090]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "270 [D loss: 0.041111, acc.: 99.95%] [G loss: 3.035182]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "271 [D loss: 0.041609, acc.: 100.00%] [G loss: 3.045629]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "272 [D loss: 0.042722, acc.: 100.00%] [G loss: 3.034119]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "273 [D loss: 0.039725, acc.: 100.00%] [G loss: 3.010916]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "274 [D loss: 0.039621, acc.: 100.00%] [G loss: 3.020333]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "275 [D loss: 0.042566, acc.: 100.00%] [G loss: 3.032021]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "276 [D loss: 0.039092, acc.: 100.00%] [G loss: 3.035799]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "277 [D loss: 0.042110, acc.: 100.00%] [G loss: 3.038259]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "278 [D loss: 0.041821, acc.: 99.95%] [G loss: 3.028381]\n",
            "32/32 [==============================] - 0s 5ms/step\n",
            "279 [D loss: 0.041227, acc.: 100.00%] [G loss: 3.025017]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "280 [D loss: 0.041888, acc.: 100.00%] [G loss: 3.028900]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "281 [D loss: 0.043721, acc.: 100.00%] [G loss: 3.031130]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "282 [D loss: 0.040757, acc.: 100.00%] [G loss: 3.029128]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "283 [D loss: 0.042964, acc.: 100.00%] [G loss: 3.028379]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "284 [D loss: 0.041990, acc.: 100.00%] [G loss: 3.029688]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "285 [D loss: 0.043154, acc.: 100.00%] [G loss: 3.041645]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "286 [D loss: 0.042041, acc.: 100.00%] [G loss: 3.018640]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "287 [D loss: 0.042500, acc.: 100.00%] [G loss: 3.031283]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "288 [D loss: 0.042883, acc.: 100.00%] [G loss: 3.025037]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "289 [D loss: 0.042899, acc.: 100.00%] [G loss: 3.054110]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "290 [D loss: 0.042687, acc.: 100.00%] [G loss: 3.032316]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "291 [D loss: 0.042286, acc.: 100.00%] [G loss: 3.019789]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "292 [D loss: 0.044906, acc.: 100.00%] [G loss: 3.044708]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "293 [D loss: 0.044201, acc.: 100.00%] [G loss: 3.038514]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "294 [D loss: 0.042263, acc.: 100.00%] [G loss: 3.046815]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "295 [D loss: 0.041930, acc.: 100.00%] [G loss: 3.048160]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "296 [D loss: 0.042515, acc.: 100.00%] [G loss: 3.026029]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "297 [D loss: 0.041782, acc.: 100.00%] [G loss: 3.034929]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "298 [D loss: 0.041754, acc.: 100.00%] [G loss: 3.048609]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "299 [D loss: 0.041277, acc.: 100.00%] [G loss: 3.042045]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "300 [D loss: 0.041279, acc.: 99.95%] [G loss: 3.036468]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "301 [D loss: 0.041291, acc.: 100.00%] [G loss: 3.048677]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "302 [D loss: 0.041422, acc.: 100.00%] [G loss: 3.047088]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "303 [D loss: 0.041139, acc.: 99.95%] [G loss: 3.043919]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "304 [D loss: 0.040588, acc.: 100.00%] [G loss: 3.045713]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "305 [D loss: 0.041757, acc.: 99.95%] [G loss: 3.067695]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "306 [D loss: 0.041642, acc.: 100.00%] [G loss: 3.041439]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "307 [D loss: 0.041256, acc.: 100.00%] [G loss: 3.047583]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "308 [D loss: 0.041479, acc.: 99.95%] [G loss: 3.036654]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "309 [D loss: 0.041747, acc.: 99.95%] [G loss: 3.042587]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "310 [D loss: 0.041351, acc.: 99.95%] [G loss: 3.048066]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "311 [D loss: 0.042011, acc.: 99.95%] [G loss: 3.035028]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "312 [D loss: 0.041932, acc.: 100.00%] [G loss: 3.053571]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "313 [D loss: 0.043812, acc.: 100.00%] [G loss: 3.062717]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "314 [D loss: 0.040711, acc.: 100.00%] [G loss: 3.052769]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "315 [D loss: 0.039604, acc.: 100.00%] [G loss: 3.053512]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "316 [D loss: 0.041346, acc.: 100.00%] [G loss: 3.048106]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "317 [D loss: 0.043302, acc.: 100.00%] [G loss: 3.044290]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "318 [D loss: 0.041317, acc.: 100.00%] [G loss: 3.054258]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "319 [D loss: 0.041663, acc.: 100.00%] [G loss: 3.056450]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "320 [D loss: 0.043897, acc.: 99.95%] [G loss: 3.047830]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "321 [D loss: 0.041436, acc.: 100.00%] [G loss: 3.059517]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "322 [D loss: 0.041289, acc.: 100.00%] [G loss: 3.071226]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "323 [D loss: 0.042590, acc.: 99.95%] [G loss: 3.073355]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "324 [D loss: 0.042954, acc.: 99.95%] [G loss: 3.059683]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "325 [D loss: 0.042196, acc.: 99.90%] [G loss: 3.060933]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "326 [D loss: 0.040688, acc.: 99.95%] [G loss: 3.087151]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "327 [D loss: 0.041499, acc.: 100.00%] [G loss: 3.079163]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "328 [D loss: 0.039299, acc.: 100.00%] [G loss: 3.073363]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "329 [D loss: 0.041194, acc.: 100.00%] [G loss: 3.088912]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "330 [D loss: 0.040953, acc.: 99.95%] [G loss: 3.089108]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "331 [D loss: 0.041879, acc.: 100.00%] [G loss: 3.080789]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "332 [D loss: 0.041144, acc.: 100.00%] [G loss: 3.066875]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "333 [D loss: 0.041088, acc.: 100.00%] [G loss: 3.087465]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "334 [D loss: 0.041524, acc.: 100.00%] [G loss: 3.093488]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "335 [D loss: 0.039648, acc.: 100.00%] [G loss: 3.094853]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "336 [D loss: 0.042151, acc.: 100.00%] [G loss: 3.093100]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "337 [D loss: 0.040766, acc.: 100.00%] [G loss: 3.101635]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "338 [D loss: 0.040846, acc.: 100.00%] [G loss: 3.108485]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "339 [D loss: 0.040441, acc.: 100.00%] [G loss: 3.121927]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "340 [D loss: 0.042132, acc.: 99.95%] [G loss: 3.113238]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "341 [D loss: 0.041102, acc.: 100.00%] [G loss: 3.140468]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "342 [D loss: 0.040292, acc.: 100.00%] [G loss: 3.129261]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "343 [D loss: 0.039430, acc.: 99.95%] [G loss: 3.128106]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "344 [D loss: 0.039996, acc.: 100.00%] [G loss: 3.144711]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "345 [D loss: 0.039756, acc.: 100.00%] [G loss: 3.143171]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "346 [D loss: 0.040172, acc.: 100.00%] [G loss: 3.145738]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "347 [D loss: 0.041801, acc.: 100.00%] [G loss: 3.150191]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "348 [D loss: 0.038795, acc.: 99.95%] [G loss: 3.119961]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "349 [D loss: 0.039194, acc.: 100.00%] [G loss: 3.147094]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "350 [D loss: 0.040342, acc.: 100.00%] [G loss: 3.151002]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "351 [D loss: 0.039381, acc.: 100.00%] [G loss: 3.164869]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "352 [D loss: 0.039550, acc.: 100.00%] [G loss: 3.160980]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "353 [D loss: 0.037579, acc.: 100.00%] [G loss: 3.160172]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "354 [D loss: 0.039742, acc.: 100.00%] [G loss: 3.185147]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "355 [D loss: 0.038612, acc.: 100.00%] [G loss: 3.173933]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "356 [D loss: 0.041335, acc.: 100.00%] [G loss: 3.161869]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "357 [D loss: 0.040396, acc.: 99.95%] [G loss: 3.156016]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "358 [D loss: 0.040735, acc.: 100.00%] [G loss: 3.166064]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "359 [D loss: 0.040763, acc.: 99.95%] [G loss: 3.162411]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "360 [D loss: 0.039404, acc.: 99.95%] [G loss: 3.146904]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "361 [D loss: 0.041279, acc.: 100.00%] [G loss: 3.191550]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "362 [D loss: 0.040249, acc.: 100.00%] [G loss: 3.167949]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "363 [D loss: 0.039011, acc.: 99.95%] [G loss: 3.171337]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "364 [D loss: 0.038185, acc.: 100.00%] [G loss: 3.189434]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "365 [D loss: 0.039167, acc.: 100.00%] [G loss: 3.183201]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "366 [D loss: 0.038035, acc.: 100.00%] [G loss: 3.198450]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "367 [D loss: 0.039817, acc.: 99.95%] [G loss: 3.211400]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "368 [D loss: 0.038514, acc.: 100.00%] [G loss: 3.197586]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "369 [D loss: 0.037498, acc.: 100.00%] [G loss: 3.199070]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "370 [D loss: 0.037187, acc.: 100.00%] [G loss: 3.194522]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "371 [D loss: 0.038626, acc.: 100.00%] [G loss: 3.236972]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "372 [D loss: 0.036577, acc.: 100.00%] [G loss: 3.238156]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "373 [D loss: 0.037720, acc.: 99.95%] [G loss: 3.231071]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "374 [D loss: 0.037454, acc.: 100.00%] [G loss: 3.232189]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "375 [D loss: 0.037787, acc.: 99.95%] [G loss: 3.235643]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "376 [D loss: 0.039243, acc.: 100.00%] [G loss: 3.232209]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "377 [D loss: 0.035768, acc.: 100.00%] [G loss: 3.227940]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "378 [D loss: 0.037862, acc.: 100.00%] [G loss: 3.236394]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "379 [D loss: 0.036732, acc.: 100.00%] [G loss: 3.252004]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "380 [D loss: 0.039448, acc.: 99.90%] [G loss: 3.239292]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "381 [D loss: 0.037882, acc.: 100.00%] [G loss: 3.247368]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "382 [D loss: 0.037059, acc.: 100.00%] [G loss: 3.243902]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "383 [D loss: 0.037295, acc.: 100.00%] [G loss: 3.254174]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "384 [D loss: 0.037231, acc.: 100.00%] [G loss: 3.264431]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "385 [D loss: 0.036276, acc.: 100.00%] [G loss: 3.276470]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "386 [D loss: 0.036759, acc.: 100.00%] [G loss: 3.278343]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "387 [D loss: 0.037685, acc.: 100.00%] [G loss: 3.276088]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "388 [D loss: 0.036733, acc.: 100.00%] [G loss: 3.276332]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "389 [D loss: 0.039532, acc.: 99.95%] [G loss: 3.259248]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "390 [D loss: 0.037379, acc.: 100.00%] [G loss: 3.288837]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "391 [D loss: 0.036315, acc.: 100.00%] [G loss: 3.290211]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "392 [D loss: 0.036276, acc.: 99.95%] [G loss: 3.293150]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "393 [D loss: 0.037102, acc.: 100.00%] [G loss: 3.295810]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "394 [D loss: 0.036317, acc.: 99.95%] [G loss: 3.304670]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "395 [D loss: 0.036501, acc.: 100.00%] [G loss: 3.296961]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "396 [D loss: 0.036496, acc.: 99.95%] [G loss: 3.313203]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "397 [D loss: 0.038343, acc.: 100.00%] [G loss: 3.310419]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "398 [D loss: 0.037169, acc.: 100.00%] [G loss: 3.329782]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "399 [D loss: 0.036414, acc.: 99.95%] [G loss: 3.324654]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "400 [D loss: 0.035079, acc.: 100.00%] [G loss: 3.318892]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "401 [D loss: 0.036262, acc.: 100.00%] [G loss: 3.333277]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "402 [D loss: 0.036138, acc.: 100.00%] [G loss: 3.334277]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "403 [D loss: 0.035502, acc.: 100.00%] [G loss: 3.332450]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "404 [D loss: 0.035952, acc.: 100.00%] [G loss: 3.346346]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "405 [D loss: 0.037950, acc.: 100.00%] [G loss: 3.365808]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "406 [D loss: 0.036565, acc.: 100.00%] [G loss: 3.356429]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "407 [D loss: 0.034834, acc.: 100.00%] [G loss: 3.375941]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "408 [D loss: 0.035758, acc.: 100.00%] [G loss: 3.349632]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "409 [D loss: 0.035439, acc.: 100.00%] [G loss: 3.376793]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "410 [D loss: 0.036201, acc.: 100.00%] [G loss: 3.384490]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "411 [D loss: 0.034395, acc.: 100.00%] [G loss: 3.369620]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "412 [D loss: 0.034479, acc.: 100.00%] [G loss: 3.360819]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "413 [D loss: 0.035318, acc.: 100.00%] [G loss: 3.349847]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "414 [D loss: 0.037736, acc.: 100.00%] [G loss: 3.382701]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "415 [D loss: 0.035624, acc.: 100.00%] [G loss: 3.375907]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "416 [D loss: 0.036480, acc.: 100.00%] [G loss: 3.406001]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "417 [D loss: 0.035685, acc.: 100.00%] [G loss: 3.408274]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "418 [D loss: 0.035776, acc.: 100.00%] [G loss: 3.427998]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "419 [D loss: 0.034037, acc.: 100.00%] [G loss: 3.431800]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "420 [D loss: 0.035279, acc.: 100.00%] [G loss: 3.410500]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "421 [D loss: 0.034946, acc.: 100.00%] [G loss: 3.409616]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "422 [D loss: 0.037558, acc.: 99.95%] [G loss: 3.428657]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "423 [D loss: 0.034575, acc.: 100.00%] [G loss: 3.429364]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "424 [D loss: 0.035330, acc.: 99.95%] [G loss: 3.419326]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "425 [D loss: 0.035394, acc.: 100.00%] [G loss: 3.419621]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "426 [D loss: 0.034434, acc.: 100.00%] [G loss: 3.427691]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "427 [D loss: 0.035669, acc.: 100.00%] [G loss: 3.455580]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "428 [D loss: 0.035831, acc.: 100.00%] [G loss: 3.439145]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "429 [D loss: 0.036413, acc.: 100.00%] [G loss: 3.451401]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "430 [D loss: 0.035988, acc.: 100.00%] [G loss: 3.467088]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "431 [D loss: 0.035233, acc.: 100.00%] [G loss: 3.435917]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "432 [D loss: 0.035870, acc.: 100.00%] [G loss: 3.469820]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "433 [D loss: 0.034946, acc.: 100.00%] [G loss: 3.491188]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "434 [D loss: 0.035836, acc.: 99.95%] [G loss: 3.475376]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "435 [D loss: 0.036538, acc.: 99.95%] [G loss: 3.461726]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "436 [D loss: 0.033850, acc.: 99.95%] [G loss: 3.496466]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "437 [D loss: 0.034525, acc.: 100.00%] [G loss: 3.493609]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "438 [D loss: 0.037900, acc.: 99.85%] [G loss: 3.488292]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "439 [D loss: 0.035231, acc.: 99.95%] [G loss: 3.493725]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "440 [D loss: 0.035996, acc.: 100.00%] [G loss: 3.463140]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "441 [D loss: 0.035994, acc.: 100.00%] [G loss: 3.522658]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "442 [D loss: 0.034127, acc.: 100.00%] [G loss: 3.527872]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "443 [D loss: 0.036231, acc.: 99.95%] [G loss: 3.524223]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "444 [D loss: 0.031715, acc.: 99.95%] [G loss: 3.507901]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "445 [D loss: 0.037206, acc.: 100.00%] [G loss: 3.501212]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "446 [D loss: 0.034879, acc.: 99.95%] [G loss: 3.485642]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "447 [D loss: 0.033724, acc.: 100.00%] [G loss: 3.490803]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "448 [D loss: 0.036100, acc.: 99.90%] [G loss: 3.502168]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "449 [D loss: 0.035571, acc.: 99.95%] [G loss: 3.505648]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "450 [D loss: 0.036706, acc.: 100.00%] [G loss: 3.508971]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "451 [D loss: 0.034484, acc.: 100.00%] [G loss: 3.517813]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "452 [D loss: 0.034362, acc.: 99.95%] [G loss: 3.524054]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "453 [D loss: 0.034435, acc.: 100.00%] [G loss: 3.505706]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "454 [D loss: 0.037410, acc.: 100.00%] [G loss: 3.532554]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "455 [D loss: 0.035238, acc.: 100.00%] [G loss: 3.539082]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "456 [D loss: 0.037084, acc.: 99.95%] [G loss: 3.531899]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "457 [D loss: 0.035947, acc.: 100.00%] [G loss: 3.550979]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "458 [D loss: 0.035168, acc.: 100.00%] [G loss: 3.547848]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "459 [D loss: 0.035288, acc.: 100.00%] [G loss: 3.549895]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "460 [D loss: 0.034731, acc.: 100.00%] [G loss: 3.552848]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "461 [D loss: 0.036747, acc.: 100.00%] [G loss: 3.553643]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "462 [D loss: 0.035318, acc.: 100.00%] [G loss: 3.574460]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "463 [D loss: 0.036236, acc.: 100.00%] [G loss: 3.553296]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "464 [D loss: 0.036944, acc.: 99.95%] [G loss: 3.573665]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "465 [D loss: 0.038587, acc.: 100.00%] [G loss: 3.575356]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "466 [D loss: 0.038969, acc.: 100.00%] [G loss: 3.599280]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "467 [D loss: 0.035939, acc.: 100.00%] [G loss: 3.591040]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "468 [D loss: 0.035346, acc.: 100.00%] [G loss: 3.595986]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "469 [D loss: 0.035722, acc.: 100.00%] [G loss: 3.585665]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "470 [D loss: 0.035278, acc.: 100.00%] [G loss: 3.579614]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "471 [D loss: 0.036223, acc.: 100.00%] [G loss: 3.577161]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "472 [D loss: 0.037980, acc.: 99.95%] [G loss: 3.547517]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "473 [D loss: 0.037671, acc.: 100.00%] [G loss: 3.593083]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "474 [D loss: 0.039108, acc.: 99.95%] [G loss: 3.596719]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "475 [D loss: 0.037951, acc.: 100.00%] [G loss: 3.623579]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "476 [D loss: 0.037940, acc.: 99.95%] [G loss: 3.593106]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "477 [D loss: 0.038309, acc.: 100.00%] [G loss: 3.610615]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "478 [D loss: 0.038656, acc.: 100.00%] [G loss: 3.627957]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "479 [D loss: 0.036699, acc.: 100.00%] [G loss: 3.631145]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "480 [D loss: 0.038821, acc.: 100.00%] [G loss: 3.625693]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "481 [D loss: 0.040215, acc.: 100.00%] [G loss: 3.648330]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "482 [D loss: 0.037496, acc.: 100.00%] [G loss: 3.636764]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "483 [D loss: 0.039769, acc.: 99.95%] [G loss: 3.623134]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "484 [D loss: 0.039857, acc.: 99.95%] [G loss: 3.635388]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "485 [D loss: 0.041696, acc.: 99.90%] [G loss: 3.643803]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "486 [D loss: 0.038428, acc.: 100.00%] [G loss: 3.683018]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "487 [D loss: 0.036878, acc.: 100.00%] [G loss: 3.670475]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "488 [D loss: 0.037863, acc.: 100.00%] [G loss: 3.640362]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "489 [D loss: 0.041969, acc.: 99.85%] [G loss: 3.656180]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "490 [D loss: 0.043071, acc.: 99.90%] [G loss: 3.683313]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "491 [D loss: 0.042177, acc.: 99.90%] [G loss: 3.652309]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "492 [D loss: 0.041192, acc.: 100.00%] [G loss: 3.695520]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "493 [D loss: 0.040443, acc.: 100.00%] [G loss: 3.701643]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "494 [D loss: 0.041139, acc.: 99.85%] [G loss: 3.691943]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "495 [D loss: 0.041327, acc.: 100.00%] [G loss: 3.677495]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "496 [D loss: 0.041339, acc.: 99.90%] [G loss: 3.686932]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "497 [D loss: 0.042470, acc.: 99.95%] [G loss: 3.705614]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "498 [D loss: 0.044952, acc.: 99.95%] [G loss: 3.747084]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "499 [D loss: 0.041838, acc.: 100.00%] [G loss: 3.675231]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "500 [D loss: 0.041813, acc.: 99.90%] [G loss: 3.664542]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "501 [D loss: 0.041980, acc.: 99.90%] [G loss: 3.714461]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "502 [D loss: 0.045355, acc.: 99.90%] [G loss: 3.707721]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "503 [D loss: 0.048398, acc.: 99.80%] [G loss: 3.681463]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "504 [D loss: 0.044127, acc.: 99.90%] [G loss: 3.709734]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "505 [D loss: 0.043517, acc.: 99.95%] [G loss: 3.711939]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "506 [D loss: 0.047537, acc.: 99.85%] [G loss: 3.713614]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "507 [D loss: 0.046954, acc.: 99.90%] [G loss: 3.744855]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "508 [D loss: 0.043536, acc.: 99.95%] [G loss: 3.753474]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "509 [D loss: 0.048011, acc.: 99.80%] [G loss: 3.739260]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "510 [D loss: 0.052224, acc.: 99.85%] [G loss: 3.734199]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "511 [D loss: 0.047476, acc.: 99.90%] [G loss: 3.727755]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "512 [D loss: 0.050994, acc.: 99.76%] [G loss: 3.747584]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "513 [D loss: 0.045799, acc.: 99.90%] [G loss: 3.721127]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "514 [D loss: 0.050270, acc.: 99.76%] [G loss: 3.736770]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "515 [D loss: 0.049166, acc.: 99.85%] [G loss: 3.771800]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "516 [D loss: 0.049004, acc.: 99.85%] [G loss: 3.748198]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "517 [D loss: 0.049034, acc.: 99.85%] [G loss: 3.794537]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "518 [D loss: 0.050162, acc.: 99.80%] [G loss: 3.755473]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "519 [D loss: 0.051632, acc.: 99.85%] [G loss: 3.777804]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "520 [D loss: 0.053591, acc.: 99.80%] [G loss: 3.704587]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "521 [D loss: 0.054345, acc.: 99.66%] [G loss: 3.785098]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "522 [D loss: 0.058195, acc.: 99.56%] [G loss: 3.766278]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "523 [D loss: 0.055745, acc.: 99.76%] [G loss: 3.764009]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "524 [D loss: 0.055011, acc.: 99.71%] [G loss: 3.788987]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "525 [D loss: 0.058835, acc.: 99.66%] [G loss: 3.769027]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "526 [D loss: 0.066865, acc.: 99.51%] [G loss: 3.862596]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "527 [D loss: 0.058887, acc.: 99.71%] [G loss: 3.843772]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "528 [D loss: 0.068258, acc.: 99.32%] [G loss: 3.846128]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "529 [D loss: 0.061731, acc.: 99.32%] [G loss: 3.865249]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "530 [D loss: 0.066619, acc.: 99.17%] [G loss: 3.847231]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "531 [D loss: 0.065090, acc.: 99.37%] [G loss: 3.823991]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "532 [D loss: 0.063251, acc.: 99.56%] [G loss: 3.855424]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "533 [D loss: 0.062694, acc.: 99.56%] [G loss: 3.842277]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "534 [D loss: 0.066058, acc.: 99.51%] [G loss: 3.830088]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "535 [D loss: 0.067512, acc.: 99.22%] [G loss: 3.879503]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "536 [D loss: 0.074773, acc.: 98.93%] [G loss: 3.867342]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "537 [D loss: 0.069479, acc.: 99.07%] [G loss: 3.902100]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "538 [D loss: 0.069797, acc.: 99.46%] [G loss: 3.888423]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "539 [D loss: 0.085949, acc.: 98.68%] [G loss: 3.953615]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "540 [D loss: 0.076280, acc.: 99.46%] [G loss: 3.918712]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "541 [D loss: 0.082811, acc.: 98.54%] [G loss: 3.909388]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "542 [D loss: 0.077261, acc.: 99.17%] [G loss: 3.947147]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "543 [D loss: 0.080925, acc.: 99.02%] [G loss: 3.916066]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "544 [D loss: 0.089663, acc.: 98.19%] [G loss: 3.973485]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "545 [D loss: 0.095327, acc.: 98.10%] [G loss: 4.037846]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "546 [D loss: 0.105985, acc.: 97.80%] [G loss: 3.915939]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "547 [D loss: 0.085056, acc.: 98.58%] [G loss: 3.957188]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "548 [D loss: 0.094913, acc.: 98.14%] [G loss: 4.011683]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "549 [D loss: 0.104171, acc.: 97.46%] [G loss: 4.102616]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "550 [D loss: 0.113384, acc.: 97.71%] [G loss: 4.025449]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "551 [D loss: 0.103518, acc.: 97.90%] [G loss: 4.019551]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "552 [D loss: 0.110509, acc.: 97.75%] [G loss: 4.127126]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "553 [D loss: 0.127093, acc.: 96.44%] [G loss: 4.080231]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "554 [D loss: 0.120336, acc.: 96.92%] [G loss: 4.060870]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "555 [D loss: 0.129123, acc.: 96.53%] [G loss: 4.051252]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "556 [D loss: 0.128748, acc.: 96.53%] [G loss: 4.103737]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "557 [D loss: 0.129714, acc.: 96.53%] [G loss: 4.206984]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "558 [D loss: 0.178448, acc.: 93.65%] [G loss: 4.181612]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "559 [D loss: 0.158713, acc.: 95.07%] [G loss: 4.215291]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "560 [D loss: 0.170603, acc.: 93.85%] [G loss: 4.230911]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "561 [D loss: 0.180240, acc.: 93.99%] [G loss: 4.197051]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "562 [D loss: 0.157978, acc.: 95.12%] [G loss: 4.232257]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "563 [D loss: 0.183982, acc.: 93.75%] [G loss: 4.329036]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "564 [D loss: 0.248860, acc.: 90.72%] [G loss: 4.528690]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "565 [D loss: 0.348801, acc.: 84.52%] [G loss: 4.486016]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "566 [D loss: 0.300448, acc.: 86.96%] [G loss: 4.588920]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "567 [D loss: 0.321714, acc.: 87.11%] [G loss: 4.473195]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "568 [D loss: 0.301850, acc.: 87.55%] [G loss: 4.592330]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "569 [D loss: 0.360555, acc.: 84.72%] [G loss: 4.788732]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "570 [D loss: 0.493732, acc.: 79.15%] [G loss: 4.602546]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "571 [D loss: 0.306944, acc.: 87.40%] [G loss: 4.557172]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "572 [D loss: 0.342037, acc.: 86.08%] [G loss: 4.623289]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "573 [D loss: 0.397542, acc.: 82.71%] [G loss: 4.503500]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "574 [D loss: 0.389842, acc.: 82.62%] [G loss: 4.711514]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "575 [D loss: 0.534026, acc.: 78.81%] [G loss: 4.474720]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "576 [D loss: 0.411281, acc.: 82.96%] [G loss: 4.683730]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "577 [D loss: 0.573009, acc.: 76.32%] [G loss: 4.396232]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "578 [D loss: 0.352337, acc.: 85.99%] [G loss: 4.519820]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "579 [D loss: 0.615337, acc.: 74.90%] [G loss: 4.308767]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "580 [D loss: 0.455272, acc.: 81.30%] [G loss: 4.590136]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "581 [D loss: 0.827624, acc.: 69.68%] [G loss: 4.128138]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "582 [D loss: 0.399956, acc.: 83.54%] [G loss: 4.447233]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "583 [D loss: 0.837262, acc.: 68.46%] [G loss: 4.110761]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "584 [D loss: 0.499756, acc.: 80.13%] [G loss: 4.152333]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "585 [D loss: 0.673037, acc.: 74.07%] [G loss: 4.106552]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "586 [D loss: 0.729119, acc.: 71.83%] [G loss: 4.117173]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "587 [D loss: 0.790929, acc.: 70.46%] [G loss: 3.847795]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "588 [D loss: 0.608988, acc.: 75.24%] [G loss: 3.820121]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "589 [D loss: 0.593595, acc.: 75.68%] [G loss: 3.874709]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "590 [D loss: 0.837234, acc.: 68.55%] [G loss: 3.638740]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "591 [D loss: 0.651030, acc.: 74.66%] [G loss: 3.522802]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "592 [D loss: 0.650977, acc.: 73.88%] [G loss: 3.565499]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "593 [D loss: 0.820905, acc.: 68.51%] [G loss: 3.587643]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "594 [D loss: 0.878395, acc.: 67.19%] [G loss: 3.143221]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "595 [D loss: 0.534466, acc.: 77.93%] [G loss: 3.489877]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "596 [D loss: 0.994172, acc.: 61.91%] [G loss: 2.809342]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "597 [D loss: 0.505490, acc.: 79.35%] [G loss: 3.566401]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "598 [D loss: 1.270200, acc.: 52.73%] [G loss: 2.327180]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "599 [D loss: 0.402874, acc.: 82.71%] [G loss: 3.122504]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "600 [D loss: 0.799870, acc.: 67.82%] [G loss: 2.657432]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "601 [D loss: 0.558560, acc.: 76.86%] [G loss: 3.013013]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "602 [D loss: 0.821407, acc.: 64.94%] [G loss: 2.563773]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "603 [D loss: 0.566187, acc.: 75.83%] [G loss: 2.793761]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "604 [D loss: 0.702302, acc.: 70.61%] [G loss: 2.596194]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "605 [D loss: 0.621317, acc.: 73.00%] [G loss: 2.617974]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "606 [D loss: 0.765761, acc.: 66.70%] [G loss: 2.505656]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "607 [D loss: 0.705952, acc.: 69.78%] [G loss: 2.467481]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "608 [D loss: 0.689035, acc.: 70.12%] [G loss: 2.392450]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "609 [D loss: 0.679206, acc.: 69.63%] [G loss: 2.392133]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "610 [D loss: 0.700661, acc.: 68.75%] [G loss: 2.276374]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "611 [D loss: 0.632060, acc.: 71.88%] [G loss: 2.341385]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "612 [D loss: 0.728032, acc.: 67.33%] [G loss: 2.175392]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "613 [D loss: 0.612175, acc.: 72.51%] [G loss: 2.164051]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "614 [D loss: 0.636559, acc.: 70.46%] [G loss: 2.126197]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "615 [D loss: 0.653217, acc.: 70.26%] [G loss: 2.190588]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "616 [D loss: 0.662430, acc.: 69.68%] [G loss: 2.036879]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "617 [D loss: 0.631323, acc.: 70.90%] [G loss: 2.046926]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "618 [D loss: 0.623825, acc.: 71.97%] [G loss: 1.982710]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "619 [D loss: 0.612894, acc.: 71.83%] [G loss: 2.012744]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "620 [D loss: 0.638198, acc.: 71.29%] [G loss: 1.986366]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "621 [D loss: 0.655005, acc.: 69.97%] [G loss: 1.965067]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "622 [D loss: 0.657290, acc.: 69.19%] [G loss: 1.934726]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "623 [D loss: 0.640363, acc.: 69.97%] [G loss: 1.912459]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "624 [D loss: 0.627020, acc.: 68.90%] [G loss: 1.904849]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "625 [D loss: 0.656089, acc.: 68.07%] [G loss: 1.847883]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "626 [D loss: 0.602137, acc.: 71.29%] [G loss: 1.851433]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "627 [D loss: 0.632268, acc.: 69.04%] [G loss: 1.860596]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "628 [D loss: 0.626458, acc.: 70.95%] [G loss: 1.799033]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "629 [D loss: 0.611695, acc.: 70.75%] [G loss: 1.787584]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "630 [D loss: 0.625374, acc.: 70.65%] [G loss: 1.770289]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "631 [D loss: 0.598394, acc.: 72.27%] [G loss: 1.729341]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "632 [D loss: 0.594095, acc.: 71.04%] [G loss: 1.734511]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "633 [D loss: 0.591939, acc.: 71.58%] [G loss: 1.723406]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "634 [D loss: 0.600105, acc.: 71.00%] [G loss: 1.710150]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "635 [D loss: 0.573475, acc.: 72.66%] [G loss: 1.722419]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "636 [D loss: 0.613141, acc.: 70.56%] [G loss: 1.686085]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "637 [D loss: 0.601216, acc.: 71.63%] [G loss: 1.699168]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "638 [D loss: 0.606160, acc.: 70.17%] [G loss: 1.667317]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "639 [D loss: 0.577308, acc.: 72.17%] [G loss: 1.669838]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "640 [D loss: 0.611534, acc.: 70.17%] [G loss: 1.654559]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "641 [D loss: 0.636571, acc.: 68.41%] [G loss: 1.657315]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "642 [D loss: 0.612924, acc.: 68.95%] [G loss: 1.606538]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "643 [D loss: 0.616496, acc.: 68.36%] [G loss: 1.664809]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "644 [D loss: 0.596063, acc.: 71.00%] [G loss: 1.585519]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "645 [D loss: 0.568317, acc.: 72.51%] [G loss: 1.595195]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "646 [D loss: 0.606661, acc.: 70.26%] [G loss: 1.587774]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "647 [D loss: 0.582652, acc.: 70.75%] [G loss: 1.626621]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "648 [D loss: 0.590224, acc.: 70.90%] [G loss: 1.607760]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "649 [D loss: 0.595325, acc.: 70.31%] [G loss: 1.581037]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "650 [D loss: 0.597663, acc.: 69.53%] [G loss: 1.565776]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "651 [D loss: 0.593204, acc.: 71.48%] [G loss: 1.546634]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "652 [D loss: 0.578924, acc.: 71.53%] [G loss: 1.573758]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "653 [D loss: 0.587189, acc.: 70.51%] [G loss: 1.540094]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "654 [D loss: 0.586479, acc.: 69.82%] [G loss: 1.533678]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "655 [D loss: 0.603787, acc.: 68.12%] [G loss: 1.549375]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "656 [D loss: 0.595872, acc.: 69.53%] [G loss: 1.537098]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "657 [D loss: 0.596020, acc.: 69.34%] [G loss: 1.526543]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "658 [D loss: 0.596559, acc.: 69.63%] [G loss: 1.512521]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "659 [D loss: 0.600118, acc.: 69.34%] [G loss: 1.539995]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "660 [D loss: 0.585723, acc.: 69.97%] [G loss: 1.500925]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "661 [D loss: 0.591802, acc.: 69.38%] [G loss: 1.489089]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "662 [D loss: 0.587856, acc.: 68.75%] [G loss: 1.504223]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "663 [D loss: 0.594071, acc.: 70.02%] [G loss: 1.517085]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "664 [D loss: 0.580388, acc.: 69.24%] [G loss: 1.517386]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "665 [D loss: 0.569034, acc.: 70.26%] [G loss: 1.479753]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "666 [D loss: 0.586180, acc.: 70.56%] [G loss: 1.479388]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "667 [D loss: 0.590123, acc.: 68.90%] [G loss: 1.475351]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "668 [D loss: 0.597669, acc.: 68.26%] [G loss: 1.474548]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "669 [D loss: 0.578370, acc.: 69.87%] [G loss: 1.463180]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "670 [D loss: 0.576912, acc.: 69.97%] [G loss: 1.461553]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "671 [D loss: 0.580550, acc.: 70.21%] [G loss: 1.445798]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "672 [D loss: 0.590235, acc.: 69.68%] [G loss: 1.461646]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "673 [D loss: 0.593549, acc.: 69.68%] [G loss: 1.486807]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "674 [D loss: 0.571470, acc.: 69.78%] [G loss: 1.439283]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "675 [D loss: 0.575629, acc.: 69.38%] [G loss: 1.445849]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "676 [D loss: 0.576871, acc.: 69.63%] [G loss: 1.449709]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "677 [D loss: 0.601952, acc.: 67.77%] [G loss: 1.481620]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "678 [D loss: 0.589989, acc.: 68.65%] [G loss: 1.434431]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "679 [D loss: 0.580991, acc.: 69.97%] [G loss: 1.431931]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "680 [D loss: 0.587056, acc.: 68.55%] [G loss: 1.424864]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "681 [D loss: 0.590498, acc.: 67.29%] [G loss: 1.440934]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "682 [D loss: 0.583024, acc.: 69.78%] [G loss: 1.418802]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "683 [D loss: 0.559153, acc.: 70.12%] [G loss: 1.409426]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "684 [D loss: 0.583828, acc.: 67.87%] [G loss: 1.388241]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "685 [D loss: 0.568607, acc.: 70.90%] [G loss: 1.401394]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "686 [D loss: 0.543932, acc.: 70.56%] [G loss: 1.395432]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "687 [D loss: 0.584818, acc.: 67.92%] [G loss: 1.405586]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "688 [D loss: 0.563809, acc.: 70.80%] [G loss: 1.396896]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "689 [D loss: 0.592255, acc.: 67.14%] [G loss: 1.386946]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "690 [D loss: 0.583387, acc.: 68.85%] [G loss: 1.410750]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "691 [D loss: 0.569516, acc.: 69.63%] [G loss: 1.382924]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "692 [D loss: 0.572197, acc.: 68.51%] [G loss: 1.390377]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "693 [D loss: 0.565508, acc.: 69.73%] [G loss: 1.401514]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "694 [D loss: 0.566596, acc.: 69.48%] [G loss: 1.394417]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "695 [D loss: 0.570487, acc.: 69.04%] [G loss: 1.391170]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "696 [D loss: 0.552216, acc.: 71.24%] [G loss: 1.365329]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "697 [D loss: 0.570396, acc.: 69.14%] [G loss: 1.371250]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "698 [D loss: 0.568791, acc.: 69.04%] [G loss: 1.362653]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "699 [D loss: 0.587034, acc.: 67.33%] [G loss: 1.382630]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "700 [D loss: 0.583627, acc.: 66.89%] [G loss: 1.374612]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "701 [D loss: 0.562146, acc.: 70.02%] [G loss: 1.364180]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "702 [D loss: 0.579462, acc.: 68.31%] [G loss: 1.349014]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "703 [D loss: 0.578858, acc.: 68.02%] [G loss: 1.348030]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "704 [D loss: 0.544649, acc.: 71.19%] [G loss: 1.362633]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "705 [D loss: 0.576954, acc.: 67.77%] [G loss: 1.352393]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "706 [D loss: 0.565959, acc.: 68.55%] [G loss: 1.361809]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "707 [D loss: 0.570304, acc.: 68.26%] [G loss: 1.361761]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "708 [D loss: 0.574664, acc.: 68.65%] [G loss: 1.354027]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "709 [D loss: 0.564442, acc.: 69.73%] [G loss: 1.357147]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "710 [D loss: 0.580404, acc.: 67.48%] [G loss: 1.339322]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "711 [D loss: 0.548016, acc.: 70.36%] [G loss: 1.360385]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "712 [D loss: 0.572962, acc.: 69.04%] [G loss: 1.328141]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "713 [D loss: 0.563409, acc.: 68.16%] [G loss: 1.347309]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "714 [D loss: 0.558590, acc.: 69.68%] [G loss: 1.332967]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "715 [D loss: 0.576028, acc.: 67.19%] [G loss: 1.313325]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "716 [D loss: 0.556974, acc.: 69.29%] [G loss: 1.345342]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "717 [D loss: 0.549835, acc.: 70.51%] [G loss: 1.334405]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "718 [D loss: 0.553369, acc.: 71.00%] [G loss: 1.309508]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "719 [D loss: 0.584848, acc.: 66.94%] [G loss: 1.350930]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "720 [D loss: 0.547728, acc.: 69.82%] [G loss: 1.306776]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "721 [D loss: 0.572089, acc.: 67.33%] [G loss: 1.319489]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "722 [D loss: 0.560281, acc.: 69.87%] [G loss: 1.343645]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "723 [D loss: 0.543967, acc.: 71.39%] [G loss: 1.320788]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "724 [D loss: 0.580480, acc.: 68.51%] [G loss: 1.302309]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "725 [D loss: 0.552449, acc.: 70.70%] [G loss: 1.308625]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "726 [D loss: 0.557865, acc.: 69.09%] [G loss: 1.320809]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "727 [D loss: 0.548947, acc.: 70.12%] [G loss: 1.324276]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "728 [D loss: 0.567248, acc.: 68.90%] [G loss: 1.322954]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "729 [D loss: 0.560952, acc.: 69.04%] [G loss: 1.314013]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "730 [D loss: 0.566480, acc.: 68.21%] [G loss: 1.317164]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "731 [D loss: 0.555279, acc.: 70.41%] [G loss: 1.311084]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "732 [D loss: 0.545799, acc.: 69.48%] [G loss: 1.300418]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "733 [D loss: 0.558527, acc.: 68.95%] [G loss: 1.302948]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "734 [D loss: 0.567345, acc.: 68.36%] [G loss: 1.296124]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "735 [D loss: 0.539783, acc.: 71.24%] [G loss: 1.307659]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "736 [D loss: 0.566020, acc.: 68.80%] [G loss: 1.297453]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "737 [D loss: 0.554461, acc.: 69.14%] [G loss: 1.304353]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "738 [D loss: 0.580881, acc.: 66.80%] [G loss: 1.301132]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "739 [D loss: 0.560783, acc.: 69.38%] [G loss: 1.305223]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "740 [D loss: 0.560697, acc.: 68.60%] [G loss: 1.307530]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "741 [D loss: 0.571175, acc.: 67.14%] [G loss: 1.299235]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "742 [D loss: 0.570475, acc.: 68.95%] [G loss: 1.314206]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "743 [D loss: 0.569283, acc.: 67.53%] [G loss: 1.302348]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "744 [D loss: 0.553923, acc.: 69.78%] [G loss: 1.297990]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "745 [D loss: 0.564851, acc.: 69.53%] [G loss: 1.290502]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "746 [D loss: 0.549185, acc.: 69.97%] [G loss: 1.285232]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "747 [D loss: 0.566042, acc.: 67.92%] [G loss: 1.287422]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "748 [D loss: 0.558216, acc.: 68.46%] [G loss: 1.271747]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "749 [D loss: 0.554718, acc.: 68.75%] [G loss: 1.281661]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "750 [D loss: 0.555602, acc.: 69.48%] [G loss: 1.272953]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "751 [D loss: 0.571732, acc.: 68.26%] [G loss: 1.268201]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "752 [D loss: 0.567149, acc.: 67.92%] [G loss: 1.272901]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "753 [D loss: 0.565842, acc.: 68.80%] [G loss: 1.261925]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "754 [D loss: 0.545436, acc.: 70.07%] [G loss: 1.283258]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "755 [D loss: 0.564291, acc.: 68.21%] [G loss: 1.276955]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "756 [D loss: 0.567375, acc.: 68.46%] [G loss: 1.284754]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "757 [D loss: 0.553687, acc.: 69.04%] [G loss: 1.280719]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "758 [D loss: 0.569350, acc.: 66.89%] [G loss: 1.301369]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "759 [D loss: 0.572028, acc.: 67.33%] [G loss: 1.273487]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "760 [D loss: 0.571360, acc.: 68.70%] [G loss: 1.248703]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "761 [D loss: 0.546502, acc.: 70.07%] [G loss: 1.269552]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "762 [D loss: 0.578735, acc.: 67.43%] [G loss: 1.254365]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "763 [D loss: 0.564065, acc.: 69.19%] [G loss: 1.264683]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "764 [D loss: 0.565984, acc.: 68.51%] [G loss: 1.270879]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "765 [D loss: 0.565870, acc.: 68.31%] [G loss: 1.254682]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "766 [D loss: 0.559093, acc.: 69.38%] [G loss: 1.242398]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "767 [D loss: 0.569117, acc.: 68.26%] [G loss: 1.241120]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "768 [D loss: 0.570027, acc.: 66.65%] [G loss: 1.230248]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "769 [D loss: 0.558996, acc.: 68.95%] [G loss: 1.239015]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "770 [D loss: 0.565497, acc.: 68.41%] [G loss: 1.251509]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "771 [D loss: 0.570573, acc.: 67.72%] [G loss: 1.243761]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "772 [D loss: 0.575906, acc.: 66.46%] [G loss: 1.259479]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "773 [D loss: 0.562128, acc.: 69.34%] [G loss: 1.250418]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "774 [D loss: 0.577140, acc.: 67.19%] [G loss: 1.247891]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "775 [D loss: 0.578647, acc.: 66.70%] [G loss: 1.231702]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "776 [D loss: 0.567954, acc.: 66.85%] [G loss: 1.222293]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "777 [D loss: 0.554958, acc.: 69.73%] [G loss: 1.244525]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "778 [D loss: 0.563067, acc.: 68.85%] [G loss: 1.242034]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "779 [D loss: 0.580174, acc.: 66.75%] [G loss: 1.240551]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "780 [D loss: 0.580919, acc.: 67.33%] [G loss: 1.219774]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "781 [D loss: 0.558567, acc.: 69.38%] [G loss: 1.209551]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "782 [D loss: 0.561353, acc.: 69.24%] [G loss: 1.230867]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "783 [D loss: 0.580489, acc.: 65.82%] [G loss: 1.222117]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "784 [D loss: 0.591348, acc.: 64.01%] [G loss: 1.216977]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "785 [D loss: 0.580471, acc.: 66.31%] [G loss: 1.207281]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "786 [D loss: 0.573918, acc.: 66.75%] [G loss: 1.211473]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "787 [D loss: 0.578119, acc.: 65.62%] [G loss: 1.211952]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "788 [D loss: 0.563386, acc.: 68.99%] [G loss: 1.224770]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "789 [D loss: 0.569727, acc.: 66.70%] [G loss: 1.216237]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "790 [D loss: 0.575494, acc.: 67.68%] [G loss: 1.217238]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "791 [D loss: 0.577438, acc.: 66.65%] [G loss: 1.194251]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "792 [D loss: 0.580637, acc.: 67.09%] [G loss: 1.187105]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "793 [D loss: 0.579223, acc.: 67.04%] [G loss: 1.202443]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "794 [D loss: 0.580206, acc.: 66.50%] [G loss: 1.200265]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "795 [D loss: 0.584187, acc.: 66.31%] [G loss: 1.193119]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "796 [D loss: 0.574787, acc.: 66.94%] [G loss: 1.187755]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "797 [D loss: 0.571169, acc.: 68.31%] [G loss: 1.207204]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "798 [D loss: 0.577554, acc.: 67.72%] [G loss: 1.199080]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "799 [D loss: 0.557564, acc.: 69.14%] [G loss: 1.188499]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "800 [D loss: 0.575335, acc.: 67.43%] [G loss: 1.181341]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "801 [D loss: 0.568788, acc.: 68.75%] [G loss: 1.189668]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "802 [D loss: 0.581075, acc.: 67.14%] [G loss: 1.177911]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "803 [D loss: 0.582928, acc.: 65.33%] [G loss: 1.171475]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "804 [D loss: 0.557114, acc.: 70.51%] [G loss: 1.192315]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "805 [D loss: 0.576688, acc.: 67.53%] [G loss: 1.163816]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "806 [D loss: 0.580464, acc.: 66.06%] [G loss: 1.172964]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "807 [D loss: 0.582568, acc.: 65.67%] [G loss: 1.169985]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "808 [D loss: 0.581745, acc.: 67.24%] [G loss: 1.157949]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "809 [D loss: 0.568194, acc.: 67.87%] [G loss: 1.169535]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "810 [D loss: 0.576102, acc.: 67.19%] [G loss: 1.178177]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "811 [D loss: 0.579479, acc.: 67.24%] [G loss: 1.168157]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "812 [D loss: 0.573553, acc.: 67.43%] [G loss: 1.159334]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "813 [D loss: 0.576988, acc.: 67.29%] [G loss: 1.162403]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "814 [D loss: 0.571091, acc.: 67.87%] [G loss: 1.164817]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "815 [D loss: 0.575516, acc.: 67.72%] [G loss: 1.146755]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "816 [D loss: 0.578047, acc.: 66.89%] [G loss: 1.161075]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "817 [D loss: 0.576460, acc.: 68.02%] [G loss: 1.160358]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "818 [D loss: 0.581622, acc.: 67.77%] [G loss: 1.158679]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "819 [D loss: 0.577186, acc.: 67.77%] [G loss: 1.142933]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "820 [D loss: 0.583948, acc.: 66.75%] [G loss: 1.140621]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "821 [D loss: 0.582041, acc.: 67.48%] [G loss: 1.135697]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "822 [D loss: 0.574640, acc.: 68.41%] [G loss: 1.145355]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "823 [D loss: 0.579130, acc.: 67.43%] [G loss: 1.140266]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "824 [D loss: 0.566696, acc.: 69.29%] [G loss: 1.135372]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "825 [D loss: 0.572573, acc.: 67.24%] [G loss: 1.152067]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "826 [D loss: 0.584840, acc.: 66.94%] [G loss: 1.151168]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "827 [D loss: 0.594590, acc.: 64.70%] [G loss: 1.129504]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "828 [D loss: 0.580341, acc.: 67.53%] [G loss: 1.121290]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "829 [D loss: 0.580023, acc.: 67.72%] [G loss: 1.120052]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "830 [D loss: 0.579440, acc.: 67.43%] [G loss: 1.130511]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "831 [D loss: 0.574677, acc.: 68.55%] [G loss: 1.131596]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "832 [D loss: 0.575553, acc.: 67.53%] [G loss: 1.138250]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "833 [D loss: 0.577642, acc.: 67.72%] [G loss: 1.140518]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "834 [D loss: 0.579724, acc.: 67.09%] [G loss: 1.135018]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "835 [D loss: 0.584767, acc.: 66.70%] [G loss: 1.128247]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "836 [D loss: 0.582035, acc.: 67.43%] [G loss: 1.120474]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "837 [D loss: 0.585396, acc.: 66.02%] [G loss: 1.117149]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "838 [D loss: 0.577636, acc.: 67.09%] [G loss: 1.117280]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "839 [D loss: 0.587192, acc.: 66.36%] [G loss: 1.133450]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "840 [D loss: 0.581203, acc.: 67.87%] [G loss: 1.131247]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "841 [D loss: 0.589098, acc.: 67.38%] [G loss: 1.123270]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "842 [D loss: 0.589330, acc.: 66.80%] [G loss: 1.111572]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "843 [D loss: 0.587996, acc.: 67.19%] [G loss: 1.112810]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "844 [D loss: 0.573467, acc.: 69.53%] [G loss: 1.112084]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "845 [D loss: 0.585134, acc.: 66.80%] [G loss: 1.105398]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "846 [D loss: 0.582753, acc.: 67.92%] [G loss: 1.111876]\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "847 [D loss: 0.574193, acc.: 69.14%] [G loss: 1.118834]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "848 [D loss: 0.586672, acc.: 66.65%] [G loss: 1.104100]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "849 [D loss: 0.588311, acc.: 66.75%] [G loss: 1.107875]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "850 [D loss: 0.589166, acc.: 66.16%] [G loss: 1.097926]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "851 [D loss: 0.590090, acc.: 66.41%] [G loss: 1.107904]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "852 [D loss: 0.599258, acc.: 64.89%] [G loss: 1.099146]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "853 [D loss: 0.593321, acc.: 66.16%] [G loss: 1.113227]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "854 [D loss: 0.589466, acc.: 66.85%] [G loss: 1.114450]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "855 [D loss: 0.594561, acc.: 65.53%] [G loss: 1.104159]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "856 [D loss: 0.591760, acc.: 66.94%] [G loss: 1.110438]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "857 [D loss: 0.585517, acc.: 67.92%] [G loss: 1.098191]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "858 [D loss: 0.595443, acc.: 65.67%] [G loss: 1.091372]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "859 [D loss: 0.594247, acc.: 66.75%] [G loss: 1.095463]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "860 [D loss: 0.591909, acc.: 66.55%] [G loss: 1.095208]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "861 [D loss: 0.578996, acc.: 67.58%] [G loss: 1.095402]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "862 [D loss: 0.595342, acc.: 66.41%] [G loss: 1.087250]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "863 [D loss: 0.589429, acc.: 65.92%] [G loss: 1.087242]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "864 [D loss: 0.586443, acc.: 66.50%] [G loss: 1.086267]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "865 [D loss: 0.590574, acc.: 67.24%] [G loss: 1.086892]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "866 [D loss: 0.589946, acc.: 67.38%] [G loss: 1.095297]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "867 [D loss: 0.590190, acc.: 66.26%] [G loss: 1.071006]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "868 [D loss: 0.593016, acc.: 65.67%] [G loss: 1.095062]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "869 [D loss: 0.600057, acc.: 65.38%] [G loss: 1.075183]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "870 [D loss: 0.586154, acc.: 67.14%] [G loss: 1.096870]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "871 [D loss: 0.602351, acc.: 65.48%] [G loss: 1.082240]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "872 [D loss: 0.594949, acc.: 65.87%] [G loss: 1.077557]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "873 [D loss: 0.588768, acc.: 68.07%] [G loss: 1.083806]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "874 [D loss: 0.591385, acc.: 66.41%] [G loss: 1.086406]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "875 [D loss: 0.595452, acc.: 67.77%] [G loss: 1.068291]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "876 [D loss: 0.601241, acc.: 65.72%] [G loss: 1.063647]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "877 [D loss: 0.595416, acc.: 67.09%] [G loss: 1.052662]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "878 [D loss: 0.593745, acc.: 66.55%] [G loss: 1.069938]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "879 [D loss: 0.594021, acc.: 66.46%] [G loss: 1.061371]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "880 [D loss: 0.592692, acc.: 66.36%] [G loss: 1.070920]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "881 [D loss: 0.598202, acc.: 65.72%] [G loss: 1.056616]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "882 [D loss: 0.597783, acc.: 66.65%] [G loss: 1.046996]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "883 [D loss: 0.598882, acc.: 66.89%] [G loss: 1.059799]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "884 [D loss: 0.595193, acc.: 66.89%] [G loss: 1.065190]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "885 [D loss: 0.592749, acc.: 67.72%] [G loss: 1.063141]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "886 [D loss: 0.594639, acc.: 67.48%] [G loss: 1.054406]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "887 [D loss: 0.601547, acc.: 64.84%] [G loss: 1.056755]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "888 [D loss: 0.597498, acc.: 66.06%] [G loss: 1.057396]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "889 [D loss: 0.599626, acc.: 65.72%] [G loss: 1.052284]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "890 [D loss: 0.598480, acc.: 67.14%] [G loss: 1.052113]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "891 [D loss: 0.599120, acc.: 66.41%] [G loss: 1.064462]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "892 [D loss: 0.594883, acc.: 66.55%] [G loss: 1.058313]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "893 [D loss: 0.599975, acc.: 67.19%] [G loss: 1.050904]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "894 [D loss: 0.595780, acc.: 68.60%] [G loss: 1.049863]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "895 [D loss: 0.595217, acc.: 68.07%] [G loss: 1.048131]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "896 [D loss: 0.604603, acc.: 64.75%] [G loss: 1.041665]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "897 [D loss: 0.597527, acc.: 65.72%] [G loss: 1.040947]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "898 [D loss: 0.602427, acc.: 65.97%] [G loss: 1.042138]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "899 [D loss: 0.599932, acc.: 66.11%] [G loss: 1.046150]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "900 [D loss: 0.614029, acc.: 64.21%] [G loss: 1.031629]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "901 [D loss: 0.601458, acc.: 66.21%] [G loss: 1.021719]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "902 [D loss: 0.593758, acc.: 67.63%] [G loss: 1.029751]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "903 [D loss: 0.608485, acc.: 65.33%] [G loss: 1.036404]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "904 [D loss: 0.603040, acc.: 65.43%] [G loss: 1.037449]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "905 [D loss: 0.602816, acc.: 65.72%] [G loss: 1.030544]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "906 [D loss: 0.607629, acc.: 65.14%] [G loss: 1.031337]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "907 [D loss: 0.597548, acc.: 67.72%] [G loss: 1.043015]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "908 [D loss: 0.596985, acc.: 67.82%] [G loss: 1.036741]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "909 [D loss: 0.607245, acc.: 66.41%] [G loss: 1.037746]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "910 [D loss: 0.618471, acc.: 63.53%] [G loss: 1.036359]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "911 [D loss: 0.597707, acc.: 66.50%] [G loss: 1.024102]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "912 [D loss: 0.605730, acc.: 66.11%] [G loss: 1.031236]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "913 [D loss: 0.610215, acc.: 65.87%] [G loss: 1.022689]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "914 [D loss: 0.607812, acc.: 65.82%] [G loss: 1.022126]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "915 [D loss: 0.603233, acc.: 66.11%] [G loss: 1.018801]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "916 [D loss: 0.603487, acc.: 66.65%] [G loss: 1.012065]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "917 [D loss: 0.609522, acc.: 65.72%] [G loss: 1.010147]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "918 [D loss: 0.609453, acc.: 65.14%] [G loss: 1.011949]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "919 [D loss: 0.605480, acc.: 65.92%] [G loss: 1.013133]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "920 [D loss: 0.605504, acc.: 66.16%] [G loss: 1.007619]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "921 [D loss: 0.601025, acc.: 66.80%] [G loss: 1.021922]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "922 [D loss: 0.605188, acc.: 66.55%] [G loss: 1.017505]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "923 [D loss: 0.610375, acc.: 65.23%] [G loss: 1.011925]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "924 [D loss: 0.610553, acc.: 64.99%] [G loss: 1.014467]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "925 [D loss: 0.617441, acc.: 64.11%] [G loss: 1.020145]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "926 [D loss: 0.610304, acc.: 64.65%] [G loss: 1.009257]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "927 [D loss: 0.614315, acc.: 64.11%] [G loss: 0.999836]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "928 [D loss: 0.610449, acc.: 65.14%] [G loss: 1.005399]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "929 [D loss: 0.617993, acc.: 63.72%] [G loss: 0.998612]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "930 [D loss: 0.607772, acc.: 65.43%] [G loss: 0.997474]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "931 [D loss: 0.603115, acc.: 66.11%] [G loss: 0.998902]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "932 [D loss: 0.605654, acc.: 65.72%] [G loss: 1.004882]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "933 [D loss: 0.612262, acc.: 65.23%] [G loss: 1.000225]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "934 [D loss: 0.616852, acc.: 63.62%] [G loss: 0.989956]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "935 [D loss: 0.610523, acc.: 66.36%] [G loss: 0.997252]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "936 [D loss: 0.612442, acc.: 65.67%] [G loss: 1.000259]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "937 [D loss: 0.621999, acc.: 64.16%] [G loss: 0.990238]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "938 [D loss: 0.613783, acc.: 65.23%] [G loss: 0.993850]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "939 [D loss: 0.620099, acc.: 64.06%] [G loss: 0.996846]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "940 [D loss: 0.607029, acc.: 66.94%] [G loss: 0.994590]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "941 [D loss: 0.615279, acc.: 64.65%] [G loss: 0.993115]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "942 [D loss: 0.624239, acc.: 62.50%] [G loss: 0.991699]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "943 [D loss: 0.621326, acc.: 62.55%] [G loss: 0.992345]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "944 [D loss: 0.620801, acc.: 63.18%] [G loss: 0.978512]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "945 [D loss: 0.614720, acc.: 65.38%] [G loss: 0.985722]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "946 [D loss: 0.617603, acc.: 64.45%] [G loss: 0.986671]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "947 [D loss: 0.618376, acc.: 64.84%] [G loss: 0.979570]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "948 [D loss: 0.610144, acc.: 65.53%] [G loss: 0.983801]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "949 [D loss: 0.620090, acc.: 65.53%] [G loss: 0.977926]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "950 [D loss: 0.620481, acc.: 66.06%] [G loss: 0.977502]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "951 [D loss: 0.622443, acc.: 63.82%] [G loss: 0.972976]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "952 [D loss: 0.620079, acc.: 64.84%] [G loss: 0.978293]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "953 [D loss: 0.617168, acc.: 64.11%] [G loss: 0.977573]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "954 [D loss: 0.612000, acc.: 66.21%] [G loss: 0.972560]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "955 [D loss: 0.625392, acc.: 63.53%] [G loss: 0.972696]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "956 [D loss: 0.619513, acc.: 64.26%] [G loss: 0.973226]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "957 [D loss: 0.619932, acc.: 63.96%] [G loss: 0.967390]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "958 [D loss: 0.616650, acc.: 65.48%] [G loss: 0.974305]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "959 [D loss: 0.620291, acc.: 64.65%] [G loss: 0.968691]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "960 [D loss: 0.628896, acc.: 62.84%] [G loss: 0.967361]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "961 [D loss: 0.621284, acc.: 64.21%] [G loss: 0.968258]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "962 [D loss: 0.619214, acc.: 64.31%] [G loss: 0.960294]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "963 [D loss: 0.620513, acc.: 64.40%] [G loss: 0.953264]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "964 [D loss: 0.630184, acc.: 61.38%] [G loss: 0.957654]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "965 [D loss: 0.630832, acc.: 62.26%] [G loss: 0.954195]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "966 [D loss: 0.618219, acc.: 65.43%] [G loss: 0.953920]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "967 [D loss: 0.616994, acc.: 65.28%] [G loss: 0.967393]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "968 [D loss: 0.630291, acc.: 63.77%] [G loss: 0.957450]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "969 [D loss: 0.630582, acc.: 61.62%] [G loss: 0.955837]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "970 [D loss: 0.626195, acc.: 62.45%] [G loss: 0.955900]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "971 [D loss: 0.621755, acc.: 62.74%] [G loss: 0.956048]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "972 [D loss: 0.623510, acc.: 64.06%] [G loss: 0.951614]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "973 [D loss: 0.624773, acc.: 63.28%] [G loss: 0.947869]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "974 [D loss: 0.628064, acc.: 62.99%] [G loss: 0.951090]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "975 [D loss: 0.628900, acc.: 62.50%] [G loss: 0.948783]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "976 [D loss: 0.624291, acc.: 63.77%] [G loss: 0.945319]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "977 [D loss: 0.618763, acc.: 65.09%] [G loss: 0.946299]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "978 [D loss: 0.625221, acc.: 63.57%] [G loss: 0.946713]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "979 [D loss: 0.620285, acc.: 64.36%] [G loss: 0.946587]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "980 [D loss: 0.627047, acc.: 63.72%] [G loss: 0.942312]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "981 [D loss: 0.635748, acc.: 62.16%] [G loss: 0.951470]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "982 [D loss: 0.631774, acc.: 62.35%] [G loss: 0.941923]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "983 [D loss: 0.628684, acc.: 63.13%] [G loss: 0.940406]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "984 [D loss: 0.626639, acc.: 62.60%] [G loss: 0.937621]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "985 [D loss: 0.628147, acc.: 63.18%] [G loss: 0.928848]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "986 [D loss: 0.631022, acc.: 62.74%] [G loss: 0.930000]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "987 [D loss: 0.630288, acc.: 61.87%] [G loss: 0.924157]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "988 [D loss: 0.627726, acc.: 62.60%] [G loss: 0.928081]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "989 [D loss: 0.635823, acc.: 60.69%] [G loss: 0.928101]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "990 [D loss: 0.623068, acc.: 64.11%] [G loss: 0.926074]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "991 [D loss: 0.637024, acc.: 60.99%] [G loss: 0.933753]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "992 [D loss: 0.633297, acc.: 62.70%] [G loss: 0.932510]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "993 [D loss: 0.637491, acc.: 61.57%] [G loss: 0.925812]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "994 [D loss: 0.630891, acc.: 62.89%] [G loss: 0.927278]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "995 [D loss: 0.631465, acc.: 62.65%] [G loss: 0.928509]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "996 [D loss: 0.633360, acc.: 61.52%] [G loss: 0.922014]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "997 [D loss: 0.634346, acc.: 62.50%] [G loss: 0.925248]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "998 [D loss: 0.629538, acc.: 64.31%] [G loss: 0.918589]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "999 [D loss: 0.632884, acc.: 61.72%] [G loss: 0.921882]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1000 [D loss: 0.631807, acc.: 62.01%] [G loss: 0.920357]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1001 [D loss: 0.623763, acc.: 65.77%] [G loss: 0.923493]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1002 [D loss: 0.630181, acc.: 63.09%] [G loss: 0.927530]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1003 [D loss: 0.627943, acc.: 63.92%] [G loss: 0.920912]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1004 [D loss: 0.633989, acc.: 64.16%] [G loss: 0.923283]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1005 [D loss: 0.632178, acc.: 63.28%] [G loss: 0.923807]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1006 [D loss: 0.639587, acc.: 61.47%] [G loss: 0.917909]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1007 [D loss: 0.633909, acc.: 61.82%] [G loss: 0.917012]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1008 [D loss: 0.633219, acc.: 62.35%] [G loss: 0.920037]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1009 [D loss: 0.636029, acc.: 61.57%] [G loss: 0.920116]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1010 [D loss: 0.641588, acc.: 60.21%] [G loss: 0.908520]\n",
            "32/32 [==============================] - 0s 4ms/step\n",
            "1011 [D loss: 0.638489, acc.: 61.67%] [G loss: 0.914997]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1012 [D loss: 0.642030, acc.: 60.79%] [G loss: 0.909588]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1013 [D loss: 0.632682, acc.: 63.48%] [G loss: 0.907748]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1014 [D loss: 0.637507, acc.: 61.52%] [G loss: 0.911320]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1015 [D loss: 0.632857, acc.: 63.72%] [G loss: 0.910685]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1016 [D loss: 0.631733, acc.: 63.18%] [G loss: 0.913595]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1017 [D loss: 0.631773, acc.: 62.11%] [G loss: 0.916315]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1018 [D loss: 0.634304, acc.: 61.04%] [G loss: 0.914708]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1019 [D loss: 0.637489, acc.: 62.99%] [G loss: 0.906779]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1020 [D loss: 0.630270, acc.: 63.77%] [G loss: 0.905511]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1021 [D loss: 0.637694, acc.: 63.33%] [G loss: 0.905936]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1022 [D loss: 0.635910, acc.: 62.40%] [G loss: 0.906775]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1023 [D loss: 0.638376, acc.: 62.60%] [G loss: 0.899679]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1024 [D loss: 0.639440, acc.: 61.91%] [G loss: 0.901845]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1025 [D loss: 0.628392, acc.: 64.94%] [G loss: 0.903348]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1026 [D loss: 0.634099, acc.: 63.77%] [G loss: 0.908077]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1027 [D loss: 0.630282, acc.: 64.06%] [G loss: 0.911729]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1028 [D loss: 0.633667, acc.: 65.38%] [G loss: 0.905738]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1029 [D loss: 0.632607, acc.: 63.92%] [G loss: 0.908989]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1030 [D loss: 0.635861, acc.: 62.26%] [G loss: 0.900904]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1031 [D loss: 0.639137, acc.: 62.11%] [G loss: 0.900270]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1032 [D loss: 0.636031, acc.: 62.06%] [G loss: 0.902025]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1033 [D loss: 0.632061, acc.: 63.33%] [G loss: 0.910035]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1034 [D loss: 0.633566, acc.: 63.43%] [G loss: 0.898327]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1035 [D loss: 0.642634, acc.: 59.77%] [G loss: 0.901757]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1036 [D loss: 0.634377, acc.: 63.57%] [G loss: 0.898341]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1037 [D loss: 0.648277, acc.: 59.47%] [G loss: 0.897446]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1038 [D loss: 0.631482, acc.: 63.72%] [G loss: 0.898447]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1039 [D loss: 0.636579, acc.: 62.06%] [G loss: 0.899415]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1040 [D loss: 0.637603, acc.: 62.94%] [G loss: 0.894194]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1041 [D loss: 0.636869, acc.: 63.13%] [G loss: 0.894050]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1042 [D loss: 0.638216, acc.: 61.62%] [G loss: 0.897255]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1043 [D loss: 0.633661, acc.: 63.28%] [G loss: 0.896785]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1044 [D loss: 0.640493, acc.: 63.28%] [G loss: 0.894371]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1045 [D loss: 0.632382, acc.: 63.13%] [G loss: 0.893603]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1046 [D loss: 0.638812, acc.: 61.62%] [G loss: 0.892985]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1047 [D loss: 0.641228, acc.: 61.77%] [G loss: 0.894470]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1048 [D loss: 0.633308, acc.: 62.60%] [G loss: 0.894602]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1049 [D loss: 0.643714, acc.: 61.57%] [G loss: 0.894839]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1050 [D loss: 0.639008, acc.: 62.40%] [G loss: 0.892244]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1051 [D loss: 0.642418, acc.: 60.69%] [G loss: 0.891787]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1052 [D loss: 0.634454, acc.: 62.55%] [G loss: 0.896935]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1053 [D loss: 0.636646, acc.: 63.92%] [G loss: 0.896798]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1054 [D loss: 0.634262, acc.: 64.50%] [G loss: 0.895329]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1055 [D loss: 0.642860, acc.: 60.50%] [G loss: 0.888915]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1056 [D loss: 0.638833, acc.: 61.67%] [G loss: 0.893440]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1057 [D loss: 0.640088, acc.: 62.30%] [G loss: 0.892153]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1058 [D loss: 0.640687, acc.: 62.35%] [G loss: 0.891709]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1059 [D loss: 0.636422, acc.: 63.18%] [G loss: 0.888827]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1060 [D loss: 0.636719, acc.: 63.67%] [G loss: 0.892529]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1061 [D loss: 0.642196, acc.: 61.52%] [G loss: 0.889778]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1062 [D loss: 0.637947, acc.: 62.74%] [G loss: 0.894422]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1063 [D loss: 0.635999, acc.: 63.82%] [G loss: 0.888197]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1064 [D loss: 0.644821, acc.: 61.13%] [G loss: 0.882946]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1065 [D loss: 0.641833, acc.: 61.87%] [G loss: 0.887081]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1066 [D loss: 0.640907, acc.: 62.16%] [G loss: 0.891054]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1067 [D loss: 0.639350, acc.: 62.70%] [G loss: 0.888249]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1068 [D loss: 0.641621, acc.: 61.33%] [G loss: 0.884615]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1069 [D loss: 0.640672, acc.: 61.72%] [G loss: 0.885657]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1070 [D loss: 0.637970, acc.: 63.13%] [G loss: 0.885962]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1071 [D loss: 0.640809, acc.: 61.82%] [G loss: 0.882841]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1072 [D loss: 0.644605, acc.: 61.47%] [G loss: 0.886607]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1073 [D loss: 0.641354, acc.: 63.28%] [G loss: 0.883950]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1074 [D loss: 0.632053, acc.: 63.43%] [G loss: 0.882439]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1075 [D loss: 0.638471, acc.: 62.60%] [G loss: 0.881715]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1076 [D loss: 0.638268, acc.: 62.65%] [G loss: 0.882601]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1077 [D loss: 0.642111, acc.: 61.18%] [G loss: 0.879321]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1078 [D loss: 0.643488, acc.: 61.96%] [G loss: 0.884689]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1079 [D loss: 0.640409, acc.: 62.21%] [G loss: 0.882778]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1080 [D loss: 0.642755, acc.: 62.16%] [G loss: 0.881621]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1081 [D loss: 0.639603, acc.: 63.48%] [G loss: 0.882290]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1082 [D loss: 0.641095, acc.: 63.18%] [G loss: 0.882018]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1083 [D loss: 0.641028, acc.: 61.96%] [G loss: 0.879145]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1084 [D loss: 0.643530, acc.: 62.16%] [G loss: 0.882716]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1085 [D loss: 0.642108, acc.: 61.67%] [G loss: 0.872495]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1086 [D loss: 0.640663, acc.: 62.79%] [G loss: 0.874246]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1087 [D loss: 0.638468, acc.: 63.48%] [G loss: 0.876361]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1088 [D loss: 0.644431, acc.: 59.81%] [G loss: 0.877410]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1089 [D loss: 0.641125, acc.: 61.67%] [G loss: 0.882057]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1090 [D loss: 0.645018, acc.: 61.91%] [G loss: 0.878612]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1091 [D loss: 0.648559, acc.: 61.13%] [G loss: 0.872219]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1092 [D loss: 0.651733, acc.: 58.50%] [G loss: 0.871612]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1093 [D loss: 0.638131, acc.: 64.55%] [G loss: 0.872638]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1094 [D loss: 0.644247, acc.: 61.08%] [G loss: 0.871112]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1095 [D loss: 0.643765, acc.: 62.60%] [G loss: 0.874378]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1096 [D loss: 0.641429, acc.: 63.18%] [G loss: 0.872955]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1097 [D loss: 0.643380, acc.: 62.01%] [G loss: 0.873356]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1098 [D loss: 0.648800, acc.: 60.84%] [G loss: 0.869492]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1099 [D loss: 0.638822, acc.: 64.01%] [G loss: 0.869588]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1100 [D loss: 0.641806, acc.: 62.55%] [G loss: 0.864514]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1101 [D loss: 0.639102, acc.: 62.55%] [G loss: 0.872039]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1102 [D loss: 0.643355, acc.: 62.26%] [G loss: 0.862982]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1103 [D loss: 0.642274, acc.: 60.74%] [G loss: 0.867798]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1104 [D loss: 0.646112, acc.: 60.30%] [G loss: 0.868592]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1105 [D loss: 0.636254, acc.: 64.55%] [G loss: 0.871024]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1106 [D loss: 0.642813, acc.: 62.60%] [G loss: 0.867782]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1107 [D loss: 0.646906, acc.: 61.82%] [G loss: 0.869602]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1108 [D loss: 0.639944, acc.: 63.38%] [G loss: 0.871151]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1109 [D loss: 0.644302, acc.: 62.50%] [G loss: 0.865777]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1110 [D loss: 0.643876, acc.: 61.96%] [G loss: 0.865646]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1111 [D loss: 0.643938, acc.: 62.99%] [G loss: 0.866109]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1112 [D loss: 0.642483, acc.: 63.33%] [G loss: 0.860893]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1113 [D loss: 0.637124, acc.: 63.82%] [G loss: 0.867752]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1114 [D loss: 0.642080, acc.: 62.35%] [G loss: 0.863606]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1115 [D loss: 0.641190, acc.: 63.67%] [G loss: 0.865720]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1116 [D loss: 0.642443, acc.: 62.65%] [G loss: 0.863974]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1117 [D loss: 0.637507, acc.: 64.99%] [G loss: 0.868765]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1118 [D loss: 0.641691, acc.: 62.99%] [G loss: 0.864181]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1119 [D loss: 0.643206, acc.: 62.70%] [G loss: 0.865309]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1120 [D loss: 0.643057, acc.: 62.74%] [G loss: 0.863542]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1121 [D loss: 0.647588, acc.: 60.84%] [G loss: 0.858651]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1122 [D loss: 0.639239, acc.: 63.92%] [G loss: 0.859038]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1123 [D loss: 0.645605, acc.: 61.57%] [G loss: 0.861354]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1124 [D loss: 0.639545, acc.: 63.96%] [G loss: 0.860443]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1125 [D loss: 0.643475, acc.: 62.30%] [G loss: 0.861358]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1126 [D loss: 0.640219, acc.: 64.36%] [G loss: 0.860701]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1127 [D loss: 0.641617, acc.: 62.89%] [G loss: 0.862905]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1128 [D loss: 0.648494, acc.: 61.18%] [G loss: 0.863123]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1129 [D loss: 0.641260, acc.: 63.33%] [G loss: 0.861552]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1130 [D loss: 0.645785, acc.: 61.77%] [G loss: 0.860681]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1131 [D loss: 0.643370, acc.: 62.55%] [G loss: 0.863030]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1132 [D loss: 0.644309, acc.: 62.40%] [G loss: 0.860083]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1133 [D loss: 0.641290, acc.: 63.92%] [G loss: 0.864415]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1134 [D loss: 0.644970, acc.: 62.94%] [G loss: 0.861306]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1135 [D loss: 0.641823, acc.: 63.33%] [G loss: 0.859733]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1136 [D loss: 0.640774, acc.: 64.01%] [G loss: 0.862993]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1137 [D loss: 0.640989, acc.: 65.53%] [G loss: 0.858595]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1138 [D loss: 0.648735, acc.: 60.01%] [G loss: 0.864365]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1139 [D loss: 0.638247, acc.: 65.23%] [G loss: 0.865380]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1140 [D loss: 0.636672, acc.: 65.33%] [G loss: 0.863910]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1141 [D loss: 0.641220, acc.: 63.53%] [G loss: 0.862564]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1142 [D loss: 0.642450, acc.: 63.92%] [G loss: 0.861453]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1143 [D loss: 0.643604, acc.: 63.67%] [G loss: 0.859926]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1144 [D loss: 0.641858, acc.: 63.13%] [G loss: 0.861347]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1145 [D loss: 0.639752, acc.: 64.79%] [G loss: 0.859424]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1146 [D loss: 0.643056, acc.: 62.74%] [G loss: 0.863133]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1147 [D loss: 0.642904, acc.: 62.79%] [G loss: 0.863671]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1148 [D loss: 0.637999, acc.: 64.84%] [G loss: 0.858230]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1149 [D loss: 0.644189, acc.: 63.43%] [G loss: 0.859749]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1150 [D loss: 0.641553, acc.: 64.60%] [G loss: 0.860248]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1151 [D loss: 0.646319, acc.: 62.21%] [G loss: 0.860352]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1152 [D loss: 0.641265, acc.: 64.84%] [G loss: 0.859491]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1153 [D loss: 0.638337, acc.: 65.77%] [G loss: 0.859800]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1154 [D loss: 0.642910, acc.: 64.36%] [G loss: 0.859113]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1155 [D loss: 0.639954, acc.: 65.92%] [G loss: 0.860719]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1156 [D loss: 0.641226, acc.: 64.50%] [G loss: 0.862309]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1157 [D loss: 0.641377, acc.: 65.09%] [G loss: 0.854941]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1158 [D loss: 0.638978, acc.: 65.19%] [G loss: 0.860578]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1159 [D loss: 0.642627, acc.: 63.62%] [G loss: 0.858330]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1160 [D loss: 0.637633, acc.: 65.58%] [G loss: 0.858060]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1161 [D loss: 0.642267, acc.: 64.31%] [G loss: 0.862502]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1162 [D loss: 0.635459, acc.: 66.36%] [G loss: 0.858704]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1163 [D loss: 0.642051, acc.: 63.57%] [G loss: 0.855726]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1164 [D loss: 0.635676, acc.: 66.26%] [G loss: 0.858815]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1165 [D loss: 0.644632, acc.: 63.13%] [G loss: 0.858225]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1166 [D loss: 0.642744, acc.: 63.48%] [G loss: 0.861553]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1167 [D loss: 0.639796, acc.: 64.94%] [G loss: 0.856523]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1168 [D loss: 0.640319, acc.: 65.28%] [G loss: 0.857782]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1169 [D loss: 0.641814, acc.: 64.70%] [G loss: 0.859994]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1170 [D loss: 0.642073, acc.: 63.67%] [G loss: 0.860676]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1171 [D loss: 0.643528, acc.: 65.62%] [G loss: 0.859442]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1172 [D loss: 0.638678, acc.: 66.06%] [G loss: 0.859186]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1173 [D loss: 0.645789, acc.: 63.96%] [G loss: 0.857023]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1174 [D loss: 0.641204, acc.: 63.82%] [G loss: 0.861663]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1175 [D loss: 0.641822, acc.: 64.55%] [G loss: 0.860548]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1176 [D loss: 0.641911, acc.: 65.19%] [G loss: 0.860295]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1177 [D loss: 0.641319, acc.: 64.89%] [G loss: 0.857491]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1178 [D loss: 0.635045, acc.: 66.11%] [G loss: 0.859214]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1179 [D loss: 0.638277, acc.: 65.87%] [G loss: 0.860913]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1180 [D loss: 0.636261, acc.: 66.21%] [G loss: 0.862306]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1181 [D loss: 0.638591, acc.: 66.06%] [G loss: 0.861363]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1182 [D loss: 0.640426, acc.: 63.87%] [G loss: 0.859109]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1183 [D loss: 0.637648, acc.: 65.33%] [G loss: 0.859123]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1184 [D loss: 0.635989, acc.: 66.85%] [G loss: 0.858952]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1185 [D loss: 0.638194, acc.: 65.28%] [G loss: 0.859447]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1186 [D loss: 0.637374, acc.: 66.46%] [G loss: 0.859856]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1187 [D loss: 0.640497, acc.: 65.82%] [G loss: 0.859272]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1188 [D loss: 0.638133, acc.: 66.99%] [G loss: 0.858455]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1189 [D loss: 0.634720, acc.: 66.94%] [G loss: 0.861491]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1190 [D loss: 0.638784, acc.: 66.26%] [G loss: 0.858971]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1191 [D loss: 0.640458, acc.: 64.75%] [G loss: 0.859347]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1192 [D loss: 0.636391, acc.: 66.31%] [G loss: 0.855326]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1193 [D loss: 0.631238, acc.: 68.26%] [G loss: 0.857065]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1194 [D loss: 0.632980, acc.: 68.21%] [G loss: 0.855599]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1195 [D loss: 0.640101, acc.: 64.89%] [G loss: 0.863487]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1196 [D loss: 0.640664, acc.: 65.14%] [G loss: 0.864542]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1197 [D loss: 0.635507, acc.: 66.85%] [G loss: 0.863013]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1198 [D loss: 0.640462, acc.: 65.23%] [G loss: 0.865270]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1199 [D loss: 0.639221, acc.: 65.04%] [G loss: 0.863311]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1200 [D loss: 0.633293, acc.: 67.38%] [G loss: 0.863138]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1201 [D loss: 0.638909, acc.: 66.65%] [G loss: 0.864350]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1202 [D loss: 0.635866, acc.: 66.80%] [G loss: 0.861757]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1203 [D loss: 0.635786, acc.: 67.43%] [G loss: 0.866404]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1204 [D loss: 0.637948, acc.: 66.11%] [G loss: 0.863845]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1205 [D loss: 0.632960, acc.: 68.31%] [G loss: 0.869633]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1206 [D loss: 0.637132, acc.: 66.26%] [G loss: 0.867163]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1207 [D loss: 0.637815, acc.: 66.50%] [G loss: 0.865719]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1208 [D loss: 0.632932, acc.: 69.43%] [G loss: 0.862354]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1209 [D loss: 0.635850, acc.: 66.02%] [G loss: 0.864842]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1210 [D loss: 0.636597, acc.: 66.60%] [G loss: 0.866856]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1211 [D loss: 0.633383, acc.: 67.97%] [G loss: 0.860771]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1212 [D loss: 0.635616, acc.: 66.55%] [G loss: 0.864544]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1213 [D loss: 0.636713, acc.: 65.23%] [G loss: 0.867758]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1214 [D loss: 0.631726, acc.: 68.51%] [G loss: 0.872629]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1215 [D loss: 0.635582, acc.: 67.48%] [G loss: 0.866776]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1216 [D loss: 0.638910, acc.: 66.31%] [G loss: 0.867600]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1217 [D loss: 0.632031, acc.: 69.82%] [G loss: 0.868509]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1218 [D loss: 0.633135, acc.: 67.97%] [G loss: 0.870847]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1219 [D loss: 0.635559, acc.: 67.72%] [G loss: 0.870845]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1220 [D loss: 0.631209, acc.: 68.21%] [G loss: 0.868243]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1221 [D loss: 0.635370, acc.: 67.72%] [G loss: 0.868287]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1222 [D loss: 0.636746, acc.: 66.70%] [G loss: 0.868453]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1223 [D loss: 0.635651, acc.: 67.72%] [G loss: 0.870461]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1224 [D loss: 0.631203, acc.: 68.85%] [G loss: 0.868140]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1225 [D loss: 0.634118, acc.: 67.92%] [G loss: 0.865955]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1226 [D loss: 0.634335, acc.: 67.97%] [G loss: 0.862489]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1227 [D loss: 0.629819, acc.: 69.38%] [G loss: 0.869131]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1228 [D loss: 0.630949, acc.: 69.68%] [G loss: 0.868367]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1229 [D loss: 0.629208, acc.: 70.26%] [G loss: 0.871745]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1230 [D loss: 0.631627, acc.: 68.46%] [G loss: 0.871171]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1231 [D loss: 0.629799, acc.: 69.63%] [G loss: 0.870150]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1232 [D loss: 0.633700, acc.: 69.38%] [G loss: 0.869219]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1233 [D loss: 0.630113, acc.: 69.82%] [G loss: 0.869675]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1234 [D loss: 0.627284, acc.: 71.44%] [G loss: 0.867339]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1235 [D loss: 0.633334, acc.: 69.73%] [G loss: 0.864919]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1236 [D loss: 0.629822, acc.: 68.41%] [G loss: 0.872879]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1237 [D loss: 0.630797, acc.: 68.26%] [G loss: 0.869308]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1238 [D loss: 0.630881, acc.: 70.36%] [G loss: 0.873244]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1239 [D loss: 0.628159, acc.: 69.78%] [G loss: 0.877158]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1240 [D loss: 0.634282, acc.: 68.70%] [G loss: 0.869869]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1241 [D loss: 0.628552, acc.: 70.75%] [G loss: 0.874077]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1242 [D loss: 0.631912, acc.: 69.19%] [G loss: 0.877965]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1243 [D loss: 0.629014, acc.: 70.51%] [G loss: 0.874789]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1244 [D loss: 0.630081, acc.: 69.97%] [G loss: 0.870638]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1245 [D loss: 0.632250, acc.: 69.43%] [G loss: 0.875040]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1246 [D loss: 0.629277, acc.: 70.41%] [G loss: 0.873571]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1247 [D loss: 0.623660, acc.: 71.68%] [G loss: 0.874470]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1248 [D loss: 0.632645, acc.: 68.65%] [G loss: 0.871129]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1249 [D loss: 0.630566, acc.: 70.61%] [G loss: 0.872776]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1250 [D loss: 0.630841, acc.: 70.02%] [G loss: 0.880922]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1251 [D loss: 0.626228, acc.: 71.24%] [G loss: 0.881392]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1252 [D loss: 0.629057, acc.: 70.75%] [G loss: 0.878511]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1253 [D loss: 0.632121, acc.: 70.36%] [G loss: 0.871602]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1254 [D loss: 0.624246, acc.: 71.68%] [G loss: 0.877928]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1255 [D loss: 0.623311, acc.: 73.29%] [G loss: 0.875600]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1256 [D loss: 0.627608, acc.: 70.07%] [G loss: 0.880762]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1257 [D loss: 0.628894, acc.: 71.73%] [G loss: 0.879841]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1258 [D loss: 0.625849, acc.: 73.05%] [G loss: 0.881502]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1259 [D loss: 0.626199, acc.: 72.66%] [G loss: 0.873662]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1260 [D loss: 0.626835, acc.: 70.41%] [G loss: 0.882067]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1261 [D loss: 0.628638, acc.: 71.24%] [G loss: 0.881907]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1262 [D loss: 0.622779, acc.: 73.24%] [G loss: 0.881653]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1263 [D loss: 0.626248, acc.: 71.88%] [G loss: 0.879580]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1264 [D loss: 0.625904, acc.: 71.29%] [G loss: 0.876159]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1265 [D loss: 0.621093, acc.: 73.54%] [G loss: 0.881348]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1266 [D loss: 0.625700, acc.: 72.46%] [G loss: 0.880041]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1267 [D loss: 0.624353, acc.: 72.71%] [G loss: 0.882959]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1268 [D loss: 0.625975, acc.: 71.58%] [G loss: 0.883960]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1269 [D loss: 0.620583, acc.: 73.68%] [G loss: 0.879836]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1270 [D loss: 0.624032, acc.: 71.88%] [G loss: 0.884545]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1271 [D loss: 0.626760, acc.: 71.92%] [G loss: 0.884576]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1272 [D loss: 0.627150, acc.: 72.41%] [G loss: 0.883857]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1273 [D loss: 0.620380, acc.: 73.44%] [G loss: 0.885270]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1274 [D loss: 0.622395, acc.: 71.78%] [G loss: 0.888321]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1275 [D loss: 0.624486, acc.: 72.61%] [G loss: 0.886085]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1276 [D loss: 0.625685, acc.: 71.73%] [G loss: 0.888994]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1277 [D loss: 0.622813, acc.: 72.75%] [G loss: 0.886892]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1278 [D loss: 0.623208, acc.: 71.09%] [G loss: 0.891461]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1279 [D loss: 0.620694, acc.: 74.41%] [G loss: 0.891402]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1280 [D loss: 0.623510, acc.: 72.51%] [G loss: 0.889395]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1281 [D loss: 0.625173, acc.: 72.36%] [G loss: 0.890556]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1282 [D loss: 0.623312, acc.: 73.73%] [G loss: 0.889000]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1283 [D loss: 0.624004, acc.: 72.66%] [G loss: 0.884192]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1284 [D loss: 0.620699, acc.: 74.56%] [G loss: 0.886811]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1285 [D loss: 0.621386, acc.: 72.95%] [G loss: 0.888363]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1286 [D loss: 0.621263, acc.: 72.27%] [G loss: 0.894057]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1287 [D loss: 0.618954, acc.: 75.54%] [G loss: 0.893132]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1288 [D loss: 0.622046, acc.: 73.14%] [G loss: 0.891140]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1289 [D loss: 0.621682, acc.: 73.44%] [G loss: 0.893301]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1290 [D loss: 0.619308, acc.: 75.15%] [G loss: 0.894153]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1291 [D loss: 0.624680, acc.: 72.22%] [G loss: 0.892579]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1292 [D loss: 0.616102, acc.: 75.15%] [G loss: 0.893643]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1293 [D loss: 0.624207, acc.: 73.54%] [G loss: 0.892888]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1294 [D loss: 0.616145, acc.: 74.71%] [G loss: 0.896403]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1295 [D loss: 0.618999, acc.: 73.24%] [G loss: 0.895537]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1296 [D loss: 0.618940, acc.: 75.15%] [G loss: 0.893451]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1297 [D loss: 0.621034, acc.: 74.80%] [G loss: 0.893183]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1298 [D loss: 0.612876, acc.: 76.32%] [G loss: 0.897807]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1299 [D loss: 0.623977, acc.: 72.95%] [G loss: 0.892082]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1300 [D loss: 0.617415, acc.: 76.61%] [G loss: 0.896888]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1301 [D loss: 0.619134, acc.: 75.29%] [G loss: 0.898750]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1302 [D loss: 0.614481, acc.: 77.00%] [G loss: 0.896262]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1303 [D loss: 0.619638, acc.: 74.66%] [G loss: 0.894301]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1304 [D loss: 0.616442, acc.: 74.12%] [G loss: 0.896369]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1305 [D loss: 0.618757, acc.: 75.10%] [G loss: 0.898870]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1306 [D loss: 0.613245, acc.: 75.88%] [G loss: 0.898447]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1307 [D loss: 0.619865, acc.: 74.85%] [G loss: 0.898890]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1308 [D loss: 0.616244, acc.: 76.12%] [G loss: 0.899543]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1309 [D loss: 0.615234, acc.: 76.03%] [G loss: 0.900093]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1310 [D loss: 0.616683, acc.: 75.93%] [G loss: 0.901526]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1311 [D loss: 0.613451, acc.: 76.12%] [G loss: 0.902397]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1312 [D loss: 0.611059, acc.: 77.05%] [G loss: 0.903712]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1313 [D loss: 0.612238, acc.: 77.59%] [G loss: 0.906402]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1314 [D loss: 0.609695, acc.: 77.20%] [G loss: 0.909145]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1315 [D loss: 0.614603, acc.: 76.61%] [G loss: 0.906780]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1316 [D loss: 0.615323, acc.: 77.64%] [G loss: 0.907901]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1317 [D loss: 0.612557, acc.: 77.64%] [G loss: 0.906423]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1318 [D loss: 0.611538, acc.: 77.34%] [G loss: 0.908359]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1319 [D loss: 0.617118, acc.: 76.71%] [G loss: 0.908697]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1320 [D loss: 0.611743, acc.: 77.25%] [G loss: 0.911027]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1321 [D loss: 0.611303, acc.: 77.25%] [G loss: 0.910577]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1322 [D loss: 0.608278, acc.: 77.98%] [G loss: 0.910087]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1323 [D loss: 0.611989, acc.: 76.12%] [G loss: 0.911590]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1324 [D loss: 0.609697, acc.: 78.66%] [G loss: 0.913371]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1325 [D loss: 0.608577, acc.: 79.64%] [G loss: 0.913041]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1326 [D loss: 0.613048, acc.: 78.12%] [G loss: 0.907877]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1327 [D loss: 0.607092, acc.: 79.44%] [G loss: 0.911021]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1328 [D loss: 0.608321, acc.: 78.08%] [G loss: 0.914831]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1329 [D loss: 0.609535, acc.: 78.81%] [G loss: 0.915603]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1330 [D loss: 0.609240, acc.: 78.96%] [G loss: 0.917053]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1331 [D loss: 0.612952, acc.: 77.25%] [G loss: 0.912803]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1332 [D loss: 0.605161, acc.: 80.03%] [G loss: 0.917780]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1333 [D loss: 0.612949, acc.: 77.64%] [G loss: 0.915458]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1334 [D loss: 0.604499, acc.: 80.37%] [G loss: 0.917310]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1335 [D loss: 0.605568, acc.: 78.86%] [G loss: 0.918750]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1336 [D loss: 0.611220, acc.: 78.22%] [G loss: 0.919239]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1337 [D loss: 0.607353, acc.: 78.52%] [G loss: 0.918660]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1338 [D loss: 0.611813, acc.: 76.90%] [G loss: 0.921503]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1339 [D loss: 0.610480, acc.: 77.59%] [G loss: 0.917282]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1340 [D loss: 0.608627, acc.: 77.78%] [G loss: 0.924013]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1341 [D loss: 0.603228, acc.: 80.42%] [G loss: 0.924948]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1342 [D loss: 0.609976, acc.: 78.32%] [G loss: 0.922626]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1343 [D loss: 0.605601, acc.: 79.05%] [G loss: 0.927208]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1344 [D loss: 0.610715, acc.: 77.93%] [G loss: 0.928097]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1345 [D loss: 0.606062, acc.: 80.08%] [G loss: 0.927202]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1346 [D loss: 0.601174, acc.: 80.76%] [G loss: 0.924365]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1347 [D loss: 0.607537, acc.: 78.86%] [G loss: 0.930764]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1348 [D loss: 0.601460, acc.: 82.28%] [G loss: 0.922978]\n",
            "32/32 [==============================] - 0s 4ms/step\n",
            "1349 [D loss: 0.609827, acc.: 78.22%] [G loss: 0.928999]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1350 [D loss: 0.603334, acc.: 79.69%] [G loss: 0.931333]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1351 [D loss: 0.601395, acc.: 81.05%] [G loss: 0.931738]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1352 [D loss: 0.605261, acc.: 79.79%] [G loss: 0.932663]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1353 [D loss: 0.604456, acc.: 79.69%] [G loss: 0.930383]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1354 [D loss: 0.605795, acc.: 79.00%] [G loss: 0.932352]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1355 [D loss: 0.606934, acc.: 79.83%] [G loss: 0.930083]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1356 [D loss: 0.596050, acc.: 81.05%] [G loss: 0.938517]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1357 [D loss: 0.595944, acc.: 81.79%] [G loss: 0.940703]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1358 [D loss: 0.606192, acc.: 79.59%] [G loss: 0.936322]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1359 [D loss: 0.602116, acc.: 80.13%] [G loss: 0.937695]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1360 [D loss: 0.598826, acc.: 79.74%] [G loss: 0.935256]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1361 [D loss: 0.596589, acc.: 81.40%] [G loss: 0.939256]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1362 [D loss: 0.601543, acc.: 81.01%] [G loss: 0.942641]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1363 [D loss: 0.601832, acc.: 80.71%] [G loss: 0.939218]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1364 [D loss: 0.598830, acc.: 79.69%] [G loss: 0.942560]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1365 [D loss: 0.599989, acc.: 80.96%] [G loss: 0.944189]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1366 [D loss: 0.602559, acc.: 80.42%] [G loss: 0.942212]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1367 [D loss: 0.601827, acc.: 79.88%] [G loss: 0.936062]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1368 [D loss: 0.595335, acc.: 81.25%] [G loss: 0.936933]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1369 [D loss: 0.597541, acc.: 80.32%] [G loss: 0.946733]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1370 [D loss: 0.603579, acc.: 79.30%] [G loss: 0.943637]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1371 [D loss: 0.600505, acc.: 80.37%] [G loss: 0.945055]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1372 [D loss: 0.598311, acc.: 82.13%] [G loss: 0.942317]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1373 [D loss: 0.597931, acc.: 80.91%] [G loss: 0.948791]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1374 [D loss: 0.595588, acc.: 82.13%] [G loss: 0.949172]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1375 [D loss: 0.598714, acc.: 81.40%] [G loss: 0.944740]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1376 [D loss: 0.600187, acc.: 80.32%] [G loss: 0.945467]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1377 [D loss: 0.598132, acc.: 81.01%] [G loss: 0.948319]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1378 [D loss: 0.594821, acc.: 80.91%] [G loss: 0.946641]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1379 [D loss: 0.591996, acc.: 81.74%] [G loss: 0.956678]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1380 [D loss: 0.594420, acc.: 83.54%] [G loss: 0.952943]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1381 [D loss: 0.589742, acc.: 83.45%] [G loss: 0.959156]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1382 [D loss: 0.593806, acc.: 82.28%] [G loss: 0.958212]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1383 [D loss: 0.596705, acc.: 80.42%] [G loss: 0.954850]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1384 [D loss: 0.596029, acc.: 82.03%] [G loss: 0.952127]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1385 [D loss: 0.595751, acc.: 80.76%] [G loss: 0.954828]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1386 [D loss: 0.593522, acc.: 82.76%] [G loss: 0.954565]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1387 [D loss: 0.589702, acc.: 82.76%] [G loss: 0.955349]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1388 [D loss: 0.597910, acc.: 80.22%] [G loss: 0.959830]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1389 [D loss: 0.590670, acc.: 82.62%] [G loss: 0.956834]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1390 [D loss: 0.593534, acc.: 81.54%] [G loss: 0.960091]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1391 [D loss: 0.589592, acc.: 82.52%] [G loss: 0.961240]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1392 [D loss: 0.590731, acc.: 82.91%] [G loss: 0.959469]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1393 [D loss: 0.592357, acc.: 80.81%] [G loss: 0.961226]\n",
            "32/32 [==============================] - 0s 4ms/step\n",
            "1394 [D loss: 0.591020, acc.: 82.57%] [G loss: 0.966831]\n",
            "32/32 [==============================] - 0s 4ms/step\n",
            "1395 [D loss: 0.581858, acc.: 84.81%] [G loss: 0.968376]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1396 [D loss: 0.588949, acc.: 82.71%] [G loss: 0.966974]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1397 [D loss: 0.597516, acc.: 81.01%] [G loss: 0.966391]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1398 [D loss: 0.592989, acc.: 81.59%] [G loss: 0.971624]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1399 [D loss: 0.588183, acc.: 82.18%] [G loss: 0.971443]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1400 [D loss: 0.593174, acc.: 81.35%] [G loss: 0.971866]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1401 [D loss: 0.589529, acc.: 82.96%] [G loss: 0.971943]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1402 [D loss: 0.588382, acc.: 82.57%] [G loss: 0.971121]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1403 [D loss: 0.585381, acc.: 83.20%] [G loss: 0.975123]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1404 [D loss: 0.583703, acc.: 83.50%] [G loss: 0.974984]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1405 [D loss: 0.588209, acc.: 82.91%] [G loss: 0.977505]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1406 [D loss: 0.591495, acc.: 81.93%] [G loss: 0.975779]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1407 [D loss: 0.589138, acc.: 82.57%] [G loss: 0.975528]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1408 [D loss: 0.583821, acc.: 81.84%] [G loss: 0.977335]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1409 [D loss: 0.589227, acc.: 82.47%] [G loss: 0.978175]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1410 [D loss: 0.585336, acc.: 82.91%] [G loss: 0.978296]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1411 [D loss: 0.581833, acc.: 83.74%] [G loss: 0.979508]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1412 [D loss: 0.587510, acc.: 81.74%] [G loss: 0.978137]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1413 [D loss: 0.581722, acc.: 83.98%] [G loss: 0.987620]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1414 [D loss: 0.591772, acc.: 82.42%] [G loss: 0.978813]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1415 [D loss: 0.579968, acc.: 83.45%] [G loss: 0.986053]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1416 [D loss: 0.586137, acc.: 83.40%] [G loss: 0.989559]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1417 [D loss: 0.591769, acc.: 82.13%] [G loss: 0.979966]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1418 [D loss: 0.583892, acc.: 82.67%] [G loss: 0.983407]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1419 [D loss: 0.580094, acc.: 82.67%] [G loss: 0.987255]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1420 [D loss: 0.584125, acc.: 83.45%] [G loss: 0.988405]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1421 [D loss: 0.577066, acc.: 84.08%] [G loss: 0.990207]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1422 [D loss: 0.584230, acc.: 83.01%] [G loss: 0.994928]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1423 [D loss: 0.583212, acc.: 83.06%] [G loss: 0.990194]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1424 [D loss: 0.579414, acc.: 84.81%] [G loss: 0.990415]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1425 [D loss: 0.585171, acc.: 82.67%] [G loss: 0.996797]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1426 [D loss: 0.582786, acc.: 82.67%] [G loss: 0.995675]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1427 [D loss: 0.579549, acc.: 82.71%] [G loss: 0.999919]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1428 [D loss: 0.580462, acc.: 83.15%] [G loss: 0.997837]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1429 [D loss: 0.580133, acc.: 83.06%] [G loss: 0.998514]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1430 [D loss: 0.581728, acc.: 84.23%] [G loss: 0.995042]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1431 [D loss: 0.583951, acc.: 82.67%] [G loss: 0.992317]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1432 [D loss: 0.584784, acc.: 82.03%] [G loss: 0.996477]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1433 [D loss: 0.572708, acc.: 84.42%] [G loss: 0.998959]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1434 [D loss: 0.578166, acc.: 82.96%] [G loss: 1.003645]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1435 [D loss: 0.579662, acc.: 83.40%] [G loss: 1.000948]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1436 [D loss: 0.584003, acc.: 81.74%] [G loss: 0.997840]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1437 [D loss: 0.574006, acc.: 83.59%] [G loss: 1.002997]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1438 [D loss: 0.574399, acc.: 83.11%] [G loss: 1.006497]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1439 [D loss: 0.576177, acc.: 83.50%] [G loss: 1.013523]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1440 [D loss: 0.579474, acc.: 82.52%] [G loss: 1.009905]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1441 [D loss: 0.574943, acc.: 82.47%] [G loss: 1.012584]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1442 [D loss: 0.578502, acc.: 83.89%] [G loss: 1.013219]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1443 [D loss: 0.577824, acc.: 82.28%] [G loss: 1.012837]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1444 [D loss: 0.584565, acc.: 82.03%] [G loss: 1.006191]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1445 [D loss: 0.581214, acc.: 81.93%] [G loss: 1.004341]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1446 [D loss: 0.581119, acc.: 82.32%] [G loss: 1.009320]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1447 [D loss: 0.574784, acc.: 83.50%] [G loss: 1.012281]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1448 [D loss: 0.573008, acc.: 83.84%] [G loss: 1.016147]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1449 [D loss: 0.574587, acc.: 83.54%] [G loss: 1.009703]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1450 [D loss: 0.578750, acc.: 81.98%] [G loss: 1.014677]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1451 [D loss: 0.575106, acc.: 83.59%] [G loss: 1.016212]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1452 [D loss: 0.569518, acc.: 83.94%] [G loss: 1.015781]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1453 [D loss: 0.564967, acc.: 84.91%] [G loss: 1.025354]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1454 [D loss: 0.572780, acc.: 83.89%] [G loss: 1.018766]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1455 [D loss: 0.567453, acc.: 84.77%] [G loss: 1.022423]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1456 [D loss: 0.568285, acc.: 83.69%] [G loss: 1.035517]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1457 [D loss: 0.569800, acc.: 83.11%] [G loss: 1.033401]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1458 [D loss: 0.572755, acc.: 82.52%] [G loss: 1.031561]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1459 [D loss: 0.576918, acc.: 82.71%] [G loss: 1.025808]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1460 [D loss: 0.569987, acc.: 82.42%] [G loss: 1.026404]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1461 [D loss: 0.571152, acc.: 82.42%] [G loss: 1.035516]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1462 [D loss: 0.566898, acc.: 83.01%] [G loss: 1.031670]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1463 [D loss: 0.562790, acc.: 82.62%] [G loss: 1.042218]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1464 [D loss: 0.571272, acc.: 82.52%] [G loss: 1.029676]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1465 [D loss: 0.574172, acc.: 81.49%] [G loss: 1.042774]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1466 [D loss: 0.564758, acc.: 83.40%] [G loss: 1.041715]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1467 [D loss: 0.567761, acc.: 83.89%] [G loss: 1.045279]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1468 [D loss: 0.567749, acc.: 83.11%] [G loss: 1.035122]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1469 [D loss: 0.570043, acc.: 82.86%] [G loss: 1.040017]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1470 [D loss: 0.574105, acc.: 82.67%] [G loss: 1.042455]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1471 [D loss: 0.574903, acc.: 81.64%] [G loss: 1.041493]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1472 [D loss: 0.565160, acc.: 83.01%] [G loss: 1.043013]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1473 [D loss: 0.569398, acc.: 82.91%] [G loss: 1.040664]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1474 [D loss: 0.572160, acc.: 81.40%] [G loss: 1.051114]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1475 [D loss: 0.567651, acc.: 81.69%] [G loss: 1.039248]\n",
            "32/32 [==============================] - 0s 4ms/step\n",
            "1476 [D loss: 0.569065, acc.: 82.57%] [G loss: 1.053137]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1477 [D loss: 0.574325, acc.: 80.66%] [G loss: 1.043916]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1478 [D loss: 0.573053, acc.: 81.25%] [G loss: 1.049813]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1479 [D loss: 0.572254, acc.: 82.03%] [G loss: 1.047871]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1480 [D loss: 0.559907, acc.: 82.76%] [G loss: 1.055033]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1481 [D loss: 0.567667, acc.: 82.23%] [G loss: 1.048777]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1482 [D loss: 0.564095, acc.: 83.06%] [G loss: 1.056989]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1483 [D loss: 0.569905, acc.: 80.76%] [G loss: 1.052256]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1484 [D loss: 0.572251, acc.: 80.86%] [G loss: 1.045957]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1485 [D loss: 0.567473, acc.: 83.11%] [G loss: 1.053016]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1486 [D loss: 0.559974, acc.: 82.08%] [G loss: 1.057788]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1487 [D loss: 0.566730, acc.: 81.84%] [G loss: 1.057507]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1488 [D loss: 0.564186, acc.: 81.59%] [G loss: 1.062993]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1489 [D loss: 0.560273, acc.: 82.96%] [G loss: 1.057944]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1490 [D loss: 0.571689, acc.: 81.05%] [G loss: 1.059327]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1491 [D loss: 0.567644, acc.: 81.35%] [G loss: 1.066365]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1492 [D loss: 0.566842, acc.: 80.57%] [G loss: 1.065452]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1493 [D loss: 0.560048, acc.: 81.45%] [G loss: 1.069664]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1494 [D loss: 0.564855, acc.: 80.42%] [G loss: 1.059364]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1495 [D loss: 0.575534, acc.: 79.88%] [G loss: 1.055492]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1496 [D loss: 0.560585, acc.: 81.93%] [G loss: 1.066158]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1497 [D loss: 0.565197, acc.: 81.01%] [G loss: 1.063353]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1498 [D loss: 0.563860, acc.: 81.88%] [G loss: 1.060932]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1499 [D loss: 0.567786, acc.: 81.79%] [G loss: 1.059827]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1500 [D loss: 0.560708, acc.: 80.71%] [G loss: 1.069345]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1501 [D loss: 0.561346, acc.: 80.96%] [G loss: 1.062979]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1502 [D loss: 0.564128, acc.: 81.01%] [G loss: 1.066157]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1503 [D loss: 0.560898, acc.: 82.42%] [G loss: 1.077532]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1504 [D loss: 0.560486, acc.: 81.79%] [G loss: 1.072159]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1505 [D loss: 0.564313, acc.: 80.13%] [G loss: 1.072282]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1506 [D loss: 0.559141, acc.: 81.84%] [G loss: 1.074858]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1507 [D loss: 0.568864, acc.: 79.79%] [G loss: 1.067092]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1508 [D loss: 0.560628, acc.: 81.98%] [G loss: 1.072605]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1509 [D loss: 0.562747, acc.: 80.27%] [G loss: 1.067039]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1510 [D loss: 0.558841, acc.: 80.91%] [G loss: 1.084834]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1511 [D loss: 0.561653, acc.: 80.22%] [G loss: 1.071957]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1512 [D loss: 0.562993, acc.: 79.54%] [G loss: 1.077223]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1513 [D loss: 0.557612, acc.: 81.10%] [G loss: 1.075902]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1514 [D loss: 0.555977, acc.: 81.69%] [G loss: 1.081384]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1515 [D loss: 0.564723, acc.: 80.13%] [G loss: 1.082222]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1516 [D loss: 0.563154, acc.: 80.96%] [G loss: 1.078109]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1517 [D loss: 0.565120, acc.: 78.96%] [G loss: 1.085719]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1518 [D loss: 0.559853, acc.: 80.81%] [G loss: 1.090438]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1519 [D loss: 0.564992, acc.: 79.64%] [G loss: 1.089367]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1520 [D loss: 0.557633, acc.: 80.96%] [G loss: 1.085638]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1521 [D loss: 0.558820, acc.: 78.71%] [G loss: 1.079624]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1522 [D loss: 0.564192, acc.: 79.05%] [G loss: 1.084419]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1523 [D loss: 0.560163, acc.: 80.37%] [G loss: 1.085447]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1524 [D loss: 0.560902, acc.: 80.47%] [G loss: 1.099603]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1525 [D loss: 0.563788, acc.: 79.35%] [G loss: 1.091343]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1526 [D loss: 0.566532, acc.: 79.20%] [G loss: 1.089132]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1527 [D loss: 0.568378, acc.: 79.00%] [G loss: 1.089581]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1528 [D loss: 0.558710, acc.: 80.47%] [G loss: 1.079392]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1529 [D loss: 0.566801, acc.: 77.98%] [G loss: 1.088878]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1530 [D loss: 0.556170, acc.: 80.76%] [G loss: 1.094415]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1531 [D loss: 0.560809, acc.: 78.96%] [G loss: 1.095384]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1532 [D loss: 0.568528, acc.: 78.91%] [G loss: 1.089711]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1533 [D loss: 0.566675, acc.: 79.49%] [G loss: 1.092516]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1534 [D loss: 0.557270, acc.: 80.42%] [G loss: 1.101065]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1535 [D loss: 0.557719, acc.: 78.52%] [G loss: 1.095193]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1536 [D loss: 0.575969, acc.: 77.25%] [G loss: 1.097588]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1537 [D loss: 0.563244, acc.: 77.25%] [G loss: 1.091959]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1538 [D loss: 0.561407, acc.: 79.39%] [G loss: 1.083738]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1539 [D loss: 0.554153, acc.: 79.25%] [G loss: 1.104500]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1540 [D loss: 0.560525, acc.: 79.30%] [G loss: 1.100641]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1541 [D loss: 0.567155, acc.: 77.34%] [G loss: 1.102601]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1542 [D loss: 0.566526, acc.: 77.15%] [G loss: 1.101612]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1543 [D loss: 0.560715, acc.: 78.52%] [G loss: 1.103782]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1544 [D loss: 0.565434, acc.: 77.29%] [G loss: 1.094041]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1545 [D loss: 0.565170, acc.: 77.88%] [G loss: 1.094841]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1546 [D loss: 0.567958, acc.: 77.59%] [G loss: 1.091007]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1547 [D loss: 0.562988, acc.: 78.08%] [G loss: 1.095351]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1548 [D loss: 0.565713, acc.: 78.81%] [G loss: 1.098656]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1549 [D loss: 0.561304, acc.: 78.22%] [G loss: 1.099207]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1550 [D loss: 0.567011, acc.: 76.86%] [G loss: 1.107540]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1551 [D loss: 0.564290, acc.: 77.15%] [G loss: 1.100940]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1552 [D loss: 0.563789, acc.: 77.69%] [G loss: 1.109067]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1553 [D loss: 0.567903, acc.: 75.93%] [G loss: 1.100415]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1554 [D loss: 0.562382, acc.: 78.37%] [G loss: 1.096351]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1555 [D loss: 0.560850, acc.: 78.22%] [G loss: 1.105410]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1556 [D loss: 0.557820, acc.: 78.61%] [G loss: 1.106277]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1557 [D loss: 0.568103, acc.: 76.56%] [G loss: 1.104138]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1558 [D loss: 0.562173, acc.: 78.08%] [G loss: 1.116263]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1559 [D loss: 0.566811, acc.: 77.39%] [G loss: 1.115459]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1560 [D loss: 0.562441, acc.: 76.56%] [G loss: 1.123489]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1561 [D loss: 0.566160, acc.: 75.83%] [G loss: 1.118840]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1562 [D loss: 0.568668, acc.: 75.73%] [G loss: 1.113033]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1563 [D loss: 0.565981, acc.: 76.32%] [G loss: 1.113844]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1564 [D loss: 0.569960, acc.: 76.46%] [G loss: 1.116622]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1565 [D loss: 0.568888, acc.: 76.61%] [G loss: 1.117375]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1566 [D loss: 0.565531, acc.: 76.51%] [G loss: 1.116829]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1567 [D loss: 0.570477, acc.: 76.17%] [G loss: 1.112729]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1568 [D loss: 0.564251, acc.: 77.10%] [G loss: 1.109862]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1569 [D loss: 0.572008, acc.: 76.46%] [G loss: 1.113308]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1570 [D loss: 0.567666, acc.: 76.76%] [G loss: 1.110446]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1571 [D loss: 0.556330, acc.: 78.12%] [G loss: 1.112202]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1572 [D loss: 0.572159, acc.: 76.03%] [G loss: 1.105073]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1573 [D loss: 0.575210, acc.: 75.24%] [G loss: 1.115255]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1574 [D loss: 0.565562, acc.: 76.37%] [G loss: 1.109270]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1575 [D loss: 0.568150, acc.: 75.98%] [G loss: 1.110846]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1576 [D loss: 0.568082, acc.: 76.86%] [G loss: 1.116721]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1577 [D loss: 0.568243, acc.: 76.56%] [G loss: 1.113964]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1578 [D loss: 0.558190, acc.: 77.34%] [G loss: 1.124165]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1579 [D loss: 0.578092, acc.: 74.22%] [G loss: 1.111053]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1580 [D loss: 0.573240, acc.: 74.61%] [G loss: 1.111947]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1581 [D loss: 0.564830, acc.: 76.81%] [G loss: 1.105909]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1582 [D loss: 0.579491, acc.: 74.95%] [G loss: 1.103910]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1583 [D loss: 0.563922, acc.: 77.05%] [G loss: 1.119331]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1584 [D loss: 0.574937, acc.: 74.71%] [G loss: 1.116749]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1585 [D loss: 0.561819, acc.: 77.59%] [G loss: 1.117380]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1586 [D loss: 0.571764, acc.: 75.39%] [G loss: 1.115806]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1587 [D loss: 0.578718, acc.: 74.51%] [G loss: 1.105357]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1588 [D loss: 0.560029, acc.: 77.15%] [G loss: 1.103383]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1589 [D loss: 0.566998, acc.: 76.42%] [G loss: 1.118430]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1590 [D loss: 0.566413, acc.: 76.37%] [G loss: 1.115407]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1591 [D loss: 0.564712, acc.: 78.03%] [G loss: 1.117864]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1592 [D loss: 0.574732, acc.: 74.27%] [G loss: 1.111008]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1593 [D loss: 0.568907, acc.: 75.24%] [G loss: 1.118581]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1594 [D loss: 0.572131, acc.: 75.59%] [G loss: 1.110799]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1595 [D loss: 0.556779, acc.: 77.73%] [G loss: 1.110021]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1596 [D loss: 0.565962, acc.: 75.78%] [G loss: 1.111177]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1597 [D loss: 0.576561, acc.: 74.32%] [G loss: 1.120845]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1598 [D loss: 0.579598, acc.: 73.83%] [G loss: 1.109964]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1599 [D loss: 0.567803, acc.: 76.12%] [G loss: 1.108405]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1600 [D loss: 0.572670, acc.: 74.95%] [G loss: 1.119134]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1601 [D loss: 0.580216, acc.: 73.93%] [G loss: 1.124085]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1602 [D loss: 0.575316, acc.: 73.24%] [G loss: 1.110076]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1603 [D loss: 0.574095, acc.: 74.12%] [G loss: 1.114787]\n",
            "32/32 [==============================] - 0s 4ms/step\n",
            "1604 [D loss: 0.564487, acc.: 75.93%] [G loss: 1.110634]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1605 [D loss: 0.574350, acc.: 73.44%] [G loss: 1.108164]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1606 [D loss: 0.571577, acc.: 75.29%] [G loss: 1.120326]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1607 [D loss: 0.582827, acc.: 73.88%] [G loss: 1.123807]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1608 [D loss: 0.573877, acc.: 73.44%] [G loss: 1.123625]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1609 [D loss: 0.572166, acc.: 75.39%] [G loss: 1.129910]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1610 [D loss: 0.574102, acc.: 74.12%] [G loss: 1.126581]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1611 [D loss: 0.569929, acc.: 74.76%] [G loss: 1.111820]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1612 [D loss: 0.579929, acc.: 73.97%] [G loss: 1.126104]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1613 [D loss: 0.572460, acc.: 75.54%] [G loss: 1.120850]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1614 [D loss: 0.569206, acc.: 75.59%] [G loss: 1.122041]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1615 [D loss: 0.567012, acc.: 75.54%] [G loss: 1.118954]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1616 [D loss: 0.576893, acc.: 73.24%] [G loss: 1.121762]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1617 [D loss: 0.570016, acc.: 74.76%] [G loss: 1.127738]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1618 [D loss: 0.568354, acc.: 75.63%] [G loss: 1.123294]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1619 [D loss: 0.584664, acc.: 72.85%] [G loss: 1.111921]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1620 [D loss: 0.568516, acc.: 74.95%] [G loss: 1.122279]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1621 [D loss: 0.565966, acc.: 76.61%] [G loss: 1.116888]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1622 [D loss: 0.576301, acc.: 73.68%] [G loss: 1.112934]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1623 [D loss: 0.569875, acc.: 74.71%] [G loss: 1.123191]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1624 [D loss: 0.575075, acc.: 73.97%] [G loss: 1.125170]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1625 [D loss: 0.566965, acc.: 75.24%] [G loss: 1.130480]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1626 [D loss: 0.572488, acc.: 74.37%] [G loss: 1.111694]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1627 [D loss: 0.580968, acc.: 73.93%] [G loss: 1.120737]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1628 [D loss: 0.574969, acc.: 74.66%] [G loss: 1.118307]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1629 [D loss: 0.575802, acc.: 73.73%] [G loss: 1.111994]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1630 [D loss: 0.569278, acc.: 75.54%] [G loss: 1.117052]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1631 [D loss: 0.570696, acc.: 76.07%] [G loss: 1.122915]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1632 [D loss: 0.581868, acc.: 73.19%] [G loss: 1.128368]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1633 [D loss: 0.574929, acc.: 73.78%] [G loss: 1.120816]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1634 [D loss: 0.578917, acc.: 72.85%] [G loss: 1.131519]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1635 [D loss: 0.567651, acc.: 75.39%] [G loss: 1.115961]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1636 [D loss: 0.577520, acc.: 72.80%] [G loss: 1.120105]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1637 [D loss: 0.573054, acc.: 74.80%] [G loss: 1.121634]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1638 [D loss: 0.581291, acc.: 73.00%] [G loss: 1.117272]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1639 [D loss: 0.582322, acc.: 73.73%] [G loss: 1.125998]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1640 [D loss: 0.573396, acc.: 75.10%] [G loss: 1.131425]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1641 [D loss: 0.572566, acc.: 75.39%] [G loss: 1.131205]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1642 [D loss: 0.571418, acc.: 74.27%] [G loss: 1.129577]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1643 [D loss: 0.581876, acc.: 72.27%] [G loss: 1.139159]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1644 [D loss: 0.576065, acc.: 73.58%] [G loss: 1.126724]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1645 [D loss: 0.570865, acc.: 73.88%] [G loss: 1.127313]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1646 [D loss: 0.573198, acc.: 73.93%] [G loss: 1.131356]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1647 [D loss: 0.576999, acc.: 73.49%] [G loss: 1.125335]\n",
            "32/32 [==============================] - 0s 4ms/step\n",
            "1648 [D loss: 0.581583, acc.: 72.85%] [G loss: 1.127035]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1649 [D loss: 0.586546, acc.: 71.58%] [G loss: 1.122377]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1650 [D loss: 0.577593, acc.: 73.34%] [G loss: 1.117703]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1651 [D loss: 0.579963, acc.: 72.66%] [G loss: 1.133903]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1652 [D loss: 0.571225, acc.: 74.41%] [G loss: 1.122321]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1653 [D loss: 0.573038, acc.: 74.32%] [G loss: 1.122317]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1654 [D loss: 0.574935, acc.: 75.34%] [G loss: 1.112470]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1655 [D loss: 0.580363, acc.: 73.14%] [G loss: 1.128111]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1656 [D loss: 0.575434, acc.: 73.10%] [G loss: 1.126787]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1657 [D loss: 0.577249, acc.: 73.24%] [G loss: 1.126732]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1658 [D loss: 0.565355, acc.: 75.39%] [G loss: 1.121303]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1659 [D loss: 0.582629, acc.: 73.24%] [G loss: 1.137035]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1660 [D loss: 0.568984, acc.: 75.34%] [G loss: 1.147618]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1661 [D loss: 0.587106, acc.: 71.73%] [G loss: 1.130571]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1662 [D loss: 0.582597, acc.: 74.46%] [G loss: 1.129393]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1663 [D loss: 0.575637, acc.: 73.78%] [G loss: 1.124669]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1664 [D loss: 0.579283, acc.: 73.58%] [G loss: 1.123026]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1665 [D loss: 0.577889, acc.: 72.85%] [G loss: 1.131493]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1666 [D loss: 0.576499, acc.: 72.66%] [G loss: 1.116580]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1667 [D loss: 0.575034, acc.: 73.73%] [G loss: 1.121666]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1668 [D loss: 0.574348, acc.: 73.44%] [G loss: 1.116787]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1669 [D loss: 0.569486, acc.: 75.39%] [G loss: 1.129252]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1670 [D loss: 0.576534, acc.: 73.29%] [G loss: 1.122950]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1671 [D loss: 0.585745, acc.: 71.58%] [G loss: 1.128748]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1672 [D loss: 0.588644, acc.: 71.48%] [G loss: 1.122124]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1673 [D loss: 0.578057, acc.: 74.02%] [G loss: 1.115866]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1674 [D loss: 0.583056, acc.: 72.80%] [G loss: 1.121602]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1675 [D loss: 0.581545, acc.: 73.19%] [G loss: 1.126838]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1676 [D loss: 0.574028, acc.: 74.46%] [G loss: 1.119777]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1677 [D loss: 0.573454, acc.: 74.61%] [G loss: 1.122768]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1678 [D loss: 0.576926, acc.: 74.02%] [G loss: 1.121550]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1679 [D loss: 0.579405, acc.: 73.14%] [G loss: 1.122167]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1680 [D loss: 0.580939, acc.: 72.80%] [G loss: 1.130899]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1681 [D loss: 0.589591, acc.: 71.68%] [G loss: 1.122650]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1682 [D loss: 0.577453, acc.: 74.37%] [G loss: 1.121516]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1683 [D loss: 0.585383, acc.: 73.00%] [G loss: 1.119952]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1684 [D loss: 0.577253, acc.: 72.31%] [G loss: 1.131471]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1685 [D loss: 0.583402, acc.: 73.05%] [G loss: 1.138585]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1686 [D loss: 0.587448, acc.: 72.22%] [G loss: 1.127435]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1687 [D loss: 0.579647, acc.: 73.93%] [G loss: 1.127550]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1688 [D loss: 0.575028, acc.: 73.73%] [G loss: 1.135947]\n",
            "32/32 [==============================] - 0s 4ms/step\n",
            "1689 [D loss: 0.583789, acc.: 71.92%] [G loss: 1.125386]\n",
            "32/32 [==============================] - 0s 4ms/step\n",
            "1690 [D loss: 0.586019, acc.: 73.19%] [G loss: 1.136877]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1691 [D loss: 0.578515, acc.: 73.63%] [G loss: 1.137713]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1692 [D loss: 0.576389, acc.: 74.37%] [G loss: 1.140461]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1693 [D loss: 0.586183, acc.: 73.00%] [G loss: 1.113555]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1694 [D loss: 0.578534, acc.: 72.80%] [G loss: 1.140028]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1695 [D loss: 0.569886, acc.: 75.00%] [G loss: 1.130421]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1696 [D loss: 0.570052, acc.: 74.85%] [G loss: 1.140492]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1697 [D loss: 0.578413, acc.: 74.07%] [G loss: 1.144270]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1698 [D loss: 0.584914, acc.: 73.88%] [G loss: 1.127135]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1699 [D loss: 0.578889, acc.: 73.24%] [G loss: 1.143138]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1700 [D loss: 0.575892, acc.: 73.24%] [G loss: 1.128336]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1701 [D loss: 0.577355, acc.: 72.95%] [G loss: 1.137257]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1702 [D loss: 0.579127, acc.: 72.90%] [G loss: 1.129174]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1703 [D loss: 0.573099, acc.: 73.88%] [G loss: 1.122268]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1704 [D loss: 0.578495, acc.: 71.92%] [G loss: 1.132196]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1705 [D loss: 0.578330, acc.: 74.27%] [G loss: 1.135841]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1706 [D loss: 0.571251, acc.: 74.61%] [G loss: 1.140748]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1707 [D loss: 0.576592, acc.: 72.90%] [G loss: 1.129337]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1708 [D loss: 0.580277, acc.: 73.05%] [G loss: 1.144466]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1709 [D loss: 0.581615, acc.: 72.80%] [G loss: 1.134709]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1710 [D loss: 0.573476, acc.: 74.66%] [G loss: 1.137603]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1711 [D loss: 0.579976, acc.: 73.97%] [G loss: 1.135019]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1712 [D loss: 0.567955, acc.: 75.10%] [G loss: 1.134695]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1713 [D loss: 0.580710, acc.: 73.78%] [G loss: 1.130378]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1714 [D loss: 0.577263, acc.: 74.17%] [G loss: 1.131052]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1715 [D loss: 0.576976, acc.: 73.10%] [G loss: 1.144941]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1716 [D loss: 0.572308, acc.: 74.90%] [G loss: 1.144599]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1717 [D loss: 0.573878, acc.: 74.46%] [G loss: 1.135202]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1718 [D loss: 0.570137, acc.: 74.90%] [G loss: 1.140105]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1719 [D loss: 0.569598, acc.: 74.71%] [G loss: 1.132958]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1720 [D loss: 0.573050, acc.: 74.27%] [G loss: 1.135594]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1721 [D loss: 0.573472, acc.: 74.41%] [G loss: 1.145326]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1722 [D loss: 0.564639, acc.: 75.10%] [G loss: 1.149334]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1723 [D loss: 0.579086, acc.: 72.41%] [G loss: 1.146263]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1724 [D loss: 0.573453, acc.: 74.32%] [G loss: 1.146390]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1725 [D loss: 0.580969, acc.: 73.73%] [G loss: 1.146397]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1726 [D loss: 0.583256, acc.: 73.24%] [G loss: 1.125540]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1727 [D loss: 0.569160, acc.: 75.78%] [G loss: 1.140249]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1728 [D loss: 0.573900, acc.: 74.61%] [G loss: 1.152755]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1729 [D loss: 0.573218, acc.: 74.02%] [G loss: 1.147304]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1730 [D loss: 0.576912, acc.: 73.34%] [G loss: 1.150119]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1731 [D loss: 0.574122, acc.: 74.61%] [G loss: 1.140615]\n",
            "32/32 [==============================] - 0s 4ms/step\n",
            "1732 [D loss: 0.567327, acc.: 75.29%] [G loss: 1.146067]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1733 [D loss: 0.582243, acc.: 73.05%] [G loss: 1.135670]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1734 [D loss: 0.575661, acc.: 74.17%] [G loss: 1.135338]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1735 [D loss: 0.574444, acc.: 74.46%] [G loss: 1.152213]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1736 [D loss: 0.586402, acc.: 72.07%] [G loss: 1.166291]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1737 [D loss: 0.574236, acc.: 73.54%] [G loss: 1.149632]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1738 [D loss: 0.575020, acc.: 73.29%] [G loss: 1.151919]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1739 [D loss: 0.564176, acc.: 75.98%] [G loss: 1.146271]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1740 [D loss: 0.575401, acc.: 73.73%] [G loss: 1.143507]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1741 [D loss: 0.582837, acc.: 73.24%] [G loss: 1.140141]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1742 [D loss: 0.577560, acc.: 73.39%] [G loss: 1.134071]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1743 [D loss: 0.574066, acc.: 73.49%] [G loss: 1.150726]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1744 [D loss: 0.571921, acc.: 75.05%] [G loss: 1.145431]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1745 [D loss: 0.567812, acc.: 75.39%] [G loss: 1.156370]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1746 [D loss: 0.573129, acc.: 73.97%] [G loss: 1.154100]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1747 [D loss: 0.572899, acc.: 74.66%] [G loss: 1.142589]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1748 [D loss: 0.578145, acc.: 73.68%] [G loss: 1.154863]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1749 [D loss: 0.576035, acc.: 73.34%] [G loss: 1.137208]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1750 [D loss: 0.571320, acc.: 75.68%] [G loss: 1.149471]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1751 [D loss: 0.590591, acc.: 71.83%] [G loss: 1.140758]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1752 [D loss: 0.566849, acc.: 75.34%] [G loss: 1.143947]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1753 [D loss: 0.569927, acc.: 74.80%] [G loss: 1.152938]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1754 [D loss: 0.572600, acc.: 74.27%] [G loss: 1.152329]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1755 [D loss: 0.571335, acc.: 75.44%] [G loss: 1.151203]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1756 [D loss: 0.570327, acc.: 73.58%] [G loss: 1.139648]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1757 [D loss: 0.577302, acc.: 74.71%] [G loss: 1.156438]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1758 [D loss: 0.575384, acc.: 75.15%] [G loss: 1.153784]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1759 [D loss: 0.567722, acc.: 75.54%] [G loss: 1.144852]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1760 [D loss: 0.580173, acc.: 73.63%] [G loss: 1.153192]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1761 [D loss: 0.568905, acc.: 74.61%] [G loss: 1.160120]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1762 [D loss: 0.581231, acc.: 73.88%] [G loss: 1.152774]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1763 [D loss: 0.576959, acc.: 74.56%] [G loss: 1.176189]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1764 [D loss: 0.564270, acc.: 76.37%] [G loss: 1.159372]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1765 [D loss: 0.570545, acc.: 74.71%] [G loss: 1.168153]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1766 [D loss: 0.569799, acc.: 77.15%] [G loss: 1.158221]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1767 [D loss: 0.575250, acc.: 73.83%] [G loss: 1.171280]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1768 [D loss: 0.568783, acc.: 76.32%] [G loss: 1.160349]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1769 [D loss: 0.575382, acc.: 74.71%] [G loss: 1.160879]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1770 [D loss: 0.572133, acc.: 75.34%] [G loss: 1.166819]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1771 [D loss: 0.577763, acc.: 73.97%] [G loss: 1.164762]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1772 [D loss: 0.570944, acc.: 75.15%] [G loss: 1.171256]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1773 [D loss: 0.577444, acc.: 73.54%] [G loss: 1.153306]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1774 [D loss: 0.575603, acc.: 73.88%] [G loss: 1.142028]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1775 [D loss: 0.568341, acc.: 76.17%] [G loss: 1.155945]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1776 [D loss: 0.577340, acc.: 74.27%] [G loss: 1.175658]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1777 [D loss: 0.578809, acc.: 74.95%] [G loss: 1.165120]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1778 [D loss: 0.579514, acc.: 73.54%] [G loss: 1.178402]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1779 [D loss: 0.589026, acc.: 72.85%] [G loss: 1.176962]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1780 [D loss: 0.575395, acc.: 74.37%] [G loss: 1.166575]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1781 [D loss: 0.573713, acc.: 76.03%] [G loss: 1.150697]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1782 [D loss: 0.578885, acc.: 73.83%] [G loss: 1.156760]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1783 [D loss: 0.572250, acc.: 74.85%] [G loss: 1.162996]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1784 [D loss: 0.574016, acc.: 75.63%] [G loss: 1.163113]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1785 [D loss: 0.582614, acc.: 73.14%] [G loss: 1.163334]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1786 [D loss: 0.564379, acc.: 75.78%] [G loss: 1.169136]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1787 [D loss: 0.581589, acc.: 73.00%] [G loss: 1.164779]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1788 [D loss: 0.572075, acc.: 74.61%] [G loss: 1.161680]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1789 [D loss: 0.575321, acc.: 74.85%] [G loss: 1.168013]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1790 [D loss: 0.570055, acc.: 75.05%] [G loss: 1.158692]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1791 [D loss: 0.584838, acc.: 72.71%] [G loss: 1.157493]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1792 [D loss: 0.577178, acc.: 73.58%] [G loss: 1.166879]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1793 [D loss: 0.564372, acc.: 76.61%] [G loss: 1.153842]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1794 [D loss: 0.575714, acc.: 73.49%] [G loss: 1.158015]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1795 [D loss: 0.580000, acc.: 73.49%] [G loss: 1.170001]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1796 [D loss: 0.583385, acc.: 71.83%] [G loss: 1.162383]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1797 [D loss: 0.576934, acc.: 75.20%] [G loss: 1.162159]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1798 [D loss: 0.567659, acc.: 74.66%] [G loss: 1.166480]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1799 [D loss: 0.584937, acc.: 72.41%] [G loss: 1.155050]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1800 [D loss: 0.588323, acc.: 71.78%] [G loss: 1.157510]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1801 [D loss: 0.577338, acc.: 73.29%] [G loss: 1.162398]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1802 [D loss: 0.575927, acc.: 73.34%] [G loss: 1.181624]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1803 [D loss: 0.582282, acc.: 73.44%] [G loss: 1.167104]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1804 [D loss: 0.574436, acc.: 73.54%] [G loss: 1.167934]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1805 [D loss: 0.571672, acc.: 75.20%] [G loss: 1.161343]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1806 [D loss: 0.569176, acc.: 74.32%] [G loss: 1.169061]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1807 [D loss: 0.588344, acc.: 72.46%] [G loss: 1.160140]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1808 [D loss: 0.574550, acc.: 73.49%] [G loss: 1.164061]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1809 [D loss: 0.584907, acc.: 72.41%] [G loss: 1.161210]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1810 [D loss: 0.580880, acc.: 73.44%] [G loss: 1.150269]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1811 [D loss: 0.572524, acc.: 74.37%] [G loss: 1.151411]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1812 [D loss: 0.577503, acc.: 74.27%] [G loss: 1.180401]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1813 [D loss: 0.575921, acc.: 74.02%] [G loss: 1.153457]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1814 [D loss: 0.580443, acc.: 73.19%] [G loss: 1.157731]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1815 [D loss: 0.577326, acc.: 73.24%] [G loss: 1.161245]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1816 [D loss: 0.582293, acc.: 72.71%] [G loss: 1.181158]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1817 [D loss: 0.581947, acc.: 72.22%] [G loss: 1.180598]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1818 [D loss: 0.576063, acc.: 74.07%] [G loss: 1.162556]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1819 [D loss: 0.585931, acc.: 71.83%] [G loss: 1.164266]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1820 [D loss: 0.583656, acc.: 71.68%] [G loss: 1.172040]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1821 [D loss: 0.578150, acc.: 74.41%] [G loss: 1.176531]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1822 [D loss: 0.594770, acc.: 69.53%] [G loss: 1.167369]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1823 [D loss: 0.581723, acc.: 72.66%] [G loss: 1.158061]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1824 [D loss: 0.576871, acc.: 74.22%] [G loss: 1.151650]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1825 [D loss: 0.580074, acc.: 74.17%] [G loss: 1.156019]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1826 [D loss: 0.586848, acc.: 71.04%] [G loss: 1.162976]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1827 [D loss: 0.585300, acc.: 72.61%] [G loss: 1.164653]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1828 [D loss: 0.590183, acc.: 71.68%] [G loss: 1.151052]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1829 [D loss: 0.580155, acc.: 72.71%] [G loss: 1.171538]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1830 [D loss: 0.587188, acc.: 72.56%] [G loss: 1.151515]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1831 [D loss: 0.585399, acc.: 72.85%] [G loss: 1.169253]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1832 [D loss: 0.586064, acc.: 71.78%] [G loss: 1.165625]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1833 [D loss: 0.588699, acc.: 71.63%] [G loss: 1.179416]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1834 [D loss: 0.592560, acc.: 71.00%] [G loss: 1.151432]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1835 [D loss: 0.590530, acc.: 71.68%] [G loss: 1.159765]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1836 [D loss: 0.576565, acc.: 74.17%] [G loss: 1.182044]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1837 [D loss: 0.589993, acc.: 71.04%] [G loss: 1.149444]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1838 [D loss: 0.576036, acc.: 73.24%] [G loss: 1.172420]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1839 [D loss: 0.581187, acc.: 71.83%] [G loss: 1.170963]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1840 [D loss: 0.583255, acc.: 72.31%] [G loss: 1.166745]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1841 [D loss: 0.590201, acc.: 70.95%] [G loss: 1.167764]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1842 [D loss: 0.589990, acc.: 71.44%] [G loss: 1.161104]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1843 [D loss: 0.586130, acc.: 72.41%] [G loss: 1.158945]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1844 [D loss: 0.585256, acc.: 72.75%] [G loss: 1.165644]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1845 [D loss: 0.590143, acc.: 71.04%] [G loss: 1.163488]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1846 [D loss: 0.591187, acc.: 70.90%] [G loss: 1.156059]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1847 [D loss: 0.590929, acc.: 72.85%] [G loss: 1.171237]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1848 [D loss: 0.589153, acc.: 70.75%] [G loss: 1.164698]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1849 [D loss: 0.585046, acc.: 72.51%] [G loss: 1.157986]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1850 [D loss: 0.595553, acc.: 69.73%] [G loss: 1.158998]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1851 [D loss: 0.593505, acc.: 71.44%] [G loss: 1.165598]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1852 [D loss: 0.585293, acc.: 71.97%] [G loss: 1.152832]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1853 [D loss: 0.597197, acc.: 70.17%] [G loss: 1.162770]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1854 [D loss: 0.587910, acc.: 72.31%] [G loss: 1.162749]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1855 [D loss: 0.588822, acc.: 71.19%] [G loss: 1.150963]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1856 [D loss: 0.586757, acc.: 72.75%] [G loss: 1.155866]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1857 [D loss: 0.596153, acc.: 70.70%] [G loss: 1.155359]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1858 [D loss: 0.581399, acc.: 73.10%] [G loss: 1.166990]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1859 [D loss: 0.590598, acc.: 70.95%] [G loss: 1.158796]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1860 [D loss: 0.601787, acc.: 70.02%] [G loss: 1.161329]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1861 [D loss: 0.599955, acc.: 70.90%] [G loss: 1.152350]\n",
            "32/32 [==============================] - 0s 4ms/step\n",
            "1862 [D loss: 0.593210, acc.: 71.63%] [G loss: 1.155121]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1863 [D loss: 0.583862, acc.: 73.05%] [G loss: 1.171843]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1864 [D loss: 0.601083, acc.: 69.29%] [G loss: 1.159101]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1865 [D loss: 0.585673, acc.: 71.83%] [G loss: 1.151807]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1866 [D loss: 0.592301, acc.: 71.48%] [G loss: 1.153735]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1867 [D loss: 0.590702, acc.: 71.73%] [G loss: 1.167178]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1868 [D loss: 0.585106, acc.: 72.22%] [G loss: 1.163140]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1869 [D loss: 0.594088, acc.: 70.70%] [G loss: 1.156834]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1870 [D loss: 0.602398, acc.: 69.63%] [G loss: 1.157290]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1871 [D loss: 0.588645, acc.: 71.39%] [G loss: 1.156471]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1872 [D loss: 0.583560, acc.: 72.66%] [G loss: 1.143326]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1873 [D loss: 0.591141, acc.: 71.00%] [G loss: 1.150807]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1874 [D loss: 0.595109, acc.: 71.19%] [G loss: 1.154146]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1875 [D loss: 0.595853, acc.: 69.24%] [G loss: 1.155026]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1876 [D loss: 0.590147, acc.: 71.78%] [G loss: 1.135979]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1877 [D loss: 0.592168, acc.: 70.85%] [G loss: 1.149130]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1878 [D loss: 0.594803, acc.: 70.70%] [G loss: 1.146523]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1879 [D loss: 0.596293, acc.: 70.61%] [G loss: 1.148976]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1880 [D loss: 0.609793, acc.: 67.19%] [G loss: 1.152740]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1881 [D loss: 0.592596, acc.: 72.17%] [G loss: 1.148541]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1882 [D loss: 0.603021, acc.: 69.73%] [G loss: 1.158868]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1883 [D loss: 0.597841, acc.: 70.70%] [G loss: 1.172602]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1884 [D loss: 0.599895, acc.: 71.14%] [G loss: 1.147385]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1885 [D loss: 0.591416, acc.: 72.36%] [G loss: 1.146869]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1886 [D loss: 0.596926, acc.: 69.73%] [G loss: 1.141610]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1887 [D loss: 0.596713, acc.: 71.24%] [G loss: 1.148911]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1888 [D loss: 0.598111, acc.: 70.07%] [G loss: 1.147731]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1889 [D loss: 0.590475, acc.: 71.78%] [G loss: 1.160705]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1890 [D loss: 0.595323, acc.: 70.90%] [G loss: 1.150017]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1891 [D loss: 0.606937, acc.: 69.43%] [G loss: 1.137814]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1892 [D loss: 0.597032, acc.: 69.34%] [G loss: 1.134390]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1893 [D loss: 0.587845, acc.: 72.36%] [G loss: 1.143366]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1894 [D loss: 0.581207, acc.: 73.73%] [G loss: 1.157678]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1895 [D loss: 0.591755, acc.: 71.97%] [G loss: 1.139514]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1896 [D loss: 0.598403, acc.: 69.58%] [G loss: 1.144527]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1897 [D loss: 0.587284, acc.: 72.27%] [G loss: 1.155293]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1898 [D loss: 0.590606, acc.: 72.61%] [G loss: 1.141693]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1899 [D loss: 0.603246, acc.: 70.02%] [G loss: 1.146508]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1900 [D loss: 0.600264, acc.: 70.36%] [G loss: 1.149885]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1901 [D loss: 0.588287, acc.: 70.90%] [G loss: 1.152151]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1902 [D loss: 0.607047, acc.: 68.90%] [G loss: 1.142928]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1903 [D loss: 0.587696, acc.: 71.88%] [G loss: 1.150479]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1904 [D loss: 0.583588, acc.: 72.80%] [G loss: 1.157938]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1905 [D loss: 0.598343, acc.: 68.75%] [G loss: 1.144069]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1906 [D loss: 0.600157, acc.: 69.82%] [G loss: 1.149865]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1907 [D loss: 0.605595, acc.: 69.73%] [G loss: 1.150403]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1908 [D loss: 0.604324, acc.: 68.85%] [G loss: 1.153759]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1909 [D loss: 0.593009, acc.: 71.34%] [G loss: 1.137384]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1910 [D loss: 0.594114, acc.: 71.39%] [G loss: 1.145092]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1911 [D loss: 0.592463, acc.: 71.04%] [G loss: 1.150964]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1912 [D loss: 0.591407, acc.: 72.17%] [G loss: 1.148236]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1913 [D loss: 0.598652, acc.: 69.58%] [G loss: 1.156759]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1914 [D loss: 0.601980, acc.: 69.68%] [G loss: 1.154712]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1915 [D loss: 0.599255, acc.: 70.31%] [G loss: 1.161026]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1916 [D loss: 0.595328, acc.: 70.95%] [G loss: 1.148257]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1917 [D loss: 0.599369, acc.: 69.19%] [G loss: 1.153964]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1918 [D loss: 0.591935, acc.: 71.92%] [G loss: 1.144794]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1919 [D loss: 0.595138, acc.: 71.68%] [G loss: 1.143376]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1920 [D loss: 0.592215, acc.: 71.48%] [G loss: 1.149644]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1921 [D loss: 0.600836, acc.: 69.68%] [G loss: 1.139824]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1922 [D loss: 0.598304, acc.: 70.07%] [G loss: 1.141076]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1923 [D loss: 0.605862, acc.: 68.41%] [G loss: 1.138819]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1924 [D loss: 0.604242, acc.: 68.95%] [G loss: 1.148915]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1925 [D loss: 0.596651, acc.: 71.09%] [G loss: 1.142703]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1926 [D loss: 0.594194, acc.: 71.53%] [G loss: 1.141999]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1927 [D loss: 0.600775, acc.: 70.17%] [G loss: 1.147440]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1928 [D loss: 0.591574, acc.: 72.22%] [G loss: 1.148269]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1929 [D loss: 0.593835, acc.: 71.78%] [G loss: 1.144949]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1930 [D loss: 0.592800, acc.: 70.17%] [G loss: 1.155504]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1931 [D loss: 0.594643, acc.: 72.12%] [G loss: 1.137311]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1932 [D loss: 0.603234, acc.: 68.16%] [G loss: 1.144706]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1933 [D loss: 0.588015, acc.: 72.31%] [G loss: 1.132289]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1934 [D loss: 0.593080, acc.: 71.09%] [G loss: 1.152049]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1935 [D loss: 0.591605, acc.: 71.63%] [G loss: 1.145986]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1936 [D loss: 0.595862, acc.: 71.48%] [G loss: 1.145075]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1937 [D loss: 0.593200, acc.: 71.73%] [G loss: 1.150710]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1938 [D loss: 0.592418, acc.: 71.14%] [G loss: 1.156654]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1939 [D loss: 0.598532, acc.: 69.58%] [G loss: 1.156979]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1940 [D loss: 0.599099, acc.: 70.36%] [G loss: 1.163756]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1941 [D loss: 0.596626, acc.: 70.90%] [G loss: 1.147092]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1942 [D loss: 0.603682, acc.: 70.02%] [G loss: 1.136649]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1943 [D loss: 0.592899, acc.: 71.53%] [G loss: 1.153977]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1944 [D loss: 0.594859, acc.: 70.61%] [G loss: 1.135674]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1945 [D loss: 0.594949, acc.: 71.58%] [G loss: 1.143294]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1946 [D loss: 0.598448, acc.: 71.00%] [G loss: 1.143229]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1947 [D loss: 0.597508, acc.: 70.61%] [G loss: 1.154670]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1948 [D loss: 0.598808, acc.: 69.97%] [G loss: 1.151195]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1949 [D loss: 0.598311, acc.: 70.36%] [G loss: 1.140347]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1950 [D loss: 0.591332, acc.: 71.97%] [G loss: 1.139331]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1951 [D loss: 0.598943, acc.: 70.31%] [G loss: 1.153481]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1952 [D loss: 0.591426, acc.: 73.00%] [G loss: 1.147015]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1953 [D loss: 0.594582, acc.: 71.44%] [G loss: 1.128405]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1954 [D loss: 0.605467, acc.: 70.95%] [G loss: 1.142102]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1955 [D loss: 0.593044, acc.: 72.80%] [G loss: 1.134017]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1956 [D loss: 0.581616, acc.: 74.80%] [G loss: 1.139487]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1957 [D loss: 0.588063, acc.: 72.80%] [G loss: 1.144007]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1958 [D loss: 0.600441, acc.: 70.07%] [G loss: 1.130347]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1959 [D loss: 0.586703, acc.: 72.85%] [G loss: 1.149480]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1960 [D loss: 0.593216, acc.: 71.00%] [G loss: 1.150663]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1961 [D loss: 0.596830, acc.: 71.09%] [G loss: 1.152429]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1962 [D loss: 0.598457, acc.: 70.80%] [G loss: 1.144069]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1963 [D loss: 0.595858, acc.: 70.90%] [G loss: 1.146734]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1964 [D loss: 0.594335, acc.: 72.46%] [G loss: 1.139792]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1965 [D loss: 0.600108, acc.: 69.73%] [G loss: 1.156026]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1966 [D loss: 0.597812, acc.: 70.51%] [G loss: 1.136234]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1967 [D loss: 0.598055, acc.: 70.65%] [G loss: 1.143244]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1968 [D loss: 0.586481, acc.: 72.71%] [G loss: 1.147926]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1969 [D loss: 0.597094, acc.: 70.95%] [G loss: 1.156659]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1970 [D loss: 0.593391, acc.: 71.63%] [G loss: 1.149430]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1971 [D loss: 0.590674, acc.: 72.90%] [G loss: 1.139284]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1972 [D loss: 0.586403, acc.: 72.12%] [G loss: 1.148105]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1973 [D loss: 0.597292, acc.: 70.75%] [G loss: 1.150263]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1974 [D loss: 0.598599, acc.: 70.21%] [G loss: 1.156097]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1975 [D loss: 0.589148, acc.: 72.95%] [G loss: 1.148470]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1976 [D loss: 0.592161, acc.: 71.48%] [G loss: 1.145048]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1977 [D loss: 0.592276, acc.: 72.56%] [G loss: 1.140795]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1978 [D loss: 0.590223, acc.: 73.49%] [G loss: 1.138850]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1979 [D loss: 0.589210, acc.: 71.00%] [G loss: 1.148471]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1980 [D loss: 0.594246, acc.: 73.00%] [G loss: 1.141029]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1981 [D loss: 0.586033, acc.: 72.80%] [G loss: 1.139594]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1982 [D loss: 0.593077, acc.: 70.70%] [G loss: 1.135836]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1983 [D loss: 0.592664, acc.: 71.53%] [G loss: 1.137230]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1984 [D loss: 0.594033, acc.: 71.09%] [G loss: 1.147263]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1985 [D loss: 0.593150, acc.: 71.24%] [G loss: 1.154987]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1986 [D loss: 0.590826, acc.: 72.02%] [G loss: 1.133548]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1987 [D loss: 0.588677, acc.: 71.78%] [G loss: 1.145249]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1988 [D loss: 0.594479, acc.: 72.71%] [G loss: 1.149719]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1989 [D loss: 0.597585, acc.: 70.90%] [G loss: 1.137950]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1990 [D loss: 0.586198, acc.: 74.12%] [G loss: 1.140841]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1991 [D loss: 0.588482, acc.: 73.10%] [G loss: 1.150132]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1992 [D loss: 0.582688, acc.: 73.34%] [G loss: 1.138693]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1993 [D loss: 0.596369, acc.: 70.90%] [G loss: 1.159071]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1994 [D loss: 0.587066, acc.: 71.48%] [G loss: 1.145432]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1995 [D loss: 0.589514, acc.: 73.24%] [G loss: 1.140481]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1996 [D loss: 0.594358, acc.: 72.31%] [G loss: 1.154044]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1997 [D loss: 0.581988, acc.: 73.14%] [G loss: 1.156593]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "1998 [D loss: 0.583383, acc.: 74.12%] [G loss: 1.152988]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "1999 [D loss: 0.587380, acc.: 73.44%] [G loss: 1.165709]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2000 [D loss: 0.590817, acc.: 72.80%] [G loss: 1.147778]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2001 [D loss: 0.582410, acc.: 73.34%] [G loss: 1.150716]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2002 [D loss: 0.585348, acc.: 72.41%] [G loss: 1.150592]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2003 [D loss: 0.585133, acc.: 73.39%] [G loss: 1.167026]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2004 [D loss: 0.589031, acc.: 72.07%] [G loss: 1.151762]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2005 [D loss: 0.587469, acc.: 71.63%] [G loss: 1.153685]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2006 [D loss: 0.585009, acc.: 72.56%] [G loss: 1.156976]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2007 [D loss: 0.594283, acc.: 72.07%] [G loss: 1.146522]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2008 [D loss: 0.578771, acc.: 73.58%] [G loss: 1.153661]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2009 [D loss: 0.593211, acc.: 71.44%] [G loss: 1.150919]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2010 [D loss: 0.577310, acc.: 74.12%] [G loss: 1.151830]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2011 [D loss: 0.580579, acc.: 73.49%] [G loss: 1.180306]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2012 [D loss: 0.587911, acc.: 71.88%] [G loss: 1.162277]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2013 [D loss: 0.586402, acc.: 73.83%] [G loss: 1.147983]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2014 [D loss: 0.592472, acc.: 70.65%] [G loss: 1.148311]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2015 [D loss: 0.582547, acc.: 74.12%] [G loss: 1.151202]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2016 [D loss: 0.576725, acc.: 74.12%] [G loss: 1.156320]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2017 [D loss: 0.577958, acc.: 74.41%] [G loss: 1.149619]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2018 [D loss: 0.577525, acc.: 75.00%] [G loss: 1.159464]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2019 [D loss: 0.580730, acc.: 74.17%] [G loss: 1.160641]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2020 [D loss: 0.574422, acc.: 74.90%] [G loss: 1.159871]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2021 [D loss: 0.587366, acc.: 73.63%] [G loss: 1.155676]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2022 [D loss: 0.578948, acc.: 74.85%] [G loss: 1.147469]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2023 [D loss: 0.584975, acc.: 73.10%] [G loss: 1.155606]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2024 [D loss: 0.588708, acc.: 73.10%] [G loss: 1.152418]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2025 [D loss: 0.585058, acc.: 73.29%] [G loss: 1.161123]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2026 [D loss: 0.575989, acc.: 74.12%] [G loss: 1.157024]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2027 [D loss: 0.572925, acc.: 75.44%] [G loss: 1.164437]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2028 [D loss: 0.583241, acc.: 74.02%] [G loss: 1.154088]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2029 [D loss: 0.575916, acc.: 74.61%] [G loss: 1.167386]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2030 [D loss: 0.570261, acc.: 75.78%] [G loss: 1.165891]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2031 [D loss: 0.571013, acc.: 75.15%] [G loss: 1.166882]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2032 [D loss: 0.575988, acc.: 75.29%] [G loss: 1.163198]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2033 [D loss: 0.568514, acc.: 76.81%] [G loss: 1.172630]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2034 [D loss: 0.571662, acc.: 75.78%] [G loss: 1.174166]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2035 [D loss: 0.585949, acc.: 73.39%] [G loss: 1.162684]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2036 [D loss: 0.578527, acc.: 74.37%] [G loss: 1.160335]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2037 [D loss: 0.573504, acc.: 74.56%] [G loss: 1.167913]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2038 [D loss: 0.578261, acc.: 73.97%] [G loss: 1.161725]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2039 [D loss: 0.580320, acc.: 75.15%] [G loss: 1.179690]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2040 [D loss: 0.580845, acc.: 74.27%] [G loss: 1.169811]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2041 [D loss: 0.575794, acc.: 74.32%] [G loss: 1.174592]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2042 [D loss: 0.566071, acc.: 76.27%] [G loss: 1.173138]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2043 [D loss: 0.575042, acc.: 75.29%] [G loss: 1.173961]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2044 [D loss: 0.570372, acc.: 74.90%] [G loss: 1.168386]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2045 [D loss: 0.569801, acc.: 75.44%] [G loss: 1.177121]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2046 [D loss: 0.571053, acc.: 76.32%] [G loss: 1.178632]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2047 [D loss: 0.570732, acc.: 76.27%] [G loss: 1.188910]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2048 [D loss: 0.568152, acc.: 75.98%] [G loss: 1.177764]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2049 [D loss: 0.576921, acc.: 75.59%] [G loss: 1.181729]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2050 [D loss: 0.564736, acc.: 76.95%] [G loss: 1.177046]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2051 [D loss: 0.566379, acc.: 76.61%] [G loss: 1.166282]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2052 [D loss: 0.568174, acc.: 75.29%] [G loss: 1.189036]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2053 [D loss: 0.573217, acc.: 76.12%] [G loss: 1.194143]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2054 [D loss: 0.572212, acc.: 75.15%] [G loss: 1.184140]\n",
            "32/32 [==============================] - 0s 4ms/step\n",
            "2055 [D loss: 0.579218, acc.: 74.27%] [G loss: 1.172473]\n",
            "32/32 [==============================] - 0s 4ms/step\n",
            "2056 [D loss: 0.578774, acc.: 74.27%] [G loss: 1.173825]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2057 [D loss: 0.573442, acc.: 75.39%] [G loss: 1.164820]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2058 [D loss: 0.572660, acc.: 73.54%] [G loss: 1.181455]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2059 [D loss: 0.563234, acc.: 77.15%] [G loss: 1.178994]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2060 [D loss: 0.571332, acc.: 75.98%] [G loss: 1.177837]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2061 [D loss: 0.556462, acc.: 77.44%] [G loss: 1.192670]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2062 [D loss: 0.569564, acc.: 76.86%] [G loss: 1.183671]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2063 [D loss: 0.573129, acc.: 74.41%] [G loss: 1.182690]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2064 [D loss: 0.570974, acc.: 76.17%] [G loss: 1.180907]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2065 [D loss: 0.569600, acc.: 76.17%] [G loss: 1.180394]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2066 [D loss: 0.577796, acc.: 74.90%] [G loss: 1.191966]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2067 [D loss: 0.563215, acc.: 77.69%] [G loss: 1.191790]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2068 [D loss: 0.577289, acc.: 74.41%] [G loss: 1.180381]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2069 [D loss: 0.557044, acc.: 77.15%] [G loss: 1.201791]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2070 [D loss: 0.570049, acc.: 75.78%] [G loss: 1.193473]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2071 [D loss: 0.567978, acc.: 76.61%] [G loss: 1.195820]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2072 [D loss: 0.562487, acc.: 77.64%] [G loss: 1.200374]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2073 [D loss: 0.565427, acc.: 75.68%] [G loss: 1.199180]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2074 [D loss: 0.573166, acc.: 75.78%] [G loss: 1.185326]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2075 [D loss: 0.570988, acc.: 74.56%] [G loss: 1.192245]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2076 [D loss: 0.569560, acc.: 74.95%] [G loss: 1.205388]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2077 [D loss: 0.560458, acc.: 77.78%] [G loss: 1.187555]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2078 [D loss: 0.564406, acc.: 76.56%] [G loss: 1.196422]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2079 [D loss: 0.558415, acc.: 77.05%] [G loss: 1.211294]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2080 [D loss: 0.572105, acc.: 74.85%] [G loss: 1.196586]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2081 [D loss: 0.566377, acc.: 75.93%] [G loss: 1.211970]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2082 [D loss: 0.556346, acc.: 78.27%] [G loss: 1.197326]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2083 [D loss: 0.559769, acc.: 76.17%] [G loss: 1.209559]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2084 [D loss: 0.568004, acc.: 76.32%] [G loss: 1.209503]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2085 [D loss: 0.563465, acc.: 76.90%] [G loss: 1.209961]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2086 [D loss: 0.580863, acc.: 72.71%] [G loss: 1.197986]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2087 [D loss: 0.565436, acc.: 76.90%] [G loss: 1.204876]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2088 [D loss: 0.560431, acc.: 76.81%] [G loss: 1.205963]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2089 [D loss: 0.560140, acc.: 77.05%] [G loss: 1.199476]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2090 [D loss: 0.564741, acc.: 76.61%] [G loss: 1.198311]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2091 [D loss: 0.566310, acc.: 75.29%] [G loss: 1.191826]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2092 [D loss: 0.568481, acc.: 76.22%] [G loss: 1.192195]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2093 [D loss: 0.566485, acc.: 76.03%] [G loss: 1.207248]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2094 [D loss: 0.576547, acc.: 74.80%] [G loss: 1.201834]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2095 [D loss: 0.561588, acc.: 76.56%] [G loss: 1.197542]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2096 [D loss: 0.552506, acc.: 77.73%] [G loss: 1.213778]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2097 [D loss: 0.563892, acc.: 76.17%] [G loss: 1.210563]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2098 [D loss: 0.566275, acc.: 75.88%] [G loss: 1.211357]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2099 [D loss: 0.569107, acc.: 74.90%] [G loss: 1.216084]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2100 [D loss: 0.569022, acc.: 75.29%] [G loss: 1.203943]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2101 [D loss: 0.565655, acc.: 76.17%] [G loss: 1.214617]\n",
            "32/32 [==============================] - 0s 4ms/step\n",
            "2102 [D loss: 0.562639, acc.: 77.05%] [G loss: 1.209057]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2103 [D loss: 0.573736, acc.: 75.15%] [G loss: 1.213780]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2104 [D loss: 0.568257, acc.: 76.71%] [G loss: 1.203727]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2105 [D loss: 0.559794, acc.: 77.00%] [G loss: 1.208266]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2106 [D loss: 0.559514, acc.: 77.20%] [G loss: 1.222890]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2107 [D loss: 0.567428, acc.: 75.10%] [G loss: 1.218242]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2108 [D loss: 0.563954, acc.: 75.29%] [G loss: 1.208873]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2109 [D loss: 0.566033, acc.: 76.61%] [G loss: 1.222473]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2110 [D loss: 0.551665, acc.: 78.32%] [G loss: 1.219733]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2111 [D loss: 0.561729, acc.: 77.29%] [G loss: 1.216524]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2112 [D loss: 0.570833, acc.: 75.54%] [G loss: 1.205707]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2113 [D loss: 0.566648, acc.: 74.56%] [G loss: 1.198284]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2114 [D loss: 0.569341, acc.: 75.54%] [G loss: 1.217454]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2115 [D loss: 0.550917, acc.: 78.71%] [G loss: 1.218914]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2116 [D loss: 0.553458, acc.: 78.03%] [G loss: 1.229734]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2117 [D loss: 0.562632, acc.: 76.42%] [G loss: 1.233479]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2118 [D loss: 0.560788, acc.: 77.25%] [G loss: 1.228654]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2119 [D loss: 0.562325, acc.: 75.49%] [G loss: 1.226521]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2120 [D loss: 0.559285, acc.: 76.90%] [G loss: 1.213807]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2121 [D loss: 0.564123, acc.: 76.07%] [G loss: 1.229720]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2122 [D loss: 0.571935, acc.: 75.49%] [G loss: 1.206707]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2123 [D loss: 0.565138, acc.: 76.07%] [G loss: 1.226646]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2124 [D loss: 0.562886, acc.: 75.98%] [G loss: 1.219677]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2125 [D loss: 0.570639, acc.: 75.63%] [G loss: 1.216408]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2126 [D loss: 0.561333, acc.: 76.61%] [G loss: 1.225964]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2127 [D loss: 0.563276, acc.: 75.93%] [G loss: 1.227562]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2128 [D loss: 0.569284, acc.: 75.24%] [G loss: 1.226816]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2129 [D loss: 0.568139, acc.: 75.24%] [G loss: 1.220998]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2130 [D loss: 0.566515, acc.: 75.68%] [G loss: 1.216013]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2131 [D loss: 0.565888, acc.: 76.22%] [G loss: 1.223938]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2132 [D loss: 0.566147, acc.: 76.07%] [G loss: 1.226500]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2133 [D loss: 0.563432, acc.: 75.93%] [G loss: 1.204854]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2134 [D loss: 0.567542, acc.: 75.39%] [G loss: 1.229019]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2135 [D loss: 0.555945, acc.: 78.37%] [G loss: 1.227378]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2136 [D loss: 0.564648, acc.: 74.95%] [G loss: 1.232308]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2137 [D loss: 0.572621, acc.: 74.32%] [G loss: 1.230915]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2138 [D loss: 0.571640, acc.: 74.66%] [G loss: 1.208082]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2139 [D loss: 0.575990, acc.: 74.37%] [G loss: 1.214922]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2140 [D loss: 0.563633, acc.: 75.93%] [G loss: 1.210333]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2141 [D loss: 0.564845, acc.: 75.59%] [G loss: 1.224830]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2142 [D loss: 0.552293, acc.: 77.49%] [G loss: 1.223273]\n",
            "32/32 [==============================] - 0s 4ms/step\n",
            "2143 [D loss: 0.557105, acc.: 77.20%] [G loss: 1.223647]\n",
            "32/32 [==============================] - 0s 4ms/step\n",
            "2144 [D loss: 0.554038, acc.: 77.73%] [G loss: 1.223445]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2145 [D loss: 0.572487, acc.: 74.07%] [G loss: 1.227161]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2146 [D loss: 0.568610, acc.: 75.59%] [G loss: 1.211688]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2147 [D loss: 0.564678, acc.: 76.03%] [G loss: 1.208521]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2148 [D loss: 0.571822, acc.: 75.24%] [G loss: 1.223387]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2149 [D loss: 0.562444, acc.: 76.51%] [G loss: 1.221429]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2150 [D loss: 0.565199, acc.: 74.71%] [G loss: 1.230044]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2151 [D loss: 0.562865, acc.: 75.78%] [G loss: 1.234549]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2152 [D loss: 0.566083, acc.: 75.98%] [G loss: 1.221807]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2153 [D loss: 0.557637, acc.: 76.03%] [G loss: 1.240302]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2154 [D loss: 0.578339, acc.: 73.39%] [G loss: 1.216210]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2155 [D loss: 0.570735, acc.: 74.95%] [G loss: 1.226285]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2156 [D loss: 0.565012, acc.: 75.29%] [G loss: 1.222785]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2157 [D loss: 0.552740, acc.: 77.10%] [G loss: 1.224032]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2158 [D loss: 0.562058, acc.: 76.90%] [G loss: 1.227352]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2159 [D loss: 0.573694, acc.: 73.29%] [G loss: 1.216884]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2160 [D loss: 0.562227, acc.: 75.20%] [G loss: 1.230763]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2161 [D loss: 0.568231, acc.: 75.88%] [G loss: 1.221976]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2162 [D loss: 0.567194, acc.: 73.73%] [G loss: 1.216904]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2163 [D loss: 0.579509, acc.: 73.19%] [G loss: 1.212799]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2164 [D loss: 0.570051, acc.: 75.83%] [G loss: 1.219002]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2165 [D loss: 0.560833, acc.: 75.93%] [G loss: 1.216567]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2166 [D loss: 0.562926, acc.: 77.29%] [G loss: 1.213740]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2167 [D loss: 0.568901, acc.: 74.71%] [G loss: 1.228161]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2168 [D loss: 0.569001, acc.: 73.68%] [G loss: 1.219941]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2169 [D loss: 0.556132, acc.: 76.71%] [G loss: 1.224814]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2170 [D loss: 0.574247, acc.: 74.12%] [G loss: 1.219325]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2171 [D loss: 0.566986, acc.: 75.44%] [G loss: 1.222388]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2172 [D loss: 0.575693, acc.: 73.83%] [G loss: 1.214025]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2173 [D loss: 0.570986, acc.: 74.51%] [G loss: 1.222801]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2174 [D loss: 0.566995, acc.: 74.80%] [G loss: 1.215215]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2175 [D loss: 0.574644, acc.: 72.95%] [G loss: 1.226428]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2176 [D loss: 0.559392, acc.: 75.00%] [G loss: 1.205251]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2177 [D loss: 0.570269, acc.: 74.37%] [G loss: 1.219245]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2178 [D loss: 0.568873, acc.: 74.37%] [G loss: 1.235379]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2179 [D loss: 0.568373, acc.: 75.05%] [G loss: 1.208943]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2180 [D loss: 0.570988, acc.: 75.15%] [G loss: 1.216710]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2181 [D loss: 0.568002, acc.: 75.20%] [G loss: 1.213801]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2182 [D loss: 0.569159, acc.: 75.34%] [G loss: 1.218462]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2183 [D loss: 0.578675, acc.: 73.44%] [G loss: 1.217016]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2184 [D loss: 0.564646, acc.: 75.49%] [G loss: 1.215095]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2185 [D loss: 0.578991, acc.: 73.14%] [G loss: 1.222077]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2186 [D loss: 0.565635, acc.: 74.27%] [G loss: 1.225812]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2187 [D loss: 0.573395, acc.: 73.58%] [G loss: 1.211738]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2188 [D loss: 0.573138, acc.: 73.68%] [G loss: 1.215280]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2189 [D loss: 0.579798, acc.: 72.36%] [G loss: 1.217774]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2190 [D loss: 0.561906, acc.: 75.49%] [G loss: 1.212181]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2191 [D loss: 0.575701, acc.: 74.37%] [G loss: 1.217429]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2192 [D loss: 0.565086, acc.: 74.56%] [G loss: 1.232558]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2193 [D loss: 0.570074, acc.: 73.44%] [G loss: 1.213827]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2194 [D loss: 0.558193, acc.: 76.12%] [G loss: 1.219078]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2195 [D loss: 0.563464, acc.: 74.61%] [G loss: 1.229572]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2196 [D loss: 0.564686, acc.: 75.83%] [G loss: 1.242858]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2197 [D loss: 0.586968, acc.: 71.58%] [G loss: 1.221463]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2198 [D loss: 0.568637, acc.: 73.83%] [G loss: 1.220303]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2199 [D loss: 0.566090, acc.: 73.78%] [G loss: 1.226932]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2200 [D loss: 0.558786, acc.: 75.05%] [G loss: 1.226353]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2201 [D loss: 0.569289, acc.: 73.93%] [G loss: 1.219726]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2202 [D loss: 0.571840, acc.: 73.68%] [G loss: 1.210712]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2203 [D loss: 0.584055, acc.: 71.68%] [G loss: 1.223853]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2204 [D loss: 0.586057, acc.: 72.31%] [G loss: 1.233986]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2205 [D loss: 0.560005, acc.: 77.05%] [G loss: 1.208732]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2206 [D loss: 0.577590, acc.: 73.34%] [G loss: 1.229081]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2207 [D loss: 0.572986, acc.: 74.51%] [G loss: 1.216582]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2208 [D loss: 0.576687, acc.: 73.34%] [G loss: 1.215409]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2209 [D loss: 0.564319, acc.: 75.78%] [G loss: 1.227013]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2210 [D loss: 0.576867, acc.: 73.44%] [G loss: 1.213898]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2211 [D loss: 0.585150, acc.: 72.17%] [G loss: 1.231756]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2212 [D loss: 0.578445, acc.: 74.80%] [G loss: 1.219054]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2213 [D loss: 0.581736, acc.: 71.68%] [G loss: 1.225022]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2214 [D loss: 0.569092, acc.: 73.19%] [G loss: 1.215430]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2215 [D loss: 0.573116, acc.: 72.75%] [G loss: 1.231952]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2216 [D loss: 0.578322, acc.: 72.51%] [G loss: 1.215559]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2217 [D loss: 0.575084, acc.: 75.24%] [G loss: 1.206106]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2218 [D loss: 0.588464, acc.: 72.02%] [G loss: 1.221503]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2219 [D loss: 0.580862, acc.: 72.51%] [G loss: 1.217446]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2220 [D loss: 0.573272, acc.: 73.93%] [G loss: 1.218322]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2221 [D loss: 0.584136, acc.: 72.17%] [G loss: 1.208886]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2222 [D loss: 0.579492, acc.: 72.56%] [G loss: 1.210329]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2223 [D loss: 0.571122, acc.: 74.22%] [G loss: 1.220690]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2224 [D loss: 0.580892, acc.: 72.51%] [G loss: 1.213509]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2225 [D loss: 0.581831, acc.: 72.56%] [G loss: 1.207696]\n",
            "32/32 [==============================] - 0s 4ms/step\n",
            "2226 [D loss: 0.578669, acc.: 73.29%] [G loss: 1.225608]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2227 [D loss: 0.569048, acc.: 73.88%] [G loss: 1.217759]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2228 [D loss: 0.580261, acc.: 72.95%] [G loss: 1.207516]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2229 [D loss: 0.580594, acc.: 71.63%] [G loss: 1.202910]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2230 [D loss: 0.582955, acc.: 72.12%] [G loss: 1.189320]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2231 [D loss: 0.583305, acc.: 72.22%] [G loss: 1.191121]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2232 [D loss: 0.587580, acc.: 71.58%] [G loss: 1.201659]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2233 [D loss: 0.571669, acc.: 73.73%] [G loss: 1.201947]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2234 [D loss: 0.573474, acc.: 72.95%] [G loss: 1.196209]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2235 [D loss: 0.588499, acc.: 70.17%] [G loss: 1.207410]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2236 [D loss: 0.571662, acc.: 73.68%] [G loss: 1.193556]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2237 [D loss: 0.571058, acc.: 73.49%] [G loss: 1.218772]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2238 [D loss: 0.579249, acc.: 73.83%] [G loss: 1.195464]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2239 [D loss: 0.579644, acc.: 72.31%] [G loss: 1.213433]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2240 [D loss: 0.575934, acc.: 72.12%] [G loss: 1.204167]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2241 [D loss: 0.577323, acc.: 72.75%] [G loss: 1.210788]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2242 [D loss: 0.591267, acc.: 70.85%] [G loss: 1.210839]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2243 [D loss: 0.585489, acc.: 70.61%] [G loss: 1.196046]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2244 [D loss: 0.586915, acc.: 71.19%] [G loss: 1.192902]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2245 [D loss: 0.583458, acc.: 72.66%] [G loss: 1.206104]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2246 [D loss: 0.582964, acc.: 71.53%] [G loss: 1.195770]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2247 [D loss: 0.576520, acc.: 74.07%] [G loss: 1.198950]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2248 [D loss: 0.578755, acc.: 72.51%] [G loss: 1.205814]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2249 [D loss: 0.576111, acc.: 72.80%] [G loss: 1.209610]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2250 [D loss: 0.587789, acc.: 70.85%] [G loss: 1.199806]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2251 [D loss: 0.576286, acc.: 72.66%] [G loss: 1.197452]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2252 [D loss: 0.594848, acc.: 69.43%] [G loss: 1.224899]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2253 [D loss: 0.576174, acc.: 73.29%] [G loss: 1.199885]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2254 [D loss: 0.578916, acc.: 72.61%] [G loss: 1.208265]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2255 [D loss: 0.577075, acc.: 71.92%] [G loss: 1.203311]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2256 [D loss: 0.583236, acc.: 71.73%] [G loss: 1.204730]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2257 [D loss: 0.590099, acc.: 70.41%] [G loss: 1.211101]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2258 [D loss: 0.584386, acc.: 71.97%] [G loss: 1.197634]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2259 [D loss: 0.574007, acc.: 72.71%] [G loss: 1.210217]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2260 [D loss: 0.584194, acc.: 71.19%] [G loss: 1.205969]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2261 [D loss: 0.580646, acc.: 71.68%] [G loss: 1.185472]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2262 [D loss: 0.587769, acc.: 71.34%] [G loss: 1.206753]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2263 [D loss: 0.589975, acc.: 70.21%] [G loss: 1.197866]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2264 [D loss: 0.581269, acc.: 72.02%] [G loss: 1.207966]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2265 [D loss: 0.589976, acc.: 70.95%] [G loss: 1.193394]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2266 [D loss: 0.586667, acc.: 72.12%] [G loss: 1.212991]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2267 [D loss: 0.585859, acc.: 71.83%] [G loss: 1.186837]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2268 [D loss: 0.589147, acc.: 70.90%] [G loss: 1.197338]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2269 [D loss: 0.579637, acc.: 72.31%] [G loss: 1.205282]\n",
            "32/32 [==============================] - 0s 4ms/step\n",
            "2270 [D loss: 0.599496, acc.: 69.78%] [G loss: 1.196361]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2271 [D loss: 0.582446, acc.: 71.34%] [G loss: 1.177408]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2272 [D loss: 0.593211, acc.: 70.02%] [G loss: 1.188638]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2273 [D loss: 0.580563, acc.: 72.61%] [G loss: 1.198074]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2274 [D loss: 0.583508, acc.: 72.12%] [G loss: 1.185039]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2275 [D loss: 0.585942, acc.: 71.53%] [G loss: 1.191544]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2276 [D loss: 0.580604, acc.: 72.75%] [G loss: 1.186777]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2277 [D loss: 0.596703, acc.: 69.53%] [G loss: 1.187109]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2278 [D loss: 0.594282, acc.: 69.82%] [G loss: 1.182967]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2279 [D loss: 0.576609, acc.: 73.39%] [G loss: 1.184523]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2280 [D loss: 0.585245, acc.: 70.41%] [G loss: 1.191273]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2281 [D loss: 0.591911, acc.: 71.04%] [G loss: 1.192641]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2282 [D loss: 0.592726, acc.: 71.24%] [G loss: 1.176838]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2283 [D loss: 0.580158, acc.: 71.88%] [G loss: 1.183019]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2284 [D loss: 0.574962, acc.: 72.17%] [G loss: 1.189414]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2285 [D loss: 0.596812, acc.: 69.58%] [G loss: 1.177539]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2286 [D loss: 0.588110, acc.: 71.34%] [G loss: 1.179829]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2287 [D loss: 0.594434, acc.: 69.92%] [G loss: 1.189448]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2288 [D loss: 0.584840, acc.: 71.04%] [G loss: 1.186723]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2289 [D loss: 0.590826, acc.: 71.09%] [G loss: 1.173955]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2290 [D loss: 0.584589, acc.: 71.92%] [G loss: 1.185035]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2291 [D loss: 0.592385, acc.: 69.24%] [G loss: 1.165537]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2292 [D loss: 0.605053, acc.: 68.26%] [G loss: 1.184865]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2293 [D loss: 0.592606, acc.: 69.63%] [G loss: 1.184226]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2294 [D loss: 0.588765, acc.: 70.95%] [G loss: 1.191903]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2295 [D loss: 0.590681, acc.: 70.36%] [G loss: 1.192604]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2296 [D loss: 0.594324, acc.: 70.46%] [G loss: 1.185114]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2297 [D loss: 0.585519, acc.: 71.53%] [G loss: 1.184854]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2298 [D loss: 0.601349, acc.: 68.36%] [G loss: 1.171735]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2299 [D loss: 0.588272, acc.: 71.14%] [G loss: 1.187673]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2300 [D loss: 0.585961, acc.: 70.85%] [G loss: 1.177819]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2301 [D loss: 0.588241, acc.: 71.29%] [G loss: 1.179441]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2302 [D loss: 0.606828, acc.: 68.85%] [G loss: 1.172789]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2303 [D loss: 0.590234, acc.: 71.88%] [G loss: 1.174280]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2304 [D loss: 0.592640, acc.: 70.70%] [G loss: 1.159734]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2305 [D loss: 0.597970, acc.: 70.31%] [G loss: 1.168347]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2306 [D loss: 0.604605, acc.: 67.53%] [G loss: 1.170655]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2307 [D loss: 0.588304, acc.: 69.82%] [G loss: 1.161788]\n",
            "32/32 [==============================] - 0s 3ms/step\n",
            "2308 [D loss: 0.594003, acc.: 69.53%] [G loss: 1.168493]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2309 [D loss: 0.601532, acc.: 69.68%] [G loss: 1.175253]\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "2310 [D loss: 0.582068, acc.: 71.83%] [G loss: 1.169376]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-33097643b7d4>\u001b[0m in \u001b[0;36m<cell line: 145>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0mdisc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-33097643b7d4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# predict using a Generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mgen_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0;31m# calculate loss functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mreal_disc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2347\u001b[0m                     )\n\u001b[1;32m   2348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2349\u001b[0;31m             data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[1;32m   2350\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1581\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1582\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1583\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1261\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;31m# simultaneous shuffles can contend on a hardware level and degrade all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;31m# performance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2238\u001b[0m     \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m     return map_op._map_v2(\n\u001b[0m\u001b[1;32m   2241\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m     35\u001b[0m       warnings.warn(\"The `deterministic` argument has no effect unless the \"\n\u001b[1;32m     36\u001b[0m                     \"`num_parallel_calls` argument is specified.\")\n\u001b[0;32m---> 37\u001b[0;31m     return _MapDataset(\n\u001b[0m\u001b[1;32m     38\u001b[0m         input_dataset, map_func, preserve_cardinality=True, name=name)\n\u001b[1;32m     39\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preserve_cardinality\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     self._map_func = structured_function.StructuredFunctionWrapper(\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorSpec\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \"\"\"\n\u001b[0;32m--> 232\u001b[0;31m     concrete_function = self._get_concrete_function_garbage_collected(\n\u001b[0m\u001b[1;32m    233\u001b[0m         *args, **kwargs)\n\u001b[1;32m    234\u001b[0m     \u001b[0mconcrete_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m       \u001b[0mconcrete_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m       \u001b[0mconcrete_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arg_keywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m_maybe_define_concrete_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_concrete_function_internal_garbage_collected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m           \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplaceholder_bound_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m           concrete_function = self._create_concrete_function(\n\u001b[0m\u001b[1;32m    397\u001b[0m               args, kwargs, func_graph)\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m_create_concrete_function\u001b[0;34m(self, args, kwargs, func_graph)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     concrete_function = monomorphic_function.ConcreteFunction(\n\u001b[0;32m--> 300\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1139\u001b[0m         convert_structure_to_signature(\n\u001b[1;32m   1140\u001b[0m             func_args, arg_names, signature_context=signature_context),\n\u001b[0;32m-> 1141\u001b[0;31m         convert_structure_to_signature(\n\u001b[0m\u001b[1;32m   1142\u001b[0m             func_kwargs, signature_context=signature_context))\n\u001b[1;32m   1143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mconvert_structure_to_signature\u001b[0;34m(structure, arg_names, signature_context)\u001b[0m\n\u001b[1;32m    137\u001b[0m     ]\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mencode_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflattened\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    137\u001b[0m     ]\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mencode_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflattened\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras. optimizers import Adam, SGD\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "num_rows = 28\n",
        "num_cols = 28\n",
        "num_channels = 1\n",
        "batch_size=1024\n",
        "input_shape = (num_rows, num_cols, num_channels)\n",
        "z_size = 1000\n",
        "\n",
        "(train_ims, _), (_, _) = mnist.load_data()\n",
        "train_ims = train_ims / 127.5 - 1.\n",
        "train_ims = np.expand_dims(train_ims, axis=3)\n",
        "\n",
        "valid = np.ones((batch_size, 1))\n",
        "fake = np.zeros((batch_size, 0))\n",
        "def build_generator():\n",
        "    gen_model = Sequential()\n",
        "    gen_model.add(Dense(256, input_dim=z_size))\n",
        "    gen_model.add(LeakyReLU(alpha=0.2))\n",
        "    gen_model.add(BatchNormalization(momentum=0.8))\n",
        "    gen_model.add(Dense(512))\n",
        "    gen_model.add(LeakyReLU(alpha=0.2))\n",
        "    gen_model.add(BatchNormalization(momentum=0.8))\n",
        "    gen_model.add(Dense(1024))\n",
        "    gen_model.add(LeakyReLU(alpha=0.2))\n",
        "    gen_model.add(BatchNormalization(momentum=0.8))\n",
        "    gen_model.add(Dense(np.prod(input_shape), activation='tanh'))\n",
        "    gen_model.add(Reshape(input_shape))\n",
        "\n",
        "    gen_noise = Input(shape=(z_size,))\n",
        "    gen_img = gen_model(gen_noise)\n",
        "    return Model(gen_noise, gen_img)\n",
        "\n",
        "def build_discriminator():\n",
        "\n",
        "\tdisc_model = Sequential()\n",
        "\tdisc_model.add(Flatten(input_shape=input_shape))\n",
        "\tdisc_model.add(Dense(512))\n",
        "\tdisc_model.add(LeakyReLU(alpha=0.2))\n",
        "\tdisc_model.add(Dense(256))\n",
        "\tdisc_model.add(LeakyReLU(alpha=0.2))\n",
        "\tdisc_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\tdisc_img = Input(shape=input_shape)\n",
        "\tvalidity = disc_model(disc_img)\n",
        "\treturn Model(disc_img, validity)\n",
        "# discriminator\n",
        "disc= build_discriminator()\n",
        "disc.compile(loss='binary_crossentropy',\n",
        "    optimizer='sgd',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "z = Input(shape=(z_size,))\n",
        "# generator\n",
        "generator = build_generator()\n",
        "img = generator(z)\n",
        "disc.trainable = False\n",
        "validity = disc(img)\n",
        "# combined model\n",
        "combined = Model(z, validity)\n",
        "combined.compile(loss='binary_crossentropy', optimizer='sgd')\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "def intialize_model():\n",
        "    disc= build_discriminator()\n",
        "    disc.compile(loss='binary_crossentropy',\n",
        "        optimizer='sgd',\n",
        "        metrics=['accuracy'])\n",
        "\n",
        "    generator = build_generator()\n",
        "\n",
        "    z = Input(shape=(z_size,))\n",
        "    img = generator(z)\n",
        "\n",
        "    disc.trainable = False\n",
        "\n",
        "    validity = disc(img)\n",
        "\n",
        "    combined = Model(z, validity)\n",
        "    combined.compile(loss='binary_crossentropy', optimizer='sgd')\n",
        "    return disc, generator, combined\n",
        "def train(epochs, batch_size, sample_interval):\n",
        "    # load images\n",
        "    (train_ims, _), (_, _) = mnist.load_data()\n",
        "    # preprocess\n",
        "    train_ims = train_ims / 127.5 - 1.\n",
        "    train_ims = np.expand_dims(train_ims, axis=3)\n",
        "\n",
        "    valid = np.ones((batch_size, 1))\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "    # training loop\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        batch_index = np.random.randint(0, train_ims.shape[0], batch_size)\n",
        "        imgs = train_ims[batch_index]\n",
        "    # create noise\n",
        "        noise = np.random.normal(0, 1, (batch_size, z_size))\n",
        "    # predict using a Generator\n",
        "        gen_imgs = gen.predict(noise)\n",
        "    # calculate loss functions\n",
        "        real_disc_loss = disc.train_on_batch(imgs, valid)\n",
        "        fake_disc_loss = disc.train_on_batch(gen_imgs, fake)\n",
        "        disc_loss_total = 0.5 * np.add(real_disc_loss, fake_disc_loss)\n",
        "\n",
        "        noise = np.random.normal(0, 1, (batch_size, z_size))\n",
        "\n",
        "        g_loss = full_model.train_on_batch(noise, valid)\n",
        "    # show progress\n",
        "        print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, disc_loss_total[0], 100*disc_loss_total[1], g_loss))\n",
        "    # save outputs every few epochs\n",
        "        if epoch % sample_interval == 0:\n",
        "            one_batch(epoch)\n",
        "def one_batch(epoch):\n",
        "    r, c = 5, 5\n",
        "    noise = np.random.normal(0, 1, (r * c, z_size))\n",
        "    gen_imgs = gen.predict(noise)\n",
        "\n",
        "    # Rescale images 0 - 1\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    fig, axs = plt.subplots(r, c)\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='binary')\n",
        "            axs[i,j].axis('off')\n",
        "            cnt += 1\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    fig.savefig(\"/content/drive/MyDrive/cars/images/f512f%d.png\" % epoch)\n",
        "    plt.close()\n",
        "disc, gen, full_model = intialize_model()\n",
        "train(epochs=9000, batch_size=1024, sample_interval=1000)"
      ]
    }
  ]
}